[
    {
        "sha": "c9200919330b9f42d4051b19b723ec054e279230",
        "author": "ThatAlexPalmer",
        "date": "2024-02-12 02:26:06+00:00",
        "message": "Merge pull request #156 from transfer-agent-protocol/dev\n\nimprovement(docs) - update readme, add slightly better logging",
        "files": [
            {
                "filename": ".env.example",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -17,5 +17,5 @@ ETHERSCAN_L2_API_KEY=UPDATE_ME\n ETHERSCAN_L1_API_KEY=UPDATE_ME\n \n # Server port\n-PORT=8080\n+PORT=8293\n "
            },
            {
                "filename": "README.md",
                "additions": 9,
                "deletions": 99,
                "patch": "@@ -1,107 +1,25 @@\n-# Transfer Agent Protocol, Specification for a compliant cap table onchain [TAP](https://transferagentprotocol.xyz)\n-\n-Developed by:\n-\n-# [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n-\n--   [Poet](https://poet.network/)\n--   [Plural Energy](https://www.pluralenergy.co/)\n--   [Fairmint](https://www.fairmint.com/)\n-\n-This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety. In development, it's meant to be run in a Docker container with a local MongoDB instance. While in active development, it's meant to be run with [Anvil](https://book.getfoundry.sh/anvil/) and [Forge](https://book.getfoundry.sh/forge/).\n-\n <div align=\"center\">\n   <a href=\"https://github.com/transfer-agent-protocol/tap-cap-table/blob/main/LICENSE\">\n     <img alt=\"License\" src=\"https://img.shields.io/github/license/transfer-agent-protocol/tap-cap-table\">\n   </a>\n </div>\n \n-## Dependencies\n-\n--   [Docker](https://docs.docker.com/get-docker/)\n-\n--   [Foundry](https://getfoundry.sh/)\n-\n-```sh\n-curl -L https://foundry.paradigm.xyz | bash\n-```\n-\n--   [Mongo Compass](https://www.mongodb.com/try/download/compass)\n-\n--   [Postman App](https://www.postman.com/downloads/)\n-\n--   [Node.js v18.16.0](https://nodejs.org/en/download/)\n-\n--   [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n-\n-We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo) for the local development database. You can find the [Docker Compose file](./docker-compose.yml) in the root of this repository.\n-\n-## Official links\n-\n--   [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n--   [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n-\n-## Getting started\n-\n-Ensure you have all dependencies setup. Clone the repository with OCF and Forge included as submodules:\n-\n-```sh\n-git clone --recurse-submodules https://github.com/poet-network/tap-cap-table.git\n-```\n-\n-## Initial setup\n-\n-Our `.env.example` files are setup for local development with Docker and Foundry. You'll need to copy them to `.env` and update the values with your own. If you're a contributor working with us, you will get those values from our Bitwarden vault.\n-\n-Copy `.env.example` to `.env` in the root of the project.\n-\n-```sh\n-cd tap-cap-table && cp .env.example .env\n-```\n-\n-In the root folder, pull the official Mongo image, and run the local development database with `docker compose`:\n+# Documentation\n \n-```sh\n-docker compose up\n-```\n-\n-## Running Anvil\n-\n-This repo is onchain first. We use [Anvil](https://book.getfoundry.sh/anvil/) to run the local blockchain and deploy our cap table smart contracts. At all times, you should have Anvil running alongside Docker and nodemon.\n-\n-With the mongo DB running on Docker you can start Anvil.\n-\n-In the `/chain` directory, run:\n-\n-```sh\n-anvil\n-```\n-\n-Install dependencies and setup [Foundry](https://book.getfoundry.sh/) and `forge` with our setup script:\n-\n-```sh\n-yarn install && yarn setup\n-```\n-\n-## Deploying the cap table smart contracts\n-\n-In our architecture, each transaction is mapped to an external library, which ensures bytecode limits are never met. To deploy these libraries:\n+Read official docs at [https://docs.transferagentprotocol.xyz](https://docs.transferagentprotocol.xyz/) to get started.\n \n-1. Ensure you have Anvil running in the `/chain` directory\n-2. Then, inside of the root directory run\n+This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety.\n \n-```sh\n-yarn deploy-libraries\n-```\n+## Contributing\n \n-This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n+We welcome all contributions. Please give a quick read to our [CONTRIBUTING](./CONTRIBUTING.md) guidelines to understand the them and the process.\n \n-```sh\n-yarn deploy-factory [.env file to use]\n-```\n+## License\n \n-This will deploy the factory as pointed to in the `.env` or the file you pass in, such as .env.local\n+This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n \n+## OUTDATED README BELOW\n+---\n ## Running the cap table server\n \n After the deployment script is completed, start the server with nodemon:\n@@ -189,11 +107,3 @@ Integration test setup from no active processes:\n     -   In MongoDB compass, create/update `implementation_address` and `factory_address` in `jest-integration.factories`\n         -   If the `jest-integration` MongoDB databases dont exist: `cd .. && yarn test-js-integration`\n     -   Run `yarn test-js-integration`!\n-\n-## Contributing\n-\n-See [CONTRIBUTING.md](./CONTRIBUTING.md) for details on how to contribute to this project.\n-\n-## License\n-\n-This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details."
            },
            {
                "filename": "src/app.js",
                "additions": 18,
                "deletions": 4,
                "patch": "@@ -73,10 +73,24 @@ export const startServer = async (finalizedOnly) => {\n     // Connect to MongoDB\n     const dbConn = await connectDB();\n \n-    const server = app.listen(PORT, async () => {\n-        console.log(`\ud83d\ude80  Server successfully launched at ${PORT}`);\n-        // Asynchronous job to track web3 events in web2\n-        startEventProcessing(finalizedOnly, dbConn);\n+    const server = app\n+        .listen(PORT, async () => {\n+            console.log(`\ud83d\ude80  Server successfully launched at ${PORT}`);\n+            // Asynchronous job to track web3 events in web2\n+            startEventProcessing(finalizedOnly, dbConn);\n+        })\n+        .on(\"error\", (err) => {\n+            if (err.code === \"EADDRINUSE\") {\n+                console.log(`Port ${PORT} is already in use.`);\n+            } else {\n+                console.log(err);\n+            }\n+        });\n+\n+    server.on(\"listening\", () => {\n+        const address = server.address();\n+        const bind = typeof address === \"string\" ? \"pipe \" + address : \"port \" + address.port;\n+        console.log(\"Listening on \" + bind);\n     });\n \n     return server;"
            },
            {
                "filename": "src/scripts/sampleData.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,5 +1,5 @@\n export const issuer = {\n-    legal_name: \"Banana Peppers Inc.\",\n+    legal_name: \"Transfer Agent Protocol\",\n     formation_date: \"2022-08-23\",\n     country_of_formation: \"US\",\n     country_subdivision_of_formation: \"DE\",\n@@ -10,7 +10,7 @@ export const issuer = {\n         },\n     ],\n     email: {\n-        email_address: \"concierge@poet.network\",\n+        email_address: \"alex@transferagentprotocol.xyz\",\n         email_type: \"BUSINESS\",\n     },\n     initial_shares_authorized: \"10000000\","
            }
        ]
    },
    {
        "sha": "643cd3f96edcf0d49874fb102af9e7e6ae9c665b",
        "author": "ThatAlexPalmer",
        "date": "2024-02-12 02:24:10+00:00",
        "message": "Merge pull request #155 from transfer-agent-protocol/apalmer/testing-clean-setup-and-deployments\n\nimprovement(docs) - testing clean setup and deployments",
        "files": [
            {
                "filename": ".env.example",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -17,5 +17,5 @@ ETHERSCAN_L2_API_KEY=UPDATE_ME\n ETHERSCAN_L1_API_KEY=UPDATE_ME\n \n # Server port\n-PORT=8080\n+PORT=8293\n "
            },
            {
                "filename": "README.md",
                "additions": 9,
                "deletions": 99,
                "patch": "@@ -1,107 +1,25 @@\n-# Transfer Agent Protocol, Specification for a compliant cap table onchain [TAP](https://transferagentprotocol.xyz)\n-\n-Developed by:\n-\n-# [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n-\n--   [Poet](https://poet.network/)\n--   [Plural Energy](https://www.pluralenergy.co/)\n--   [Fairmint](https://www.fairmint.com/)\n-\n-This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety. In development, it's meant to be run in a Docker container with a local MongoDB instance. While in active development, it's meant to be run with [Anvil](https://book.getfoundry.sh/anvil/) and [Forge](https://book.getfoundry.sh/forge/).\n-\n <div align=\"center\">\n   <a href=\"https://github.com/transfer-agent-protocol/tap-cap-table/blob/main/LICENSE\">\n     <img alt=\"License\" src=\"https://img.shields.io/github/license/transfer-agent-protocol/tap-cap-table\">\n   </a>\n </div>\n \n-## Dependencies\n-\n--   [Docker](https://docs.docker.com/get-docker/)\n-\n--   [Foundry](https://getfoundry.sh/)\n-\n-```sh\n-curl -L https://foundry.paradigm.xyz | bash\n-```\n-\n--   [Mongo Compass](https://www.mongodb.com/try/download/compass)\n-\n--   [Postman App](https://www.postman.com/downloads/)\n-\n--   [Node.js v18.16.0](https://nodejs.org/en/download/)\n-\n--   [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n-\n-We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo) for the local development database. You can find the [Docker Compose file](./docker-compose.yml) in the root of this repository.\n-\n-## Official links\n-\n--   [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n--   [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n-\n-## Getting started\n-\n-Ensure you have all dependencies setup. Clone the repository with OCF and Forge included as submodules:\n-\n-```sh\n-git clone --recurse-submodules https://github.com/poet-network/tap-cap-table.git\n-```\n-\n-## Initial setup\n-\n-Our `.env.example` files are setup for local development with Docker and Foundry. You'll need to copy them to `.env` and update the values with your own. If you're a contributor working with us, you will get those values from our Bitwarden vault.\n-\n-Copy `.env.example` to `.env` in the root of the project.\n-\n-```sh\n-cd tap-cap-table && cp .env.example .env\n-```\n-\n-In the root folder, pull the official Mongo image, and run the local development database with `docker compose`:\n+# Documentation\n \n-```sh\n-docker compose up\n-```\n-\n-## Running Anvil\n-\n-This repo is onchain first. We use [Anvil](https://book.getfoundry.sh/anvil/) to run the local blockchain and deploy our cap table smart contracts. At all times, you should have Anvil running alongside Docker and nodemon.\n-\n-With the mongo DB running on Docker you can start Anvil.\n-\n-In the `/chain` directory, run:\n-\n-```sh\n-anvil\n-```\n-\n-Install dependencies and setup [Foundry](https://book.getfoundry.sh/) and `forge` with our setup script:\n-\n-```sh\n-yarn install && yarn setup\n-```\n-\n-## Deploying the cap table smart contracts\n-\n-In our architecture, each transaction is mapped to an external library, which ensures bytecode limits are never met. To deploy these libraries:\n+Read official docs at [https://docs.transferagentprotocol.xyz](https://docs.transferagentprotocol.xyz/) to get started.\n \n-1. Ensure you have Anvil running in the `/chain` directory\n-2. Then, inside of the root directory run\n+This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety.\n \n-```sh\n-yarn deploy-libraries\n-```\n+## Contributing\n \n-This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n+We welcome all contributions. Please give a quick read to our [CONTRIBUTING](./CONTRIBUTING.md) guidelines to understand the them and the process.\n \n-```sh\n-yarn deploy-factory [.env file to use]\n-```\n+## License\n \n-This will deploy the factory as pointed to in the `.env` or the file you pass in, such as .env.local\n+This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n \n+## OUTDATED README BELOW\n+---\n ## Running the cap table server\n \n After the deployment script is completed, start the server with nodemon:\n@@ -189,11 +107,3 @@ Integration test setup from no active processes:\n     -   In MongoDB compass, create/update `implementation_address` and `factory_address` in `jest-integration.factories`\n         -   If the `jest-integration` MongoDB databases dont exist: `cd .. && yarn test-js-integration`\n     -   Run `yarn test-js-integration`!\n-\n-## Contributing\n-\n-See [CONTRIBUTING.md](./CONTRIBUTING.md) for details on how to contribute to this project.\n-\n-## License\n-\n-This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details."
            },
            {
                "filename": "src/app.js",
                "additions": 18,
                "deletions": 4,
                "patch": "@@ -73,10 +73,24 @@ export const startServer = async (finalizedOnly) => {\n     // Connect to MongoDB\n     const dbConn = await connectDB();\n \n-    const server = app.listen(PORT, async () => {\n-        console.log(`\ud83d\ude80  Server successfully launched at ${PORT}`);\n-        // Asynchronous job to track web3 events in web2\n-        startEventProcessing(finalizedOnly, dbConn);\n+    const server = app\n+        .listen(PORT, async () => {\n+            console.log(`\ud83d\ude80  Server successfully launched at ${PORT}`);\n+            // Asynchronous job to track web3 events in web2\n+            startEventProcessing(finalizedOnly, dbConn);\n+        })\n+        .on(\"error\", (err) => {\n+            if (err.code === \"EADDRINUSE\") {\n+                console.log(`Port ${PORT} is already in use.`);\n+            } else {\n+                console.log(err);\n+            }\n+        });\n+\n+    server.on(\"listening\", () => {\n+        const address = server.address();\n+        const bind = typeof address === \"string\" ? \"pipe \" + address : \"port \" + address.port;\n+        console.log(\"Listening on \" + bind);\n     });\n \n     return server;"
            },
            {
                "filename": "src/scripts/sampleData.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,5 +1,5 @@\n export const issuer = {\n-    legal_name: \"Banana Peppers Inc.\",\n+    legal_name: \"Transfer Agent Protocol\",\n     formation_date: \"2022-08-23\",\n     country_of_formation: \"US\",\n     country_subdivision_of_formation: \"DE\",\n@@ -10,7 +10,7 @@ export const issuer = {\n         },\n     ],\n     email: {\n-        email_address: \"concierge@poet.network\",\n+        email_address: \"alex@transferagentprotocol.xyz\",\n         email_type: \"BUSINESS\",\n     },\n     initial_shares_authorized: \"10000000\","
            }
        ]
    },
    {
        "sha": "eb4d722fab8eb1c8cfaeb417d6e7f61f0e0b328e",
        "author": "ThatAlexPalmer",
        "date": "2024-02-12 02:21:29+00:00",
        "message": "revert deterministic factory to default",
        "files": [
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -26,7 +26,7 @@ const allowPropagate = async () => {\n const seedExampleData = async () => {\n     const rec = await Factory.findOne();\n     if (!rec) {\n-        const deterministicFactory = \"0x164BEfB302Aa6b2B8fD5485bD2A3d472683B2794\";\n+        const deterministicFactory = \"0xDc64a140Aa3E981100a9becA4E685f962f0cF6C9\";\n         const resp = await axios.post(`${SERVER_BASE}/factory/register`, {factory_address: deterministicFactory});\n         console.log(\"Used deterministic factory address. May need to change in future\", resp.data);\n         // throw new Error("
            }
        ]
    },
    {
        "sha": "e9ce8f7a303daa5ce3c109180f1ac0e077bd904e",
        "author": "ThatAlexPalmer",
        "date": "2024-02-12 02:17:00+00:00",
        "message": "link to docs",
        "files": [
            {
                "filename": "README.md",
                "additions": 9,
                "deletions": 99,
                "patch": "@@ -1,107 +1,25 @@\n-# Transfer Agent Protocol, Specification for a compliant cap table onchain [TAP](https://transferagentprotocol.xyz)\n-\n-Developed by:\n-\n-# [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n-\n--   [Poet](https://poet.network/)\n--   [Plural Energy](https://www.pluralenergy.co/)\n--   [Fairmint](https://www.fairmint.com/)\n-\n-This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety. In development, it's meant to be run in a Docker container with a local MongoDB instance. While in active development, it's meant to be run with [Anvil](https://book.getfoundry.sh/anvil/) and [Forge](https://book.getfoundry.sh/forge/).\n-\n <div align=\"center\">\n   <a href=\"https://github.com/transfer-agent-protocol/tap-cap-table/blob/main/LICENSE\">\n     <img alt=\"License\" src=\"https://img.shields.io/github/license/transfer-agent-protocol/tap-cap-table\">\n   </a>\n </div>\n \n-## Dependencies\n-\n--   [Docker](https://docs.docker.com/get-docker/)\n-\n--   [Foundry](https://getfoundry.sh/)\n-\n-```sh\n-curl -L https://foundry.paradigm.xyz | bash\n-```\n-\n--   [Mongo Compass](https://www.mongodb.com/try/download/compass)\n-\n--   [Postman App](https://www.postman.com/downloads/)\n-\n--   [Node.js v18.16.0](https://nodejs.org/en/download/)\n-\n--   [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n-\n-We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo) for the local development database. You can find the [Docker Compose file](./docker-compose.yml) in the root of this repository.\n-\n-## Official links\n-\n--   [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n--   [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n-\n-## Getting started\n-\n-Ensure you have all dependencies setup. Clone the repository with OCF and Forge included as submodules:\n-\n-```sh\n-git clone --recurse-submodules https://github.com/poet-network/tap-cap-table.git\n-```\n-\n-## Initial setup\n-\n-Our `.env.example` files are setup for local development with Docker and Foundry. You'll need to copy them to `.env` and update the values with your own. If you're a contributor working with us, you will get those values from our Bitwarden vault.\n-\n-Copy `.env.example` to `.env` in the root of the project.\n-\n-```sh\n-cd tap-cap-table && cp .env.example .env\n-```\n-\n-In the root folder, pull the official Mongo image, and run the local development database with `docker compose`:\n+# Documentation\n \n-```sh\n-docker compose up\n-```\n-\n-## Running Anvil\n-\n-This repo is onchain first. We use [Anvil](https://book.getfoundry.sh/anvil/) to run the local blockchain and deploy our cap table smart contracts. At all times, you should have Anvil running alongside Docker and nodemon.\n-\n-With the mongo DB running on Docker you can start Anvil.\n-\n-In the `/chain` directory, run:\n-\n-```sh\n-anvil\n-```\n-\n-Install dependencies and setup [Foundry](https://book.getfoundry.sh/) and `forge` with our setup script:\n-\n-```sh\n-yarn install && yarn setup\n-```\n-\n-## Deploying the cap table smart contracts\n-\n-In our architecture, each transaction is mapped to an external library, which ensures bytecode limits are never met. To deploy these libraries:\n+Read official docs at [https://docs.transferagentprotocol.xyz](https://docs.transferagentprotocol.xyz/) to get started.\n \n-1. Ensure you have Anvil running in the `/chain` directory\n-2. Then, inside of the root directory run\n+This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety.\n \n-```sh\n-yarn deploy-libraries\n-```\n+## Contributing\n \n-This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n+We welcome all contributions. Please give a quick read to our [CONTRIBUTING](./CONTRIBUTING.md) guidelines to understand the them and the process.\n \n-```sh\n-yarn deploy-factory [.env file to use]\n-```\n+## License\n \n-This will deploy the factory as pointed to in the `.env` or the file you pass in, such as .env.local\n+This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n \n+## OUTDATED README BELOW\n+---\n ## Running the cap table server\n \n After the deployment script is completed, start the server with nodemon:\n@@ -189,11 +107,3 @@ Integration test setup from no active processes:\n     -   In MongoDB compass, create/update `implementation_address` and `factory_address` in `jest-integration.factories`\n         -   If the `jest-integration` MongoDB databases dont exist: `cd .. && yarn test-js-integration`\n     -   Run `yarn test-js-integration`!\n-\n-## Contributing\n-\n-See [CONTRIBUTING.md](./CONTRIBUTING.md) for details on how to contribute to this project.\n-\n-## License\n-\n-This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details."
            }
        ]
    },
    {
        "sha": "f7390ba3364bd721755782a292afe680b9fe76e3",
        "author": "ThatAlexPalmer",
        "date": "2024-02-12 00:11:12+00:00",
        "message": "use a unique port to avoid port confclits",
        "files": [
            {
                "filename": ".env.example",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -17,5 +17,5 @@ ETHERSCAN_L2_API_KEY=UPDATE_ME\n ETHERSCAN_L1_API_KEY=UPDATE_ME\n \n # Server port\n-PORT=8080\n+PORT=8293\n "
            }
        ]
    },
    {
        "sha": "cfd815993fda516a9af47c18c68d0517bdbd65a3",
        "author": "apfong",
        "date": "2024-02-08 17:09:44+00:00",
        "message": "Init submodules in dockerfile",
        "files": [
            {
                "filename": "Dockerfile.prod",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -9,6 +9,7 @@ COPY . .\n \n # Install dependencies and setup\n RUN yarn install\n+RUN git submodule update --init --recursive\n \n EXPOSE 8080\n # Specify the command to run on container start"
            }
        ]
    },
    {
        "sha": "7c10238454c9584dc878612302d09214e3701881",
        "author": "victormimo",
        "date": "2024-02-08 14:28:02+00:00",
        "message": "Merge pull request #152 from transfer-agent-protocol/dev\n\nDev to main",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -76,7 +76,7 @@ export const stopEventProcessing = async () => {\n     }\n }\n \n-export const pollingSleepTime = 1000;\n+export const pollingSleepTime = 10000;\n \n export const startEventProcessing = async (finalizedOnly: boolean, dbConn) => {\n     _keepProcessing = true;\n@@ -110,7 +110,7 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, final\n             return;\n         }\n         if (receipt.blockNumber > latestBlock) {\n-            console.log(\"Deployment tx not finalized\", {receipt, lastFinalizedBlock: latestBlock});\n+            // console.log(\"Deployment tx not finalized\", {receipt, lastFinalizedBlock: latestBlock});\n             return;\n         }\n         lastProcessedBlock = await issuerDeployed(issuerId, receipt, contract, dbConn);"
            }
        ]
    },
    {
        "sha": "f7110501903189ef398af3fb475f3cfc306823a2",
        "author": "victormimo",
        "date": "2024-02-08 14:27:39+00:00",
        "message": "Merge pull request #151 from transfer-agent-protocol/vmimo/remove-unnecessary-logs\n\nimprovement (vmimo/remove unnecessary logs)",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -76,7 +76,7 @@ export const stopEventProcessing = async () => {\n     }\n }\n \n-export const pollingSleepTime = 1000;\n+export const pollingSleepTime = 10000;\n \n export const startEventProcessing = async (finalizedOnly: boolean, dbConn) => {\n     _keepProcessing = true;\n@@ -110,7 +110,7 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, final\n             return;\n         }\n         if (receipt.blockNumber > latestBlock) {\n-            console.log(\"Deployment tx not finalized\", {receipt, lastFinalizedBlock: latestBlock});\n+            // console.log(\"Deployment tx not finalized\", {receipt, lastFinalizedBlock: latestBlock});\n             return;\n         }\n         lastProcessedBlock = await issuerDeployed(issuerId, receipt, contract, dbConn);"
            }
        ]
    },
    {
        "sha": "2cf5b81f44fa720a7115fcc973b30af268a273c1",
        "author": "victormimo",
        "date": "2024-02-08 14:26:04+00:00",
        "message": "removing log to avoid cluttering server logs",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -110,7 +110,7 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, final\n             return;\n         }\n         if (receipt.blockNumber > latestBlock) {\n-            console.log(\"Deployment tx not finalized\", {receipt, lastFinalizedBlock: latestBlock});\n+            // console.log(\"Deployment tx not finalized\", {receipt, lastFinalizedBlock: latestBlock});\n             return;\n         }\n         lastProcessedBlock = await issuerDeployed(issuerId, receipt, contract, dbConn);"
            }
        ]
    },
    {
        "sha": "772758644a2270dd80b81126bae6d0474d516ce8",
        "author": "victormimo",
        "date": "2024-02-08 14:24:15+00:00",
        "message": "increasing polling sleep time to 10s",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -76,7 +76,7 @@ export const stopEventProcessing = async () => {\n     }\n }\n \n-export const pollingSleepTime = 1000;\n+export const pollingSleepTime = 10000;\n \n export const startEventProcessing = async (finalizedOnly: boolean, dbConn) => {\n     _keepProcessing = true;"
            }
        ]
    },
    {
        "sha": "7af64c4db547dbb59d930605dc79151453c1e52a",
        "author": "ThatAlexPalmer",
        "date": "2024-02-07 13:53:49+00:00",
        "message": "Add new and reachable email",
        "files": [
            {
                "filename": "src/scripts/sampleData.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,5 +1,5 @@\n export const issuer = {\n-    legal_name: \"Banana Peppers Inc.\",\n+    legal_name: \"Transfer Agent Protocol\",\n     formation_date: \"2022-08-23\",\n     country_of_formation: \"US\",\n     country_subdivision_of_formation: \"DE\",\n@@ -10,7 +10,7 @@ export const issuer = {\n         },\n     ],\n     email: {\n-        email_address: \"concierge@poet.network\",\n+        email_address: \"alex@transferagentprotocol.xyz\",\n         email_type: \"BUSINESS\",\n     },\n     initial_shares_authorized: \"10000000\","
            }
        ]
    },
    {
        "sha": "0ce4fe57f8ec0996627960e07c0d2549a8f50011",
        "author": "ThatAlexPalmer",
        "date": "2024-02-07 13:53:19+00:00",
        "message": "Add error handling for port conflicts during server startup (if it's already in use)",
        "files": [
            {
                "filename": "src/app.js",
                "additions": 18,
                "deletions": 4,
                "patch": "@@ -73,10 +73,24 @@ export const startServer = async (finalizedOnly) => {\n     // Connect to MongoDB\n     const dbConn = await connectDB();\n \n-    const server = app.listen(PORT, async () => {\n-        console.log(`\ud83d\ude80  Server successfully launched at ${PORT}`);\n-        // Asynchronous job to track web3 events in web2\n-        startEventProcessing(finalizedOnly, dbConn);\n+    const server = app\n+        .listen(PORT, async () => {\n+            console.log(`\ud83d\ude80  Server successfully launched at ${PORT}`);\n+            // Asynchronous job to track web3 events in web2\n+            startEventProcessing(finalizedOnly, dbConn);\n+        })\n+        .on(\"error\", (err) => {\n+            if (err.code === \"EADDRINUSE\") {\n+                console.log(`Port ${PORT} is already in use.`);\n+            } else {\n+                console.log(err);\n+            }\n+        });\n+\n+    server.on(\"listening\", () => {\n+        const address = server.address();\n+        const bind = typeof address === \"string\" ? \"pipe \" + address : \"port \" + address.port;\n+        console.log(\"Listening on \" + bind);\n     });\n \n     return server;"
            }
        ]
    },
    {
        "sha": "b962608d8a094a67c35e62dcd05efe1107435bbe",
        "author": "ThatAlexPalmer",
        "date": "2024-02-07 13:49:54+00:00",
        "message": "Updates deterministic factory for a new account address that's deploying it",
        "files": [
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -26,7 +26,7 @@ const allowPropagate = async () => {\n const seedExampleData = async () => {\n     const rec = await Factory.findOne();\n     if (!rec) {\n-        const deterministicFactory = \"0xDc64a140Aa3E981100a9becA4E685f962f0cF6C9\";\n+        const deterministicFactory = \"0x164BEfB302Aa6b2B8fD5485bD2A3d472683B2794\";\n         const resp = await axios.post(`${SERVER_BASE}/factory/register`, {factory_address: deterministicFactory});\n         console.log(\"Used deterministic factory address. May need to change in future\", resp.data);\n         // throw new Error("
            }
        ]
    },
    {
        "sha": "778456c47379a5c0a1f2c92c73fbfe3852a20e81",
        "author": "victormimo",
        "date": "2024-02-02 15:14:06+00:00",
        "message": "Merge pull request #148 from transfer-agent-protocol/dev\n\nDev to main",
        "files": [
            {
                "filename": ".gitignore",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -26,6 +26,7 @@ yarn-error.log*\n .env.development.local\n .env.test.local\n .env.production.local\n+.env.testnet\n \n # vercel\n .vercel"
            },
            {
                "filename": "README.md",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -97,10 +97,10 @@ yarn deploy-libraries\n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n ```sh\n-yarn deploy-factory\n+yarn deploy-factory [.env file to use]\n ```\n \n-This will deploy the factory as pointed to in the `.env`\n+This will deploy the factory as pointed to in the `.env` or the file you pass in, such as .env.local\n \n ## Running the cap table server\n "
            },
            {
                "filename": "chain/src/CapTable.sol",
                "additions": 24,
                "deletions": 8,
                "patch": "@@ -168,7 +168,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     /// @inheritdoc ICapTable\n     /// @notice Setter for walletsPerStakeholder mapping\n     /// @dev Function is separate from createStakeholder since multiple wallets will be added per stakeholder at different times.\n-    function addWalletToStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyAdmin {\n+    function addWalletToStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyOperator {\n         _checkInvalidWallet(_wallet);\n         _checkStakeholderIsStored(_stakeholder_id);\n         _checkWalletAlreadyExists(_wallet);\n@@ -178,15 +178,15 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n \n     /// @inheritdoc ICapTable\n     /// @notice Removing wallet from walletsPerStakeholder mapping\n-    function removeWalletFromStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyAdmin {\n+    function removeWalletFromStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyOperator {\n         _checkInvalidWallet(_wallet);\n         _checkStakeholderIsStored(_stakeholder_id);\n \n         delete walletsPerStakeholder[_wallet];\n     }\n \n     /// @inheritdoc ICapTable\n-    function issueStock(StockIssuanceParams calldata params) external override onlyAdmin {\n+    function issueStock(StockIssuanceParams calldata params) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -201,7 +201,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external override onlyAdmin {\n+    function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -229,7 +229,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function retractStockIssuance(StockParams calldata params) external override onlyAdmin {\n+    function retractStockIssuance(StockParams calldata params) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -247,7 +247,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function reissueStock(StockParams calldata params, bytes16[] memory resulting_security_ids) external override onlyAdmin {\n+    function reissueStock(StockParams calldata params, bytes16[] memory resulting_security_ids) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n         _checkResultingSecurityIds(resulting_security_ids, params.stakeholder_id, params.stock_class_id);\n@@ -267,7 +267,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function cancelStock(StockParams calldata params, uint256 quantity) external override onlyAdmin {\n+    function cancelStock(StockParams calldata params, uint256 quantity) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -323,7 +323,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n \n     /// @inheritdoc ICapTable\n     // Stock Acceptance does not impact an active position. It's only recorded.\n-    function acceptStock(bytes16 stakeholderId, bytes16 stockClassId, bytes16 securityId, string[] memory comments) external override onlyAdmin {\n+    function acceptStock(bytes16 stakeholderId, bytes16 stockClassId, bytes16 securityId, string[] memory comments) external override onlyOperator {\n         _checkStakeholderIsStored(stakeholderId);\n         _checkInvalidStockClass(stockClassId);\n \n@@ -429,6 +429,22 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n         return (position.stock_class_id, position.quantity, position.share_price, position.timestamp);\n     }\n \n+    /// @inheritdoc ICapTable\n+    function getAveragePosition(bytes16 stakeholderId, bytes16 stockClassId) external view returns (uint, uint, uint40) {\n+        bytes16[] memory activeSecurityIDs = activeSecs.activeSecurityIdsByStockClass[stakeholderId][stockClassId];\n+        uint quantityPrice = 0;\n+        uint quantity = 0;\n+        uint40 timestamp = 0;\n+        for (uint i = 0; i < activeSecurityIDs.length; i++) {\n+            ActivePosition storage position = positions.activePositions[stakeholderId][activeSecurityIDs[i]];\n+            // Alley-oop the web2 caller to find the avg to avoid issues with fractions\n+            quantityPrice += position.quantity * position.share_price;\n+            quantity += position.quantity;\n+            timestamp = position.timestamp > timestamp ? position.timestamp : timestamp;\n+        }\n+        return (quantityPrice, quantity, timestamp);\n+    }\n+\n     /* Role Based Access Control */\n     modifier onlyOperator() {\n         /// @notice Admins are also considered Operators"
            },
            {
                "filename": "chain/src/interfaces/ICapTable.sol",
                "additions": 4,
                "deletions": 0,
                "patch": "@@ -77,6 +77,10 @@ interface ICapTable {\n     // Function to get the timestamp of an active position\n     function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40);\n \n+    /// @notice Get the avg active position for the stakeholder by dividing the first return value (quantityPrice) by the second (quantity)\n+    ///  the timestamp is the time of the latest position\n+    function getAveragePosition(bytes16 stakeholderId, bytes16 stockClassId) external view returns (uint, uint, uint40);\n+\n     function issueStock(StockIssuanceParams calldata params) external;\n \n     function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external;"
            },
            {
                "filename": "jest.config.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -5,6 +5,7 @@\n \n // This allows us to avoid messing with the state of people's standard dev database\n process.env['DATABASE_OVERRIDE'] = 'jest-integration';\n+process.env['USE_ENV_FILE'] = '.env.test.local';\n \n /** @type {import('jest').Config} */\n const config = {"
            },
            {
                "filename": "scripts/deployFactory.sh",
                "additions": 6,
                "deletions": 2,
                "patch": "@@ -1,9 +1,13 @@\n #!/bin/bash\n \n-source .env\n+# Accept a single argument of an env file to use. By default use .env at root\n+USE_ENV_FILE=${1:-.env}\n \n+source $USE_ENV_FILE\n+\n+# Copy the root .env underneath chain so we dont have to maintain two copies\n TEMP=$PWD/chain/.env\n-cp .env $TEMP\n+cp $USE_ENV_FILE $TEMP\n trap \"rm $TEMP\" EXIT\n \n set -x"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -10,9 +10,10 @@ export const connectDB = async () => {\n     const connectOptions = DATABASE_OVERRIDE ? {dbName: DATABASE_OVERRIDE} : {};\n     try {\n         await mongoose.connect(DATABASE_URL, connectOptions);\n-        console.log(\"\u2705 | Mongo connected succesfully\");\n+        console.log(\"\u2705 | Mongo connected succesfully\", DATABASE_OVERRIDE);\n         return mongoose.connection;\n     } catch (error) {\n+        console.error(error);\n         console.error(\"\u274c | Error connecting to Mongo\", error.message);\n         // Exit process with failure\n         process.exit(1);"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 7,
                "deletions": 7,
                "patch": "@@ -13,7 +13,7 @@ capTable.get(\"/\", async (req, res) => {\n     res.send(\"Hello Cap Table!\");\n });\n \n-capTable.get(\"/latest\", async (req, res) => {\n+capTable.get(\"/holdings/stock\", async (req, res) => {\n     /* \n     TODO: handle this in the polling process? or maybe just cache it once in a while?\n      It will get slow once we have 50+ stakeholders\n@@ -64,26 +64,26 @@ capTable.get(\"/latest\", async (req, res) => {\n         const stakeholderMap = Object.fromEntries(stakeholders.map((x) => { return [x._id, x]; }));\n         const stockClassMap = Object.fromEntries(stockClasses.map((x) => { return [x._id, x]; }));\n         for (const issuance of issuances) {\n-            const { stakeholder_id, security_id, stock_class_id } = issuance;\n-            const [_, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+            const { stakeholder_id, stock_class_id } = issuance;\n+            const [quantityPrice, quantity, timestamp] = await contract.getAveragePosition(\n                 convertUUIDToBytes16(stakeholder_id),\n-                convertUUIDToBytes16(security_id),\n+                convertUUIDToBytes16(stock_class_id),\n             );\n             if (quantity == 0) {\n                 continue;\n             }\n+            const sharePrice = quantityPrice / quantity;\n             holdings.push({\n-                issuance,\n                 stockClass: stockClassMap[stock_class_id],\n                 stakeholder: stakeholderMap[stakeholder_id],\n                 quantity: Number(quantity) / decimalScaleValue,\n                 sharePrice: Number(sharePrice) / decimalScaleValue,\n-                timestamp: Number(timestamp) * 1000,\n+                timestamp: Number(timestamp),\n             });\n         }\n         res.send({ holdings, stockClasses, issuer });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n })"
            },
            {
                "filename": "src/routes/factory.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -16,7 +16,7 @@ router.post(\"/register\", async (req, res) => {\n         const factory = await upsertFactory({factory_address, implementation_address});\n         res.send({factory});\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/historicalTransactions.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -16,7 +16,7 @@ historicalTransactions.get(\"/issuer-id/:issuerId\", async (req, res) => {\n \n         res.status(200).send(historicalTransactions);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/index.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -24,7 +24,7 @@ router.post(\"/mint-cap-table\", async (req, res) => {\n         const savedIssuerWithDeployedTo = await updateIssuerById(issuer._id, { deployed_to: address });\n         res.status(200).send({ issuer: savedIssuerWithDeployedTo });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send({ error });\n     }\n });"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -23,7 +23,7 @@ issuer.get(\"/id/:id\", async (req, res) => {\n \n         res.status(200).send({ issuerId, type, role });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -33,7 +33,7 @@ issuer.get(\"/total-number\", async (req, res) => {\n         const totalIssuers = await countIssuers();\n         res.status(200).send(totalIssuers);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -71,7 +71,7 @@ issuer.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ issuer });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stakeholder.js",
                "additions": 5,
                "deletions": 5,
                "patch": "@@ -28,7 +28,7 @@ stakeholder.get(\"/id/:id\", async (req, res) => {\n \n         res.status(200).send({ stakeholderId, type, role });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -40,7 +40,7 @@ stakeholder.get(\"/total-number\", async (req, res) => {\n         const totalStakeholders = await getTotalNumberOfStakeholders(contract);\n         res.status(200).send(totalStakeholders);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -75,7 +75,7 @@ stakeholder.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stakeholder });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -88,7 +88,7 @@ stakeholder.post(\"/add-wallet\", async (req, res) => {\n         await addWalletToStakeholder(contract, id, wallet);\n         res.status(200).send(\"Success\");\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -101,7 +101,7 @@ stakeholder.post(\"/remove-wallet\", async (req, res) => {\n         await removeWalletFromStakeholder(contract, id, wallet);\n         res.status(200).send(\"Success\");\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stockClass.js",
                "additions": 4,
                "deletions": 4,
                "patch": "@@ -3,8 +3,8 @@ import { v4 as uuid } from \"uuid\";\n import stockClassSchema from \"../../ocf/schema/objects/StockClass.schema.json\" assert { type: \"json\" };\n import { convertAndReflectStockClassOnchain, getStockClassById, getTotalNumberOfStockClasses } from \"../controllers/stockClassController.js\";\n import { createStockClass } from \"../db/operations/create.js\";\n-import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n import { readIssuerById } from \"../db/operations/read.js\";\n+import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n const stockClass = Router();\n \n@@ -21,7 +21,7 @@ stockClass.get(\"/id/:id\", async (req, res) => {\n \n         res.status(200).send({ stockClassId, classType, pricePerShare, initialSharesAuthorized });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -32,7 +32,7 @@ stockClass.get(\"/total-number\", async (req, res) => {\n         const totalStockClasses = await getTotalNumberOfStockClasses(contract);\n         res.status(200).send(totalStockClasses);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -65,7 +65,7 @@ stockClass.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stockClass });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stockLegend.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import stockLegendSchema from \"../../ocf/schema/objects/StockLegendTemplate.schema.json\" assert { type: \"json\" };\n import { createStockLegendTemplate } from \"../db/operations/create.js\";\n-import { countStockLegendTemplates, readStockLegendTemplateById } from \"../db/operations/read.js\";\n+import { countStockLegendTemplates, readIssuerById, readStockLegendTemplateById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const stockLegend = Router();\n \n@@ -19,7 +18,7 @@ stockLegend.get(\"/id/:id\", async (req, res) => {\n         const stockLegend = await readStockLegendTemplateById(id);\n         res.status(200).send(stockLegend);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -29,7 +28,7 @@ stockLegend.get(\"/total-number\", async (_, res) => {\n         const totalStockLegends = await countStockLegendTemplates();\n         res.status(200).send(totalStockLegends.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -58,7 +57,7 @@ stockLegend.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stockLegend });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stockPlan.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import stockPlanSchema from \"../../ocf/schema/objects/StockPlan.schema.json\" assert { type: \"json\" };\n import { createStockPlan } from \"../db/operations/create.js\";\n-import { countStockPlans, readStockPlanById } from \"../db/operations/read.js\";\n+import { countStockPlans, readIssuerById, readStockPlanById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const stockPlan = Router();\n \n@@ -19,7 +18,7 @@ stockPlan.get(\"/id/:id\", async (req, res) => {\n         const stockPlan = await readStockPlanById(id);\n         res.status(200).send(stockPlan);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -29,7 +28,7 @@ stockPlan.get(\"/total-number\", async (_, res) => {\n         const totalStockPlans = await countStockPlans();\n         res.status(200).send(totalStockPlans.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -58,7 +57,7 @@ stockPlan.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stockPlan });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/transactions.js",
                "additions": 10,
                "deletions": 11,
                "patch": "@@ -2,15 +2,15 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n \n import stockAcceptanceSchema from \"../../ocf/schema/objects/transactions/acceptance/StockAcceptance.schema.json\" assert { type: \"json\" };\n+import issuerAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n+import stockClassAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n import stockCancellationSchema from \"../../ocf/schema/objects/transactions/cancellation/StockCancellation.schema.json\" assert { type: \"json\" };\n+import convertibleIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/ConvertibleIssuance.schema.json\" assert { type: \"json\" };\n+import equityCompensationIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/EquityCompensationIssuance.schema.json\" assert { type: \"json\" };\n import stockIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/StockIssuance.schema.json\" assert { type: \"json\" };\n import stockReissuanceSchema from \"../../ocf/schema/objects/transactions/reissuance/StockReissuance.schema.json\" assert { type: \"json\" };\n import stockRepurchaseSchema from \"../../ocf/schema/objects/transactions/repurchase/StockRepurchase.schema.json\" assert { type: \"json\" };\n import stockRetractionSchema from \"../../ocf/schema/objects/transactions/retraction/StockRetraction.schema.json\" assert { type: \"json\" };\n-import stockClassAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n-import issuerAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n-import equityCompensationIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/EquityCompensationIssuance.schema.json\" assert { type: \"json\" };\n-import convertibleIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/ConvertibleIssuance.schema.json\" assert { type: \"json\" };\n \n import { convertAndAdjustIssuerAuthorizedSharesOnChain } from \"../controllers/issuerController.js\";\n import { convertAndAdjustStockClassAuthorizedSharesOnchain } from \"../controllers/stockClassController.js\";\n@@ -21,8 +21,7 @@ import { convertAndCreateReissuanceStockOnchain } from \"../controllers/transacti\n import { convertAndCreateRepurchaseStockOnchain } from \"../controllers/transactions/repurchaseController.js\";\n import { convertAndCreateRetractionStockOnchain } from \"../controllers/transactions/retractionController.js\";\n import { convertAndCreateTransferStockOnchain } from \"../controllers/transactions/transferController.js\";\n-import { createEquityCompensationIssuance } from \"../db/operations/create.js\";\n-import { createConvertibleIssuance } from \"../db/operations/create.js\";\n+import { createConvertibleIssuance, createEquityCompensationIssuance } from \"../db/operations/create.js\";\n \n import { readIssuerById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n@@ -50,7 +49,7 @@ transactions.post(\"/issuance/stock\", async (req, res) => {\n \n         res.status(200).send({ stockIssuance: incomingStockIssuance });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -68,7 +67,7 @@ transactions.post(\"/transfer/stock\", async (req, res) => {\n \n         res.status(200).send(\"success\");\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -256,7 +255,7 @@ transactions.post(\"/adjust/issuer/authorized-shares\", async (req, res) => {\n \n         res.status(200).send({ issuerAuthorizedSharesAdj });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -312,7 +311,7 @@ transactions.post(\"/issuance/equity-compensation\", async (req, res) => {\n \n         res.status(200).send({ equityCompensationIssuance: createdIssuance });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n })\n@@ -341,7 +340,7 @@ transactions.post(\"/issuance/convertible\", async (req, res) => {\n \n         res.status(200).send({ convertibleIssuance: createdIssuance });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n })"
            },
            {
                "filename": "src/routes/valuation.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import valuationSchema from \"../../ocf/schema/objects/Valuation.schema.json\" assert { type: \"json\" };\n import { createValuation } from \"../db/operations/create.js\";\n-import { countValuations, readValuationById } from \"../db/operations/read.js\";\n+import { countValuations, readIssuerById, readValuationById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const valuation = Router();\n \n@@ -18,7 +17,7 @@ valuation.get(\"/id/:id\", async (req, res) => {\n         const valuation = await readValuationById(id);\n         res.status(200).send(valuation);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -28,7 +27,7 @@ valuation.get(\"/total-number\", async (_, res) => {\n         const totalValuations = await countValuations();\n         res.status(200).send(totalValuations.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -57,7 +56,7 @@ valuation.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ valuation });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/vestingTerms.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import vestingTermsSchema from \"../../ocf/schema/objects/VestingTerms.schema.json\" assert { type: \"json\" };\n import { createVestingTerms } from \"../db/operations/create.js\";\n-import { countVestingTerms, readVestingTermsById } from \"../db/operations/read.js\";\n+import { countVestingTerms, readIssuerById, readVestingTermsById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const vestingTerms = Router();\n \n@@ -20,7 +19,7 @@ vestingTerms.get(\"/id/:id\", async (req, res) => {\n         const vestingTerms = await readVestingTermsById(id);\n         res.status(200).send(vestingTerms);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -30,7 +29,7 @@ vestingTerms.get(\"/total-number\", async (_, res) => {\n         const totalVestingTerms = await countVestingTerms();\n         res.status(200).send(totalVestingTerms.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -59,7 +58,7 @@ vestingTerms.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ vestingTerms });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 18,
                "deletions": 9,
                "patch": "@@ -26,11 +26,13 @@ const allowPropagate = async () => {\n const seedExampleData = async () => {\n     const rec = await Factory.findOne();\n     if (!rec) {\n-        throw new Error(\n-            `Manually create the {\"implementation_address\": ..., \"factory_address\": ...} record \n-            in \"factories\" collection. Run the \"forge script ...\" command from the comment \n-            in \"chain/script/CapTableFactory.s.sol\"`\n-        );\n+        const deterministicFactory = \"0xDc64a140Aa3E981100a9becA4E685f962f0cF6C9\";\n+        const resp = await axios.post(`${SERVER_BASE}/factory/register`, {factory_address: deterministicFactory});\n+        console.log(\"Used deterministic factory address. May need to change in future\", resp.data);\n+        // throw new Error(\n+        //     `Manually create the {\"implementation_address\": ..., \"factory_address\": ...} record \n+        //     in \"factories\" collection. Use output of 'yarn deploy-factory' against a local 'anvil' server`\n+        // );\n     }\n \n     const issuerResponse = await axios.post(`${SERVER_BASE}/issuer/create`, exampleIssuer);\n@@ -92,11 +94,18 @@ const seedExampleData = async () => {\n     \n     const stockTransfer2Response = await axios.post(\n         `${SERVER_BASE}/transactions/transfer/stock`,\n-        stockTransfer(issuerId, \"300\", s1Id, s3Id, stockClassId, \"10.66\")\n+        stockTransfer(issuerId, \"125\", s1Id, s3Id, stockClassId, \"10.66\")\n     );\n     console.log(\"\u2705 | stockTransfer2Response\", stockTransfer2Response.data);\n     await allowPropagate();\n \n+    const stockTransfer3Response = await axios.post(\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n+        stockTransfer(issuerId, \"175\", s1Id, s3Id, stockClassId, \"8.42\")\n+    );\n+    console.log(\"\u2705 | stockTransfer3Response\", stockTransfer3Response.data);\n+    await allowPropagate();\n+\n     // TODO: acceptance of transfer2?\n \n     // Allow time for poller process to catch up\n@@ -106,11 +115,11 @@ const seedExampleData = async () => {\n }\n \n const checkRecs = async (issuerId) => {\n-    const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n-    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice, name: stakeholder.name.legal_name}; });\n+    const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/holdings/stock?issuerId=${issuerId}`);\n+    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice: parseFloat(sharePrice.toFixed(2)), name: stakeholder.name.legal_name}; });\n     portions.sort((a, b) => b.quantity - a.quantity);\n     expect(portions).toStrictEqual([\n-        {quantity: 300, sharePrice: 10.66, name: \"Kent Kolze\"},\n+        {quantity: 300, sharePrice: 9.35, name: \"Kent Kolze\"},\n         {quantity: 200, sharePrice: 4.2, name: \"Victor Mimo\"},\n     ]);\n }"
            },
            {
                "filename": "src/utils/env.js",
                "additions": 12,
                "deletions": 4,
                "patch": "@@ -15,25 +15,33 @@ const splitPath = (path) => {\n };\n \n \n-const getEnvFile = () => {\n+const getEnvFile = (fileName) => {\n     // Find the .env file by iterating up the PWD. However do not go past the repo root!\n     const repoRootDirName = \"tap-cap-table\"\n     const cwd = process.env.PWD\n     let { dir, rightMost } = splitPath(cwd);\n-    let check = pathTools.join(cwd, \".env\");\n+    let check = pathTools.join(cwd, fileName);\n     while (!fs.existsSync(check)) {\n         if (rightMost === repoRootDirName) {\n             throw new Error(`Unable to locate .env in ${check}`);\n         }\n         // Check our current dir\n-        check = pathTools.join(dir, \".env\");\n+        check = pathTools.join(dir, fileName);\n         // The dir to look in next\n         ({ dir, rightMost } = splitPath(dir));\n     }\n     return check;\n };\n \n+let _ALREADY_SETUP = false;\n \n export const setupEnv = () => {\n-    config({path: getEnvFile()});\n+    if (_ALREADY_SETUP) {\n+        return;\n+    }\n+    const fileName = process.env.USE_ENV_FILE || '.env';\n+    const path = getEnvFile(fileName)\n+    console.log(\"setupEnv with:\", path);\n+    config({ path });\n+    _ALREADY_SETUP = true;\n };"
            }
        ]
    },
    {
        "sha": "c2a228cef064200b35672c958c0f409ae940209a",
        "author": "victormimo",
        "date": "2024-02-02 15:13:38+00:00",
        "message": "Merge pull request #147 from transfer-agent-protocol/kkolze/missed-cap-table-fixes\n\nKkolze/missed cap table fixes",
        "files": [
            {
                "filename": ".gitignore",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -26,6 +26,7 @@ yarn-error.log*\n .env.development.local\n .env.test.local\n .env.production.local\n+.env.testnet\n \n # vercel\n .vercel"
            },
            {
                "filename": "README.md",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -97,10 +97,10 @@ yarn deploy-libraries\n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n ```sh\n-yarn deploy-factory\n+yarn deploy-factory [.env file to use]\n ```\n \n-This will deploy the factory as pointed to in the `.env`\n+This will deploy the factory as pointed to in the `.env` or the file you pass in, such as .env.local\n \n ## Running the cap table server\n "
            },
            {
                "filename": "chain/src/CapTable.sol",
                "additions": 24,
                "deletions": 8,
                "patch": "@@ -168,7 +168,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     /// @inheritdoc ICapTable\n     /// @notice Setter for walletsPerStakeholder mapping\n     /// @dev Function is separate from createStakeholder since multiple wallets will be added per stakeholder at different times.\n-    function addWalletToStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyAdmin {\n+    function addWalletToStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyOperator {\n         _checkInvalidWallet(_wallet);\n         _checkStakeholderIsStored(_stakeholder_id);\n         _checkWalletAlreadyExists(_wallet);\n@@ -178,15 +178,15 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n \n     /// @inheritdoc ICapTable\n     /// @notice Removing wallet from walletsPerStakeholder mapping\n-    function removeWalletFromStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyAdmin {\n+    function removeWalletFromStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyOperator {\n         _checkInvalidWallet(_wallet);\n         _checkStakeholderIsStored(_stakeholder_id);\n \n         delete walletsPerStakeholder[_wallet];\n     }\n \n     /// @inheritdoc ICapTable\n-    function issueStock(StockIssuanceParams calldata params) external override onlyAdmin {\n+    function issueStock(StockIssuanceParams calldata params) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -201,7 +201,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external override onlyAdmin {\n+    function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -229,7 +229,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function retractStockIssuance(StockParams calldata params) external override onlyAdmin {\n+    function retractStockIssuance(StockParams calldata params) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -247,7 +247,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function reissueStock(StockParams calldata params, bytes16[] memory resulting_security_ids) external override onlyAdmin {\n+    function reissueStock(StockParams calldata params, bytes16[] memory resulting_security_ids) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n         _checkResultingSecurityIds(resulting_security_ids, params.stakeholder_id, params.stock_class_id);\n@@ -267,7 +267,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function cancelStock(StockParams calldata params, uint256 quantity) external override onlyAdmin {\n+    function cancelStock(StockParams calldata params, uint256 quantity) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -323,7 +323,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n \n     /// @inheritdoc ICapTable\n     // Stock Acceptance does not impact an active position. It's only recorded.\n-    function acceptStock(bytes16 stakeholderId, bytes16 stockClassId, bytes16 securityId, string[] memory comments) external override onlyAdmin {\n+    function acceptStock(bytes16 stakeholderId, bytes16 stockClassId, bytes16 securityId, string[] memory comments) external override onlyOperator {\n         _checkStakeholderIsStored(stakeholderId);\n         _checkInvalidStockClass(stockClassId);\n \n@@ -429,6 +429,22 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n         return (position.stock_class_id, position.quantity, position.share_price, position.timestamp);\n     }\n \n+    /// @inheritdoc ICapTable\n+    function getAveragePosition(bytes16 stakeholderId, bytes16 stockClassId) external view returns (uint, uint, uint40) {\n+        bytes16[] memory activeSecurityIDs = activeSecs.activeSecurityIdsByStockClass[stakeholderId][stockClassId];\n+        uint quantityPrice = 0;\n+        uint quantity = 0;\n+        uint40 timestamp = 0;\n+        for (uint i = 0; i < activeSecurityIDs.length; i++) {\n+            ActivePosition storage position = positions.activePositions[stakeholderId][activeSecurityIDs[i]];\n+            // Alley-oop the web2 caller to find the avg to avoid issues with fractions\n+            quantityPrice += position.quantity * position.share_price;\n+            quantity += position.quantity;\n+            timestamp = position.timestamp > timestamp ? position.timestamp : timestamp;\n+        }\n+        return (quantityPrice, quantity, timestamp);\n+    }\n+\n     /* Role Based Access Control */\n     modifier onlyOperator() {\n         /// @notice Admins are also considered Operators"
            },
            {
                "filename": "chain/src/interfaces/ICapTable.sol",
                "additions": 4,
                "deletions": 0,
                "patch": "@@ -77,6 +77,10 @@ interface ICapTable {\n     // Function to get the timestamp of an active position\n     function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40);\n \n+    /// @notice Get the avg active position for the stakeholder by dividing the first return value (quantityPrice) by the second (quantity)\n+    ///  the timestamp is the time of the latest position\n+    function getAveragePosition(bytes16 stakeholderId, bytes16 stockClassId) external view returns (uint, uint, uint40);\n+\n     function issueStock(StockIssuanceParams calldata params) external;\n \n     function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external;"
            },
            {
                "filename": "jest.config.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -5,6 +5,7 @@\n \n // This allows us to avoid messing with the state of people's standard dev database\n process.env['DATABASE_OVERRIDE'] = 'jest-integration';\n+process.env['USE_ENV_FILE'] = '.env.test.local';\n \n /** @type {import('jest').Config} */\n const config = {"
            },
            {
                "filename": "scripts/deployFactory.sh",
                "additions": 6,
                "deletions": 2,
                "patch": "@@ -1,9 +1,13 @@\n #!/bin/bash\n \n-source .env\n+# Accept a single argument of an env file to use. By default use .env at root\n+USE_ENV_FILE=${1:-.env}\n \n+source $USE_ENV_FILE\n+\n+# Copy the root .env underneath chain so we dont have to maintain two copies\n TEMP=$PWD/chain/.env\n-cp .env $TEMP\n+cp $USE_ENV_FILE $TEMP\n trap \"rm $TEMP\" EXIT\n \n set -x"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -10,9 +10,10 @@ export const connectDB = async () => {\n     const connectOptions = DATABASE_OVERRIDE ? {dbName: DATABASE_OVERRIDE} : {};\n     try {\n         await mongoose.connect(DATABASE_URL, connectOptions);\n-        console.log(\"\u2705 | Mongo connected succesfully\");\n+        console.log(\"\u2705 | Mongo connected succesfully\", DATABASE_OVERRIDE);\n         return mongoose.connection;\n     } catch (error) {\n+        console.error(error);\n         console.error(\"\u274c | Error connecting to Mongo\", error.message);\n         // Exit process with failure\n         process.exit(1);"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 7,
                "deletions": 7,
                "patch": "@@ -13,7 +13,7 @@ capTable.get(\"/\", async (req, res) => {\n     res.send(\"Hello Cap Table!\");\n });\n \n-capTable.get(\"/latest\", async (req, res) => {\n+capTable.get(\"/holdings/stock\", async (req, res) => {\n     /* \n     TODO: handle this in the polling process? or maybe just cache it once in a while?\n      It will get slow once we have 50+ stakeholders\n@@ -64,26 +64,26 @@ capTable.get(\"/latest\", async (req, res) => {\n         const stakeholderMap = Object.fromEntries(stakeholders.map((x) => { return [x._id, x]; }));\n         const stockClassMap = Object.fromEntries(stockClasses.map((x) => { return [x._id, x]; }));\n         for (const issuance of issuances) {\n-            const { stakeholder_id, security_id, stock_class_id } = issuance;\n-            const [_, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+            const { stakeholder_id, stock_class_id } = issuance;\n+            const [quantityPrice, quantity, timestamp] = await contract.getAveragePosition(\n                 convertUUIDToBytes16(stakeholder_id),\n-                convertUUIDToBytes16(security_id),\n+                convertUUIDToBytes16(stock_class_id),\n             );\n             if (quantity == 0) {\n                 continue;\n             }\n+            const sharePrice = quantityPrice / quantity;\n             holdings.push({\n-                issuance,\n                 stockClass: stockClassMap[stock_class_id],\n                 stakeholder: stakeholderMap[stakeholder_id],\n                 quantity: Number(quantity) / decimalScaleValue,\n                 sharePrice: Number(sharePrice) / decimalScaleValue,\n-                timestamp: Number(timestamp) * 1000,\n+                timestamp: Number(timestamp),\n             });\n         }\n         res.send({ holdings, stockClasses, issuer });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n })"
            },
            {
                "filename": "src/routes/factory.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -16,7 +16,7 @@ router.post(\"/register\", async (req, res) => {\n         const factory = await upsertFactory({factory_address, implementation_address});\n         res.send({factory});\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/historicalTransactions.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -16,7 +16,7 @@ historicalTransactions.get(\"/issuer-id/:issuerId\", async (req, res) => {\n \n         res.status(200).send(historicalTransactions);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/index.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -24,7 +24,7 @@ router.post(\"/mint-cap-table\", async (req, res) => {\n         const savedIssuerWithDeployedTo = await updateIssuerById(issuer._id, { deployed_to: address });\n         res.status(200).send({ issuer: savedIssuerWithDeployedTo });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send({ error });\n     }\n });"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -23,7 +23,7 @@ issuer.get(\"/id/:id\", async (req, res) => {\n \n         res.status(200).send({ issuerId, type, role });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -33,7 +33,7 @@ issuer.get(\"/total-number\", async (req, res) => {\n         const totalIssuers = await countIssuers();\n         res.status(200).send(totalIssuers);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -71,7 +71,7 @@ issuer.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ issuer });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stakeholder.js",
                "additions": 5,
                "deletions": 5,
                "patch": "@@ -28,7 +28,7 @@ stakeholder.get(\"/id/:id\", async (req, res) => {\n \n         res.status(200).send({ stakeholderId, type, role });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -40,7 +40,7 @@ stakeholder.get(\"/total-number\", async (req, res) => {\n         const totalStakeholders = await getTotalNumberOfStakeholders(contract);\n         res.status(200).send(totalStakeholders);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -75,7 +75,7 @@ stakeholder.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stakeholder });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -88,7 +88,7 @@ stakeholder.post(\"/add-wallet\", async (req, res) => {\n         await addWalletToStakeholder(contract, id, wallet);\n         res.status(200).send(\"Success\");\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -101,7 +101,7 @@ stakeholder.post(\"/remove-wallet\", async (req, res) => {\n         await removeWalletFromStakeholder(contract, id, wallet);\n         res.status(200).send(\"Success\");\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stockClass.js",
                "additions": 4,
                "deletions": 4,
                "patch": "@@ -3,8 +3,8 @@ import { v4 as uuid } from \"uuid\";\n import stockClassSchema from \"../../ocf/schema/objects/StockClass.schema.json\" assert { type: \"json\" };\n import { convertAndReflectStockClassOnchain, getStockClassById, getTotalNumberOfStockClasses } from \"../controllers/stockClassController.js\";\n import { createStockClass } from \"../db/operations/create.js\";\n-import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n import { readIssuerById } from \"../db/operations/read.js\";\n+import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n const stockClass = Router();\n \n@@ -21,7 +21,7 @@ stockClass.get(\"/id/:id\", async (req, res) => {\n \n         res.status(200).send({ stockClassId, classType, pricePerShare, initialSharesAuthorized });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -32,7 +32,7 @@ stockClass.get(\"/total-number\", async (req, res) => {\n         const totalStockClasses = await getTotalNumberOfStockClasses(contract);\n         res.status(200).send(totalStockClasses);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -65,7 +65,7 @@ stockClass.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stockClass });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stockLegend.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import stockLegendSchema from \"../../ocf/schema/objects/StockLegendTemplate.schema.json\" assert { type: \"json\" };\n import { createStockLegendTemplate } from \"../db/operations/create.js\";\n-import { countStockLegendTemplates, readStockLegendTemplateById } from \"../db/operations/read.js\";\n+import { countStockLegendTemplates, readIssuerById, readStockLegendTemplateById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const stockLegend = Router();\n \n@@ -19,7 +18,7 @@ stockLegend.get(\"/id/:id\", async (req, res) => {\n         const stockLegend = await readStockLegendTemplateById(id);\n         res.status(200).send(stockLegend);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -29,7 +28,7 @@ stockLegend.get(\"/total-number\", async (_, res) => {\n         const totalStockLegends = await countStockLegendTemplates();\n         res.status(200).send(totalStockLegends.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -58,7 +57,7 @@ stockLegend.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stockLegend });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stockPlan.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import stockPlanSchema from \"../../ocf/schema/objects/StockPlan.schema.json\" assert { type: \"json\" };\n import { createStockPlan } from \"../db/operations/create.js\";\n-import { countStockPlans, readStockPlanById } from \"../db/operations/read.js\";\n+import { countStockPlans, readIssuerById, readStockPlanById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const stockPlan = Router();\n \n@@ -19,7 +18,7 @@ stockPlan.get(\"/id/:id\", async (req, res) => {\n         const stockPlan = await readStockPlanById(id);\n         res.status(200).send(stockPlan);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -29,7 +28,7 @@ stockPlan.get(\"/total-number\", async (_, res) => {\n         const totalStockPlans = await countStockPlans();\n         res.status(200).send(totalStockPlans.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -58,7 +57,7 @@ stockPlan.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stockPlan });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/transactions.js",
                "additions": 10,
                "deletions": 11,
                "patch": "@@ -2,15 +2,15 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n \n import stockAcceptanceSchema from \"../../ocf/schema/objects/transactions/acceptance/StockAcceptance.schema.json\" assert { type: \"json\" };\n+import issuerAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n+import stockClassAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n import stockCancellationSchema from \"../../ocf/schema/objects/transactions/cancellation/StockCancellation.schema.json\" assert { type: \"json\" };\n+import convertibleIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/ConvertibleIssuance.schema.json\" assert { type: \"json\" };\n+import equityCompensationIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/EquityCompensationIssuance.schema.json\" assert { type: \"json\" };\n import stockIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/StockIssuance.schema.json\" assert { type: \"json\" };\n import stockReissuanceSchema from \"../../ocf/schema/objects/transactions/reissuance/StockReissuance.schema.json\" assert { type: \"json\" };\n import stockRepurchaseSchema from \"../../ocf/schema/objects/transactions/repurchase/StockRepurchase.schema.json\" assert { type: \"json\" };\n import stockRetractionSchema from \"../../ocf/schema/objects/transactions/retraction/StockRetraction.schema.json\" assert { type: \"json\" };\n-import stockClassAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n-import issuerAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n-import equityCompensationIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/EquityCompensationIssuance.schema.json\" assert { type: \"json\" };\n-import convertibleIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/ConvertibleIssuance.schema.json\" assert { type: \"json\" };\n \n import { convertAndAdjustIssuerAuthorizedSharesOnChain } from \"../controllers/issuerController.js\";\n import { convertAndAdjustStockClassAuthorizedSharesOnchain } from \"../controllers/stockClassController.js\";\n@@ -21,8 +21,7 @@ import { convertAndCreateReissuanceStockOnchain } from \"../controllers/transacti\n import { convertAndCreateRepurchaseStockOnchain } from \"../controllers/transactions/repurchaseController.js\";\n import { convertAndCreateRetractionStockOnchain } from \"../controllers/transactions/retractionController.js\";\n import { convertAndCreateTransferStockOnchain } from \"../controllers/transactions/transferController.js\";\n-import { createEquityCompensationIssuance } from \"../db/operations/create.js\";\n-import { createConvertibleIssuance } from \"../db/operations/create.js\";\n+import { createConvertibleIssuance, createEquityCompensationIssuance } from \"../db/operations/create.js\";\n \n import { readIssuerById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n@@ -50,7 +49,7 @@ transactions.post(\"/issuance/stock\", async (req, res) => {\n \n         res.status(200).send({ stockIssuance: incomingStockIssuance });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -68,7 +67,7 @@ transactions.post(\"/transfer/stock\", async (req, res) => {\n \n         res.status(200).send(\"success\");\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -256,7 +255,7 @@ transactions.post(\"/adjust/issuer/authorized-shares\", async (req, res) => {\n \n         res.status(200).send({ issuerAuthorizedSharesAdj });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -312,7 +311,7 @@ transactions.post(\"/issuance/equity-compensation\", async (req, res) => {\n \n         res.status(200).send({ equityCompensationIssuance: createdIssuance });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n })\n@@ -341,7 +340,7 @@ transactions.post(\"/issuance/convertible\", async (req, res) => {\n \n         res.status(200).send({ convertibleIssuance: createdIssuance });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n })"
            },
            {
                "filename": "src/routes/valuation.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import valuationSchema from \"../../ocf/schema/objects/Valuation.schema.json\" assert { type: \"json\" };\n import { createValuation } from \"../db/operations/create.js\";\n-import { countValuations, readValuationById } from \"../db/operations/read.js\";\n+import { countValuations, readIssuerById, readValuationById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const valuation = Router();\n \n@@ -18,7 +17,7 @@ valuation.get(\"/id/:id\", async (req, res) => {\n         const valuation = await readValuationById(id);\n         res.status(200).send(valuation);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -28,7 +27,7 @@ valuation.get(\"/total-number\", async (_, res) => {\n         const totalValuations = await countValuations();\n         res.status(200).send(totalValuations.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -57,7 +56,7 @@ valuation.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ valuation });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/vestingTerms.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import vestingTermsSchema from \"../../ocf/schema/objects/VestingTerms.schema.json\" assert { type: \"json\" };\n import { createVestingTerms } from \"../db/operations/create.js\";\n-import { countVestingTerms, readVestingTermsById } from \"../db/operations/read.js\";\n+import { countVestingTerms, readIssuerById, readVestingTermsById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const vestingTerms = Router();\n \n@@ -20,7 +19,7 @@ vestingTerms.get(\"/id/:id\", async (req, res) => {\n         const vestingTerms = await readVestingTermsById(id);\n         res.status(200).send(vestingTerms);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -30,7 +29,7 @@ vestingTerms.get(\"/total-number\", async (_, res) => {\n         const totalVestingTerms = await countVestingTerms();\n         res.status(200).send(totalVestingTerms.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -59,7 +58,7 @@ vestingTerms.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ vestingTerms });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 18,
                "deletions": 9,
                "patch": "@@ -26,11 +26,13 @@ const allowPropagate = async () => {\n const seedExampleData = async () => {\n     const rec = await Factory.findOne();\n     if (!rec) {\n-        throw new Error(\n-            `Manually create the {\"implementation_address\": ..., \"factory_address\": ...} record \n-            in \"factories\" collection. Run the \"forge script ...\" command from the comment \n-            in \"chain/script/CapTableFactory.s.sol\"`\n-        );\n+        const deterministicFactory = \"0xDc64a140Aa3E981100a9becA4E685f962f0cF6C9\";\n+        const resp = await axios.post(`${SERVER_BASE}/factory/register`, {factory_address: deterministicFactory});\n+        console.log(\"Used deterministic factory address. May need to change in future\", resp.data);\n+        // throw new Error(\n+        //     `Manually create the {\"implementation_address\": ..., \"factory_address\": ...} record \n+        //     in \"factories\" collection. Use output of 'yarn deploy-factory' against a local 'anvil' server`\n+        // );\n     }\n \n     const issuerResponse = await axios.post(`${SERVER_BASE}/issuer/create`, exampleIssuer);\n@@ -92,11 +94,18 @@ const seedExampleData = async () => {\n     \n     const stockTransfer2Response = await axios.post(\n         `${SERVER_BASE}/transactions/transfer/stock`,\n-        stockTransfer(issuerId, \"300\", s1Id, s3Id, stockClassId, \"10.66\")\n+        stockTransfer(issuerId, \"125\", s1Id, s3Id, stockClassId, \"10.66\")\n     );\n     console.log(\"\u2705 | stockTransfer2Response\", stockTransfer2Response.data);\n     await allowPropagate();\n \n+    const stockTransfer3Response = await axios.post(\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n+        stockTransfer(issuerId, \"175\", s1Id, s3Id, stockClassId, \"8.42\")\n+    );\n+    console.log(\"\u2705 | stockTransfer3Response\", stockTransfer3Response.data);\n+    await allowPropagate();\n+\n     // TODO: acceptance of transfer2?\n \n     // Allow time for poller process to catch up\n@@ -106,11 +115,11 @@ const seedExampleData = async () => {\n }\n \n const checkRecs = async (issuerId) => {\n-    const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n-    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice, name: stakeholder.name.legal_name}; });\n+    const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/holdings/stock?issuerId=${issuerId}`);\n+    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice: parseFloat(sharePrice.toFixed(2)), name: stakeholder.name.legal_name}; });\n     portions.sort((a, b) => b.quantity - a.quantity);\n     expect(portions).toStrictEqual([\n-        {quantity: 300, sharePrice: 10.66, name: \"Kent Kolze\"},\n+        {quantity: 300, sharePrice: 9.35, name: \"Kent Kolze\"},\n         {quantity: 200, sharePrice: 4.2, name: \"Victor Mimo\"},\n     ]);\n }"
            },
            {
                "filename": "src/utils/env.js",
                "additions": 12,
                "deletions": 4,
                "patch": "@@ -15,25 +15,33 @@ const splitPath = (path) => {\n };\n \n \n-const getEnvFile = () => {\n+const getEnvFile = (fileName) => {\n     // Find the .env file by iterating up the PWD. However do not go past the repo root!\n     const repoRootDirName = \"tap-cap-table\"\n     const cwd = process.env.PWD\n     let { dir, rightMost } = splitPath(cwd);\n-    let check = pathTools.join(cwd, \".env\");\n+    let check = pathTools.join(cwd, fileName);\n     while (!fs.existsSync(check)) {\n         if (rightMost === repoRootDirName) {\n             throw new Error(`Unable to locate .env in ${check}`);\n         }\n         // Check our current dir\n-        check = pathTools.join(dir, \".env\");\n+        check = pathTools.join(dir, fileName);\n         // The dir to look in next\n         ({ dir, rightMost } = splitPath(dir));\n     }\n     return check;\n };\n \n+let _ALREADY_SETUP = false;\n \n export const setupEnv = () => {\n-    config({path: getEnvFile()});\n+    if (_ALREADY_SETUP) {\n+        return;\n+    }\n+    const fileName = process.env.USE_ENV_FILE || '.env';\n+    const path = getEnvFile(fileName)\n+    console.log(\"setupEnv with:\", path);\n+    config({ path });\n+    _ALREADY_SETUP = true;\n };"
            }
        ]
    },
    {
        "sha": "dfd9440b285fff9e77e931d09c590bf5f042d23f",
        "author": "kentkolze",
        "date": "2024-01-31 19:07:20+00:00",
        "message": "dont multiply by 1000",
        "files": [
            {
                "filename": "src/routes/capTable.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -78,7 +78,7 @@ capTable.get(\"/holdings/stock\", async (req, res) => {\n                 stakeholder: stakeholderMap[stakeholder_id],\n                 quantity: Number(quantity) / decimalScaleValue,\n                 sharePrice: Number(sharePrice) / decimalScaleValue,\n-                timestamp: Number(timestamp) * 1000,\n+                timestamp: Number(timestamp),\n             });\n         }\n         res.send({ holdings, stockClasses, issuer });"
            }
        ]
    },
    {
        "sha": "a05bb007c78b88eb3095fe517359c2b43d194851",
        "author": "kentkolze",
        "date": "2024-01-31 16:45:15+00:00",
        "message": "missed checkin to convert back to float",
        "files": [
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -116,7 +116,7 @@ const seedExampleData = async () => {\n \n const checkRecs = async (issuerId) => {\n     const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/holdings/stock?issuerId=${issuerId}`);\n-    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice: sharePrice.toFixed(2), name: stakeholder.name.legal_name}; });\n+    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice: parseFloat(sharePrice.toFixed(2)), name: stakeholder.name.legal_name}; });\n     portions.sort((a, b) => b.quantity - a.quantity);\n     expect(portions).toStrictEqual([\n         {quantity: 300, sharePrice: 9.35, name: \"Kent Kolze\"},"
            }
        ]
    },
    {
        "sha": "e7bcef4a718797d7308afc748d69ee8386ddf779",
        "author": "kentkolze",
        "date": "2024-01-31 16:40:35+00:00",
        "message": "ensure operators can call key captable functions\nadd getAveragePosition\nfix cap table latest positions",
        "files": [
            {
                "filename": "chain/src/CapTable.sol",
                "additions": 24,
                "deletions": 8,
                "patch": "@@ -168,7 +168,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     /// @inheritdoc ICapTable\n     /// @notice Setter for walletsPerStakeholder mapping\n     /// @dev Function is separate from createStakeholder since multiple wallets will be added per stakeholder at different times.\n-    function addWalletToStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyAdmin {\n+    function addWalletToStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyOperator {\n         _checkInvalidWallet(_wallet);\n         _checkStakeholderIsStored(_stakeholder_id);\n         _checkWalletAlreadyExists(_wallet);\n@@ -178,15 +178,15 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n \n     /// @inheritdoc ICapTable\n     /// @notice Removing wallet from walletsPerStakeholder mapping\n-    function removeWalletFromStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyAdmin {\n+    function removeWalletFromStakeholder(bytes16 _stakeholder_id, address _wallet) external override onlyOperator {\n         _checkInvalidWallet(_wallet);\n         _checkStakeholderIsStored(_stakeholder_id);\n \n         delete walletsPerStakeholder[_wallet];\n     }\n \n     /// @inheritdoc ICapTable\n-    function issueStock(StockIssuanceParams calldata params) external override onlyAdmin {\n+    function issueStock(StockIssuanceParams calldata params) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -201,7 +201,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external override onlyAdmin {\n+    function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -229,7 +229,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function retractStockIssuance(StockParams calldata params) external override onlyAdmin {\n+    function retractStockIssuance(StockParams calldata params) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -247,7 +247,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function reissueStock(StockParams calldata params, bytes16[] memory resulting_security_ids) external override onlyAdmin {\n+    function reissueStock(StockParams calldata params, bytes16[] memory resulting_security_ids) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n         _checkResultingSecurityIds(resulting_security_ids, params.stakeholder_id, params.stock_class_id);\n@@ -267,7 +267,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     }\n \n     /// @inheritdoc ICapTable\n-    function cancelStock(StockParams calldata params, uint256 quantity) external override onlyAdmin {\n+    function cancelStock(StockParams calldata params, uint256 quantity) external override onlyOperator {\n         _checkStakeholderIsStored(params.stakeholder_id);\n         _checkInvalidStockClass(params.stock_class_id);\n \n@@ -323,7 +323,7 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n \n     /// @inheritdoc ICapTable\n     // Stock Acceptance does not impact an active position. It's only recorded.\n-    function acceptStock(bytes16 stakeholderId, bytes16 stockClassId, bytes16 securityId, string[] memory comments) external override onlyAdmin {\n+    function acceptStock(bytes16 stakeholderId, bytes16 stockClassId, bytes16 securityId, string[] memory comments) external override onlyOperator {\n         _checkStakeholderIsStored(stakeholderId);\n         _checkInvalidStockClass(stockClassId);\n \n@@ -429,6 +429,22 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n         return (position.stock_class_id, position.quantity, position.share_price, position.timestamp);\n     }\n \n+    /// @inheritdoc ICapTable\n+    function getAveragePosition(bytes16 stakeholderId, bytes16 stockClassId) external view returns (uint, uint, uint40) {\n+        bytes16[] memory activeSecurityIDs = activeSecs.activeSecurityIdsByStockClass[stakeholderId][stockClassId];\n+        uint quantityPrice = 0;\n+        uint quantity = 0;\n+        uint40 timestamp = 0;\n+        for (uint i = 0; i < activeSecurityIDs.length; i++) {\n+            ActivePosition storage position = positions.activePositions[stakeholderId][activeSecurityIDs[i]];\n+            // Alley-oop the web2 caller to find the avg to avoid issues with fractions\n+            quantityPrice += position.quantity * position.share_price;\n+            quantity += position.quantity;\n+            timestamp = position.timestamp > timestamp ? position.timestamp : timestamp;\n+        }\n+        return (quantityPrice, quantity, timestamp);\n+    }\n+\n     /* Role Based Access Control */\n     modifier onlyOperator() {\n         /// @notice Admins are also considered Operators"
            },
            {
                "filename": "chain/src/interfaces/ICapTable.sol",
                "additions": 4,
                "deletions": 0,
                "patch": "@@ -77,6 +77,10 @@ interface ICapTable {\n     // Function to get the timestamp of an active position\n     function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40);\n \n+    /// @notice Get the avg active position for the stakeholder by dividing the first return value (quantityPrice) by the second (quantity)\n+    ///  the timestamp is the time of the latest position\n+    function getAveragePosition(bytes16 stakeholderId, bytes16 stockClassId) external view returns (uint, uint, uint40);\n+\n     function issueStock(StockIssuanceParams calldata params) external;\n \n     function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external;"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -10,9 +10,10 @@ export const connectDB = async () => {\n     const connectOptions = DATABASE_OVERRIDE ? {dbName: DATABASE_OVERRIDE} : {};\n     try {\n         await mongoose.connect(DATABASE_URL, connectOptions);\n-        console.log(\"\u2705 | Mongo connected succesfully\");\n+        console.log(\"\u2705 | Mongo connected succesfully\", DATABASE_OVERRIDE);\n         return mongoose.connection;\n     } catch (error) {\n+        console.error(error);\n         console.error(\"\u274c | Error connecting to Mongo\", error.message);\n         // Exit process with failure\n         process.exit(1);"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 6,
                "deletions": 6,
                "patch": "@@ -13,7 +13,7 @@ capTable.get(\"/\", async (req, res) => {\n     res.send(\"Hello Cap Table!\");\n });\n \n-capTable.get(\"/latest\", async (req, res) => {\n+capTable.get(\"/holdings/stock\", async (req, res) => {\n     /* \n     TODO: handle this in the polling process? or maybe just cache it once in a while?\n      It will get slow once we have 50+ stakeholders\n@@ -64,16 +64,16 @@ capTable.get(\"/latest\", async (req, res) => {\n         const stakeholderMap = Object.fromEntries(stakeholders.map((x) => { return [x._id, x]; }));\n         const stockClassMap = Object.fromEntries(stockClasses.map((x) => { return [x._id, x]; }));\n         for (const issuance of issuances) {\n-            const { stakeholder_id, security_id, stock_class_id } = issuance;\n-            const [_, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+            const { stakeholder_id, stock_class_id } = issuance;\n+            const [quantityPrice, quantity, timestamp] = await contract.getAveragePosition(\n                 convertUUIDToBytes16(stakeholder_id),\n-                convertUUIDToBytes16(security_id),\n+                convertUUIDToBytes16(stock_class_id),\n             );\n             if (quantity == 0) {\n                 continue;\n             }\n+            const sharePrice = quantityPrice / quantity;\n             holdings.push({\n-                issuance,\n                 stockClass: stockClassMap[stock_class_id],\n                 stakeholder: stakeholderMap[stakeholder_id],\n                 quantity: Number(quantity) / decimalScaleValue,\n@@ -83,7 +83,7 @@ capTable.get(\"/latest\", async (req, res) => {\n         }\n         res.send({ holdings, stockClasses, issuer });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n })"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 18,
                "deletions": 9,
                "patch": "@@ -26,11 +26,13 @@ const allowPropagate = async () => {\n const seedExampleData = async () => {\n     const rec = await Factory.findOne();\n     if (!rec) {\n-        throw new Error(\n-            `Manually create the {\"implementation_address\": ..., \"factory_address\": ...} record \n-            in \"factories\" collection. Run the \"forge script ...\" command from the comment \n-            in \"chain/script/CapTableFactory.s.sol\"`\n-        );\n+        const deterministicFactory = \"0xDc64a140Aa3E981100a9becA4E685f962f0cF6C9\";\n+        const resp = await axios.post(`${SERVER_BASE}/factory/register`, {factory_address: deterministicFactory});\n+        console.log(\"Used deterministic factory address. May need to change in future\", resp.data);\n+        // throw new Error(\n+        //     `Manually create the {\"implementation_address\": ..., \"factory_address\": ...} record \n+        //     in \"factories\" collection. Use output of 'yarn deploy-factory' against a local 'anvil' server`\n+        // );\n     }\n \n     const issuerResponse = await axios.post(`${SERVER_BASE}/issuer/create`, exampleIssuer);\n@@ -92,11 +94,18 @@ const seedExampleData = async () => {\n     \n     const stockTransfer2Response = await axios.post(\n         `${SERVER_BASE}/transactions/transfer/stock`,\n-        stockTransfer(issuerId, \"300\", s1Id, s3Id, stockClassId, \"10.66\")\n+        stockTransfer(issuerId, \"125\", s1Id, s3Id, stockClassId, \"10.66\")\n     );\n     console.log(\"\u2705 | stockTransfer2Response\", stockTransfer2Response.data);\n     await allowPropagate();\n \n+    const stockTransfer3Response = await axios.post(\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n+        stockTransfer(issuerId, \"175\", s1Id, s3Id, stockClassId, \"8.42\")\n+    );\n+    console.log(\"\u2705 | stockTransfer3Response\", stockTransfer3Response.data);\n+    await allowPropagate();\n+\n     // TODO: acceptance of transfer2?\n \n     // Allow time for poller process to catch up\n@@ -106,11 +115,11 @@ const seedExampleData = async () => {\n }\n \n const checkRecs = async (issuerId) => {\n-    const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n-    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice, name: stakeholder.name.legal_name}; });\n+    const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/holdings/stock?issuerId=${issuerId}`);\n+    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice: sharePrice.toFixed(2), name: stakeholder.name.legal_name}; });\n     portions.sort((a, b) => b.quantity - a.quantity);\n     expect(portions).toStrictEqual([\n-        {quantity: 300, sharePrice: 10.66, name: \"Kent Kolze\"},\n+        {quantity: 300, sharePrice: 9.35, name: \"Kent Kolze\"},\n         {quantity: 200, sharePrice: 4.2, name: \"Victor Mimo\"},\n     ]);\n }"
            }
        ]
    },
    {
        "sha": "b2b9477bc6f01bfcf85318cf4cd3a983835228cc",
        "author": "kentkolze",
        "date": "2024-01-31 16:39:04+00:00",
        "message": "properly log the stack trace on unhandled exceptions in server",
        "files": [
            {
                "filename": "src/routes/factory.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -16,7 +16,7 @@ router.post(\"/register\", async (req, res) => {\n         const factory = await upsertFactory({factory_address, implementation_address});\n         res.send({factory});\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/historicalTransactions.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -16,7 +16,7 @@ historicalTransactions.get(\"/issuer-id/:issuerId\", async (req, res) => {\n \n         res.status(200).send(historicalTransactions);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/index.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -24,7 +24,7 @@ router.post(\"/mint-cap-table\", async (req, res) => {\n         const savedIssuerWithDeployedTo = await updateIssuerById(issuer._id, { deployed_to: address });\n         res.status(200).send({ issuer: savedIssuerWithDeployedTo });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send({ error });\n     }\n });"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -23,7 +23,7 @@ issuer.get(\"/id/:id\", async (req, res) => {\n \n         res.status(200).send({ issuerId, type, role });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -33,7 +33,7 @@ issuer.get(\"/total-number\", async (req, res) => {\n         const totalIssuers = await countIssuers();\n         res.status(200).send(totalIssuers);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -71,7 +71,7 @@ issuer.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ issuer });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stakeholder.js",
                "additions": 5,
                "deletions": 5,
                "patch": "@@ -28,7 +28,7 @@ stakeholder.get(\"/id/:id\", async (req, res) => {\n \n         res.status(200).send({ stakeholderId, type, role });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -40,7 +40,7 @@ stakeholder.get(\"/total-number\", async (req, res) => {\n         const totalStakeholders = await getTotalNumberOfStakeholders(contract);\n         res.status(200).send(totalStakeholders);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -75,7 +75,7 @@ stakeholder.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stakeholder });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -88,7 +88,7 @@ stakeholder.post(\"/add-wallet\", async (req, res) => {\n         await addWalletToStakeholder(contract, id, wallet);\n         res.status(200).send(\"Success\");\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -101,7 +101,7 @@ stakeholder.post(\"/remove-wallet\", async (req, res) => {\n         await removeWalletFromStakeholder(contract, id, wallet);\n         res.status(200).send(\"Success\");\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stockClass.js",
                "additions": 4,
                "deletions": 4,
                "patch": "@@ -3,8 +3,8 @@ import { v4 as uuid } from \"uuid\";\n import stockClassSchema from \"../../ocf/schema/objects/StockClass.schema.json\" assert { type: \"json\" };\n import { convertAndReflectStockClassOnchain, getStockClassById, getTotalNumberOfStockClasses } from \"../controllers/stockClassController.js\";\n import { createStockClass } from \"../db/operations/create.js\";\n-import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n import { readIssuerById } from \"../db/operations/read.js\";\n+import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n const stockClass = Router();\n \n@@ -21,7 +21,7 @@ stockClass.get(\"/id/:id\", async (req, res) => {\n \n         res.status(200).send({ stockClassId, classType, pricePerShare, initialSharesAuthorized });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -32,7 +32,7 @@ stockClass.get(\"/total-number\", async (req, res) => {\n         const totalStockClasses = await getTotalNumberOfStockClasses(contract);\n         res.status(200).send(totalStockClasses);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -65,7 +65,7 @@ stockClass.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stockClass });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stockLegend.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import stockLegendSchema from \"../../ocf/schema/objects/StockLegendTemplate.schema.json\" assert { type: \"json\" };\n import { createStockLegendTemplate } from \"../db/operations/create.js\";\n-import { countStockLegendTemplates, readStockLegendTemplateById } from \"../db/operations/read.js\";\n+import { countStockLegendTemplates, readIssuerById, readStockLegendTemplateById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const stockLegend = Router();\n \n@@ -19,7 +18,7 @@ stockLegend.get(\"/id/:id\", async (req, res) => {\n         const stockLegend = await readStockLegendTemplateById(id);\n         res.status(200).send(stockLegend);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -29,7 +28,7 @@ stockLegend.get(\"/total-number\", async (_, res) => {\n         const totalStockLegends = await countStockLegendTemplates();\n         res.status(200).send(totalStockLegends.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -58,7 +57,7 @@ stockLegend.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stockLegend });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/stockPlan.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import stockPlanSchema from \"../../ocf/schema/objects/StockPlan.schema.json\" assert { type: \"json\" };\n import { createStockPlan } from \"../db/operations/create.js\";\n-import { countStockPlans, readStockPlanById } from \"../db/operations/read.js\";\n+import { countStockPlans, readIssuerById, readStockPlanById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const stockPlan = Router();\n \n@@ -19,7 +18,7 @@ stockPlan.get(\"/id/:id\", async (req, res) => {\n         const stockPlan = await readStockPlanById(id);\n         res.status(200).send(stockPlan);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -29,7 +28,7 @@ stockPlan.get(\"/total-number\", async (_, res) => {\n         const totalStockPlans = await countStockPlans();\n         res.status(200).send(totalStockPlans.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -58,7 +57,7 @@ stockPlan.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ stockPlan });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/transactions.js",
                "additions": 10,
                "deletions": 11,
                "patch": "@@ -2,15 +2,15 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n \n import stockAcceptanceSchema from \"../../ocf/schema/objects/transactions/acceptance/StockAcceptance.schema.json\" assert { type: \"json\" };\n+import issuerAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n+import stockClassAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n import stockCancellationSchema from \"../../ocf/schema/objects/transactions/cancellation/StockCancellation.schema.json\" assert { type: \"json\" };\n+import convertibleIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/ConvertibleIssuance.schema.json\" assert { type: \"json\" };\n+import equityCompensationIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/EquityCompensationIssuance.schema.json\" assert { type: \"json\" };\n import stockIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/StockIssuance.schema.json\" assert { type: \"json\" };\n import stockReissuanceSchema from \"../../ocf/schema/objects/transactions/reissuance/StockReissuance.schema.json\" assert { type: \"json\" };\n import stockRepurchaseSchema from \"../../ocf/schema/objects/transactions/repurchase/StockRepurchase.schema.json\" assert { type: \"json\" };\n import stockRetractionSchema from \"../../ocf/schema/objects/transactions/retraction/StockRetraction.schema.json\" assert { type: \"json\" };\n-import stockClassAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n-import issuerAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n-import equityCompensationIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/EquityCompensationIssuance.schema.json\" assert { type: \"json\" };\n-import convertibleIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/ConvertibleIssuance.schema.json\" assert { type: \"json\" };\n \n import { convertAndAdjustIssuerAuthorizedSharesOnChain } from \"../controllers/issuerController.js\";\n import { convertAndAdjustStockClassAuthorizedSharesOnchain } from \"../controllers/stockClassController.js\";\n@@ -21,8 +21,7 @@ import { convertAndCreateReissuanceStockOnchain } from \"../controllers/transacti\n import { convertAndCreateRepurchaseStockOnchain } from \"../controllers/transactions/repurchaseController.js\";\n import { convertAndCreateRetractionStockOnchain } from \"../controllers/transactions/retractionController.js\";\n import { convertAndCreateTransferStockOnchain } from \"../controllers/transactions/transferController.js\";\n-import { createEquityCompensationIssuance } from \"../db/operations/create.js\";\n-import { createConvertibleIssuance } from \"../db/operations/create.js\";\n+import { createConvertibleIssuance, createEquityCompensationIssuance } from \"../db/operations/create.js\";\n \n import { readIssuerById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n@@ -50,7 +49,7 @@ transactions.post(\"/issuance/stock\", async (req, res) => {\n \n         res.status(200).send({ stockIssuance: incomingStockIssuance });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -68,7 +67,7 @@ transactions.post(\"/transfer/stock\", async (req, res) => {\n \n         res.status(200).send(\"success\");\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -256,7 +255,7 @@ transactions.post(\"/adjust/issuer/authorized-shares\", async (req, res) => {\n \n         res.status(200).send({ issuerAuthorizedSharesAdj });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -312,7 +311,7 @@ transactions.post(\"/issuance/equity-compensation\", async (req, res) => {\n \n         res.status(200).send({ equityCompensationIssuance: createdIssuance });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n })\n@@ -341,7 +340,7 @@ transactions.post(\"/issuance/convertible\", async (req, res) => {\n \n         res.status(200).send({ convertibleIssuance: createdIssuance });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n })"
            },
            {
                "filename": "src/routes/valuation.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import valuationSchema from \"../../ocf/schema/objects/Valuation.schema.json\" assert { type: \"json\" };\n import { createValuation } from \"../db/operations/create.js\";\n-import { countValuations, readValuationById } from \"../db/operations/read.js\";\n+import { countValuations, readIssuerById, readValuationById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const valuation = Router();\n \n@@ -18,7 +17,7 @@ valuation.get(\"/id/:id\", async (req, res) => {\n         const valuation = await readValuationById(id);\n         res.status(200).send(valuation);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -28,7 +27,7 @@ valuation.get(\"/total-number\", async (_, res) => {\n         const totalValuations = await countValuations();\n         res.status(200).send(totalValuations.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -57,7 +56,7 @@ valuation.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ valuation });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            },
            {
                "filename": "src/routes/vestingTerms.js",
                "additions": 4,
                "deletions": 5,
                "patch": "@@ -2,9 +2,8 @@ import { Router } from \"express\";\n import { v4 as uuid } from \"uuid\";\n import vestingTermsSchema from \"../../ocf/schema/objects/VestingTerms.schema.json\" assert { type: \"json\" };\n import { createVestingTerms } from \"../db/operations/create.js\";\n-import { countVestingTerms, readVestingTermsById } from \"../db/operations/read.js\";\n+import { countVestingTerms, readIssuerById, readVestingTermsById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n \n const vestingTerms = Router();\n \n@@ -20,7 +19,7 @@ vestingTerms.get(\"/id/:id\", async (req, res) => {\n         const vestingTerms = await readVestingTermsById(id);\n         res.status(200).send(vestingTerms);\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -30,7 +29,7 @@ vestingTerms.get(\"/total-number\", async (_, res) => {\n         const totalVestingTerms = await countVestingTerms();\n         res.status(200).send(totalVestingTerms.toString());\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });\n@@ -59,7 +58,7 @@ vestingTerms.post(\"/create\", async (req, res) => {\n \n         res.status(200).send({ vestingTerms });\n     } catch (error) {\n-        console.error(`error: ${error}`);\n+        console.error(error);\n         res.status(500).send(`${error}`);\n     }\n });"
            }
        ]
    },
    {
        "sha": "eaca9044bc36dac6a1ff17b30ffd0cd3a95dce12",
        "author": "kentkolze",
        "date": "2024-01-31 16:37:15+00:00",
        "message": "better support for multiple .env files",
        "files": [
            {
                "filename": ".gitignore",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -26,6 +26,7 @@ yarn-error.log*\n .env.development.local\n .env.test.local\n .env.production.local\n+.env.testnet\n \n # vercel\n .vercel"
            },
            {
                "filename": "README.md",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -97,10 +97,10 @@ yarn deploy-libraries\n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n ```sh\n-yarn deploy-factory\n+yarn deploy-factory [.env file to use]\n ```\n \n-This will deploy the factory as pointed to in the `.env`\n+This will deploy the factory as pointed to in the `.env` or the file you pass in, such as .env.local\n \n ## Running the cap table server\n "
            },
            {
                "filename": "jest.config.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -5,6 +5,7 @@\n \n // This allows us to avoid messing with the state of people's standard dev database\n process.env['DATABASE_OVERRIDE'] = 'jest-integration';\n+process.env['USE_ENV_FILE'] = '.env.test.local';\n \n /** @type {import('jest').Config} */\n const config = {"
            },
            {
                "filename": "scripts/deployFactory.sh",
                "additions": 6,
                "deletions": 2,
                "patch": "@@ -1,9 +1,13 @@\n #!/bin/bash\n \n-source .env\n+# Accept a single argument of an env file to use. By default use .env at root\n+USE_ENV_FILE=${1:-.env}\n \n+source $USE_ENV_FILE\n+\n+# Copy the root .env underneath chain so we dont have to maintain two copies\n TEMP=$PWD/chain/.env\n-cp .env $TEMP\n+cp $USE_ENV_FILE $TEMP\n trap \"rm $TEMP\" EXIT\n \n set -x"
            },
            {
                "filename": "src/utils/env.js",
                "additions": 12,
                "deletions": 4,
                "patch": "@@ -15,25 +15,33 @@ const splitPath = (path) => {\n };\n \n \n-const getEnvFile = () => {\n+const getEnvFile = (fileName) => {\n     // Find the .env file by iterating up the PWD. However do not go past the repo root!\n     const repoRootDirName = \"tap-cap-table\"\n     const cwd = process.env.PWD\n     let { dir, rightMost } = splitPath(cwd);\n-    let check = pathTools.join(cwd, \".env\");\n+    let check = pathTools.join(cwd, fileName);\n     while (!fs.existsSync(check)) {\n         if (rightMost === repoRootDirName) {\n             throw new Error(`Unable to locate .env in ${check}`);\n         }\n         // Check our current dir\n-        check = pathTools.join(dir, \".env\");\n+        check = pathTools.join(dir, fileName);\n         // The dir to look in next\n         ({ dir, rightMost } = splitPath(dir));\n     }\n     return check;\n };\n \n+let _ALREADY_SETUP = false;\n \n export const setupEnv = () => {\n-    config({path: getEnvFile()});\n+    if (_ALREADY_SETUP) {\n+        return;\n+    }\n+    const fileName = process.env.USE_ENV_FILE || '.env';\n+    const path = getEnvFile(fileName)\n+    console.log(\"setupEnv with:\", path);\n+    config({ path });\n+    _ALREADY_SETUP = true;\n };"
            }
        ]
    },
    {
        "sha": "abb718427673fe7973e257c9a76515d264a3ca88",
        "author": "victormimo",
        "date": "2024-01-28 00:29:32+00:00",
        "message": "Merge pull request #146 from transfer-agent-protocol/dev\n\nDev to main",
        "files": [
            {
                "filename": "Dockerfile.dev",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -12,4 +12,5 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npm\", \"run\", \"dev\"]\n+CMD [\"npx\", \"ts-node\", \"src/server.js\"]\n+"
            },
            {
                "filename": "Dockerfile.prod",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npm\", \"run\", \"prod\"]\n\\ No newline at end of file\n+CMD [\"npx\", \"tsx\", \"src/server.js\", \"--finalized-only\"]"
            }
        ]
    },
    {
        "sha": "bcb35cca9d70a361e7f1a711c40117f410a75fc8",
        "author": "victormimo",
        "date": "2024-01-28 00:29:08+00:00",
        "message": "Merge pull request #145 from transfer-agent-protocol/vmimo/fixing-dockerfiles\n\nfeat (fixing docker files)",
        "files": [
            {
                "filename": "Dockerfile.dev",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -12,4 +12,5 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npm\", \"run\", \"dev\"]\n+CMD [\"npx\", \"ts-node\", \"src/server.js\"]\n+"
            },
            {
                "filename": "Dockerfile.prod",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npm\", \"run\", \"prod\"]\n\\ No newline at end of file\n+CMD [\"npx\", \"tsx\", \"src/server.js\", \"--finalized-only\"]"
            }
        ]
    },
    {
        "sha": "8be426d26a1f08ed2114a8342967768f9fac4fbb",
        "author": "victormimo",
        "date": "2024-01-28 00:28:23+00:00",
        "message": "fixing docker files",
        "files": [
            {
                "filename": "Dockerfile.dev",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -12,4 +12,5 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npm\", \"run\", \"dev\"]\n+CMD [\"npx\", \"ts-node\", \"src/server.js\"]\n+"
            },
            {
                "filename": "Dockerfile.prod",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npm\", \"run\", \"prod\"]\n\\ No newline at end of file\n+CMD [\"npx\", \"tsx\", \"src/server.js\", \"--finalized-only\"]"
            }
        ]
    },
    {
        "sha": "1404a8af2044a9e0686b6747f3e274c66b1320d8",
        "author": "victormimo",
        "date": "2024-01-26 21:57:07+00:00",
        "message": "Merge pull request #144 from transfer-agent-protocol/dev\n\nDev to main",
        "files": [
            {
                "filename": "Dockerfile.dev",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npx tsx\", \"src/server.js\"]\n+CMD [\"npm\", \"run\", \"dev\"]"
            },
            {
                "filename": "Dockerfile.prod",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npx tsx\", \"src/server.js --finalized-only\"]\n+CMD [\"npm\", \"run\", \"prod\"]\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "7f0d0dec92e1544d8805eb966de909487bae4742",
        "author": "victormimo",
        "date": "2024-01-26 21:56:33+00:00",
        "message": "Merge pull request #143 from transfer-agent-protocol/vmimo/fixing-dockerfiles\n\nfixing dockerfiles",
        "files": [
            {
                "filename": "Dockerfile.dev",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npx tsx\", \"src/server.js\"]\n+CMD [\"npm\", \"run\", \"dev\"]"
            },
            {
                "filename": "Dockerfile.prod",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npx tsx\", \"src/server.js --finalized-only\"]\n+CMD [\"npm\", \"run\", \"prod\"]\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "0913d320cd1ed2e3f2af1fae60dcdb6e9a5af00f",
        "author": "victormimo",
        "date": "2024-01-26 21:56:07+00:00",
        "message": "fixing dockerfiles",
        "files": [
            {
                "filename": "Dockerfile.dev",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npx tsx\", \"src/server.js\"]\n+CMD [\"npm\", \"run\", \"dev\"]"
            },
            {
                "filename": "Dockerfile.prod",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"npx tsx\", \"src/server.js --finalized-only\"]\n+CMD [\"npm\", \"run\", \"prod\"]\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "91b5aa41cd4bd7ebba0f426914c0828fde23e8f2",
        "author": "victormimo",
        "date": "2024-01-26 21:24:11+00:00",
        "message": "Merge pull request #142 from transfer-agent-protocol/dev\n\nDev to main",
        "files": [
            {
                "filename": "README.md",
                "additions": 9,
                "deletions": 9,
                "patch": "@@ -59,12 +59,6 @@ Copy `.env.example` to `.env` in the root of the project.\n cd tap-cap-table && cp .env.example .env\n ```\n \n-Copy `.env.example` to `.env` inside of the `chain` folder.\n-\n-```sh\n-cd chain && cp .env.example .env\n-```\n-\n In the root folder, pull the official Mongo image, and run the local development database with `docker compose`:\n \n ```sh\n@@ -97,17 +91,23 @@ In our architecture, each transaction is mapped to an external library, which en\n 2. Then, inside of the root directory run\n \n ```sh\n-yarn build\n+yarn deploy-libraries\n ```\n \n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n+```sh\n+yarn deploy-factory\n+```\n+\n+This will deploy the factory as pointed to in the `.env`\n+\n ## Running the cap table server\n \n After the deployment script is completed, start the server with nodemon:\n \n ```sh\n-yarn start\n+yarn dev\n ```\n \n Inspect the database with Mongo Compass. To connect to it, use the same string that we provided in the `.env` file:\n@@ -157,7 +157,7 @@ Inside of `/chain`:\n \n -   Restart anvil\n -   Run `forge clean`\n--   Move back to the root directory, then run `yarn build`\n+-   Move back to the root directory, then run `yarn deploy-libraries`\n \n After, you can seed and deploy the cap table with either of the above options. If the bug persists, please open an issue with an attached screenshot and steps to reproduce.\n "
            },
            {
                "filename": "chain/.env.example",
                "additions": 0,
                "deletions": 13,
                "patch": "@@ -1,13 +0,0 @@\n-# RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n-RPC_URL=http://127.0.0.1:8545\n-\n-# Change this to the chain id of the network you are deploying to\n-# Use 31337 for Anvil's, 32586980208 for Arbitrum's\n-CHAIN_ID=31337\n-\n-# Update with the private key of the account that will be used to deploy the contracts\n-PRIVATE_KEY=UPDATE_ME\n-\n-# Etherscan API keys\n-ETHERSCAN_L2_API_KEY=UPDATE_ME\n-ETHERSCAN_L1_API_KEY=UPDATE_ME"
            },
            {
                "filename": "chain/.gitignore",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -5,12 +5,13 @@ out/\n # Ignores development broadcast logs\n !/broadcast\n /broadcast/*/31337/\n+/broadcast/*/84531/\n+/broadcast/*/84532/\n /broadcast/**/dry-run/\n /broadcast/capTable.s.sol/\n /broadcast/capTableFactory.s.sol/\n \n # Docs\n docs/\n \n-# Dotenv file\n-.env\n+.env\n\\ No newline at end of file"
            },
            {
                "filename": "package.json",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -15,7 +15,7 @@\n         \"typecheck\": \"concurrently --raw yarn:typecheck:*\",\n         \"typecheck:app\": \"tsc --noEmit\",\n         \"prepare\": \"husky install\",\n-        \"build\": \"cd chain && npx tsx ../src/scripts/deployAndLinkLibs.js\",\n+        \"deploy-factory\": \"./scripts/deployFactory.sh\",\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n         \"deseed\": \"npx tsx src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\","
            },
            {
                "filename": "scripts/deployFactory.sh",
                "additions": 11,
                "deletions": 0,
                "patch": "@@ -0,0 +1,11 @@\n+#!/bin/bash\n+\n+source .env\n+\n+TEMP=$PWD/chain/.env\n+cp .env $TEMP\n+trap \"rm $TEMP\" EXIT\n+\n+set -x\n+cd chain\n+forge script script/CapTableFactory.s.sol --broadcast --fork-url $RPC_URL"
            },
            {
                "filename": "src/app.js",
                "additions": 7,
                "deletions": 5,
                "patch": "@@ -1,13 +1,11 @@\n-import { config } from \"dotenv\";\n import express, { json, urlencoded } from \"express\";\n-config();\n-\n import { connectDB } from \"./db/config/mongoose.ts\";\n \n import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n \n // Routes\n import { capTable as capTableRoutes } from \"./routes/capTable.ts\";\n+import { router as factoryRoutes } from \"./routes/factory.ts\";\n import historicalTransactions from \"./routes/historicalTransactions.js\";\n import mainRoutes from \"./routes/index.js\";\n import issuerRoutes from \"./routes/issuer.js\";\n@@ -22,6 +20,9 @@ import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n import mongoose from \"mongoose\";\n import { readIssuerById } from \"./db/operations/read.js\";\n import { getIssuerContract } from \"./utils/caches.ts\";\n+import { setupEnv } from \"./utils/env.js\";\n+\n+setupEnv();\n \n const app = express();\n \n@@ -32,12 +33,12 @@ const PORT = process.env.PORT;\n const contractMiddleware = async (req, res, next) => {\n     if (!req.body.issuerId) {\n         console.log(\"\u274c | No issuer ID\");\n-        res.status(400).send(\"issuerId is required\");\n+        return res.status(400).send(\"issuerId is required\");\n     }\n \n     // fetch issuer to ensure it exists\n     const issuer = await readIssuerById(req.body.issuerId);\n-    if (!issuer) res.status(400).send(\"issuer not found \");\n+    if (!issuer) return res.status(400).send(\"Issuer not found\");\n \n     const { contract, provider } = await getIssuerContract(issuer);\n     req.contract = contract;\n@@ -50,6 +51,7 @@ app.enable(\"trust proxy\");\n \n app.use(\"/\", mainRoutes);\n app.use(\"/cap-table\", capTableRoutes);\n+app.use(\"/factory\", factoryRoutes);\n app.use(\"/issuer\", issuerRoutes);\n app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);"
            },
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 6,
                "deletions": 8,
                "patch": "@@ -1,27 +1,25 @@\n-import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import CAP_TABLE_FACTORY from \"../../chain/out/CapTableFactory.sol/CapTableFactory.json\" assert { type: \"json\" };\n-import { readFactory } from \"../db/operations/read.js\";\n+import { readfactories } from \"../db/operations/read.js\";\n import { toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n+import { setupEnv } from \"../utils/env.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n import getProvider from \"./getProvider.js\";\n \n-config();\n+setupEnv();\n \n async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n \n     const provider = getProvider();\n \n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-\n     console.log(\"\ud83d\uddfd | Wallet address: \", wallet.address);\n \n-    const factory = await readFactory();\n-    const factoryAddress = factory[0].factory_address;\n-\n-    console.log(\"factory \", factory);\n+    const factories = await readfactories();\n+    const factoryAddress = factories[0]?.factory_address;\n+    console.log({factories, factoryAddress});\n \n     if (!factoryAddress) {\n         throw new Error(`\u274c | Factory address not found`);"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,10 +1,10 @@\n-import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n+import { setupEnv } from \"../utils/env.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n import getProvider from \"./getProvider.js\";\n \n-config();\n+setupEnv();\n \n export const getContractInstance = (address) => {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;"
            },
            {
                "filename": "src/chain-operations/getProvider.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,6 +1,7 @@\n-import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n-config();\n+import { setupEnv } from \"../utils/env\";\n+\n+setupEnv();\n \n const RPC_URL = process.env.RPC_URL;\n const CHAIN_ID = process.env.CHAIN_ID;"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,7 +1,7 @@\n-import dotenv from \"dotenv\";\n import mongoose from \"mongoose\";\n+import { setupEnv } from \"../../utils/env\";\n \n-dotenv.config();\n+setupEnv();\n \n const DATABASE_URL = process.env.DATABASE_URL;\n const DATABASE_OVERRIDE = process.env.DATABASE_OVERRIDE;"
            },
            {
                "filename": "src/db/objects/Issuer.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -16,7 +16,7 @@ const IssuerSchema = new mongoose.Schema({\n     address: {},\n     initial_shares_authorized: String,\n     comments: [String],\n-    deployed_to: String,\n+    deployed_to: String,  // Address of its CapTable\n     tx_hash: String,\n     last_processed_block: { type: Number, default: null },\n     is_manifest_created: { type: Boolean, default: false },"
            },
            {
                "filename": "src/db/operations/create.js",
                "additions": 5,
                "deletions": 0,
                "patch": "@@ -1,3 +1,4 @@\n+import Factory from \"../objects/Factory.js\";\n import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n@@ -59,3 +60,7 @@ export const createConvertibleIssuance = (issuanceData) => {\n export const createStockTransfer = (stockTransferData) => {\n     return save(new StockTransfer(stockTransferData));\n };\n+\n+export const createFactory = (factoryData) => {\n+    return save(new Factory(factoryData));\n+}"
            },
            {
                "filename": "src/db/operations/read.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -92,6 +92,6 @@ export const readAllIssuers = async () => {\n     return await find(Issuer);\n }\n \n-export const readFactory = async () => {\n+export const readfactories = async () => {\n     return await find(Factory);\n }\n\\ No newline at end of file"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 12,
                "deletions": 1,
                "patch": "@@ -1,4 +1,5 @@\n import sleep from \"../../utils/sleep.js\";\n+import Factory from \"../objects/Factory.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n@@ -15,7 +16,8 @@ import StockReissuance from \"../objects/transactions/reissuance/StockReissuance.\n import StockRepurchase from \"../objects/transactions/repurchase/StockRepurchase.js\";\n import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import { findByIdAndUpdate } from \"./atomic.ts\";\n+import { findByIdAndUpdate, findOne } from \"./atomic.ts\";\n+import { createFactory } from \"./create.js\";\n \n \n export const web3WaitTime = 5000;\n@@ -101,3 +103,12 @@ export const upsertStockClassAuthorizedSharesAdjustment = async (id, updatedData\n export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n     return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n };\n+\n+export const upsertFactory = async (updatedData) => {\n+    // For now, we only allow a single record in the database\n+    const existing = await findOne(Factory);\n+    if (existing) {\n+        return await findByIdAndUpdate(Factory, existing._id, updatedData, { new: true });\n+    } \n+    return await createFactory(updatedData);\n+}"
            },
            {
                "filename": "src/routes/factory.ts",
                "additions": 22,
                "deletions": 0,
                "patch": "@@ -0,0 +1,22 @@\n+import { Router } from \"express\";\n+import { upsertFactory } from \"../db/operations/update\";\n+\n+export const router = Router();\n+\n+router.get(\"/\", async (req, res) => {\n+    res.send(\"Hello Factory!\");\n+});\n+\n+router.post(\"/register\", async (req, res) => {\n+    /*\n+    Register the factory contracts addresses\n+    */\n+    try {\n+        const {factory_address, implementation_address} = req.body;\n+        const factory = await upsertFactory({factory_address, implementation_address});\n+        res.send({factory});\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+});"
            },
            {
                "filename": "src/scripts/deployAndLinkLibs.js",
                "additions": 4,
                "deletions": 3,
                "patch": "@@ -1,10 +1,10 @@\n-import { config } from \"dotenv\";\n import { spawn } from \"child_process\";\n import fs from \"fs\";\n import path from \"path\";\n import sleep from \"../utils/sleep.js\";\n+import { setupEnv } from \"../utils/env.js\";\n \n-config();\n+setupEnv();\n \n const PRIVATE_KEY = process.env.PRIVATE_KEY;\n const RPC_URL = process.env.RPC_URL;\n@@ -96,7 +96,8 @@ async function deployLib(lib, libs) {\n             \"--json\",\n             ...librariesDepsArgs,\n         ];\n-        // TODO: only log when things break console.log(`Forge command arguments: ${args.join(\" \")}`);\n+        // TODO: only log when things break \n+        // console.log(`Forge command arguments: ${args.join(\" \")}`);\n \n         // Executing the forge command\n         const subprocess = spawn(\"forge\", args);"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -27,7 +27,7 @@ const seedExampleData = async () => {\n     const rec = await Factory.findOne();\n     if (!rec) {\n         throw new Error(\n-            `Manually create the {\"implementation_adress\": ..., \"factory_address\": ...} record \n+            `Manually create the {\"implementation_address\": ..., \"factory_address\": ...} record \n             in \"factories\" collection. Run the \"forge script ...\" command from the comment \n             in \"chain/script/CapTableFactory.s.sol\"`\n         );"
            },
            {
                "filename": "src/utils/env.js",
                "additions": 39,
                "deletions": 0,
                "patch": "@@ -0,0 +1,39 @@\n+import { config } from \"dotenv\";\n+import fs from \"fs\";\n+import pathTools from \"path\";\n+\n+const splitPath = (path) => {\n+    /* \n+    Split the file/dir path into its directory and the rightMost piece\n+     ie /home/user/file.txt --> {dir: \"/home/user\", rightMost: \"file.txt\"}\n+    */\n+    const normalizedPath = path.replace(/\\/+$/, '');\n+    const lastIndex = normalizedPath.lastIndexOf('/');\n+    const dir = normalizedPath.substring(0, lastIndex);\n+    const rightMost = normalizedPath.substring(lastIndex + 1);\n+    return { dir, rightMost };\n+};\n+\n+\n+const getEnvFile = () => {\n+    // Find the .env file by iterating up the PWD. However do not go past the repo root!\n+    const repoRootDirName = \"tap-cap-table\"\n+    const cwd = process.env.PWD\n+    let { dir, rightMost } = splitPath(cwd);\n+    let check = pathTools.join(cwd, \".env\");\n+    while (!fs.existsSync(check)) {\n+        if (rightMost === repoRootDirName) {\n+            throw new Error(`Unable to locate .env in ${check}`);\n+        }\n+        // Check our current dir\n+        check = pathTools.join(dir, \".env\");\n+        // The dir to look in next\n+        ({ dir, rightMost } = splitPath(dir));\n+    }\n+    return check;\n+};\n+\n+\n+export const setupEnv = () => {\n+    config({path: getEnvFile()});\n+};"
            }
        ]
    },
    {
        "sha": "6e2942b3eac9a6ba7db42ed8e8f316d648d296e5",
        "author": "victormimo",
        "date": "2024-01-26 21:16:22+00:00",
        "message": "Merge pull request #141 from transfer-agent-protocol/vmimo/restart-server-issuerid-error\n\nimprovement (Vmimo/restart server issuerid error)",
        "files": [
            {
                "filename": "src/app.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -22,7 +22,7 @@ import { readIssuerById } from \"./db/operations/read.js\";\n import { getIssuerContract } from \"./utils/caches.ts\";\n import { setupEnv } from \"./utils/env.js\";\n \n-setupEnv()\n+setupEnv();\n \n const app = express();\n \n@@ -33,12 +33,12 @@ const PORT = process.env.PORT;\n const contractMiddleware = async (req, res, next) => {\n     if (!req.body.issuerId) {\n         console.log(\"\u274c | No issuer ID\");\n-        res.status(400).send(\"issuerId is required\");\n+        return res.status(400).send(\"issuerId is required\");\n     }\n \n     // fetch issuer to ensure it exists\n     const issuer = await readIssuerById(req.body.issuerId);\n-    if (!issuer) res.status(400).send(\"issuer not found \");\n+    if (!issuer) return res.status(400).send(\"Issuer not found\");\n \n     const { contract, provider } = await getIssuerContract(issuer);\n     req.contract = contract;"
            }
        ]
    },
    {
        "sha": "20a2c4e3b8f111574ea8f64b6e855a83e8871e68",
        "author": "victormimo",
        "date": "2024-01-26 21:15:19+00:00",
        "message": "exiting middleware if not found",
        "files": [
            {
                "filename": "src/app.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -22,7 +22,7 @@ import { readIssuerById } from \"./db/operations/read.js\";\n import { getIssuerContract } from \"./utils/caches.ts\";\n import { setupEnv } from \"./utils/env.js\";\n \n-setupEnv()\n+setupEnv();\n \n const app = express();\n \n@@ -33,12 +33,12 @@ const PORT = process.env.PORT;\n const contractMiddleware = async (req, res, next) => {\n     if (!req.body.issuerId) {\n         console.log(\"\u274c | No issuer ID\");\n-        res.status(400).send(\"issuerId is required\");\n+        return res.status(400).send(\"issuerId is required\");\n     }\n \n     // fetch issuer to ensure it exists\n     const issuer = await readIssuerById(req.body.issuerId);\n-    if (!issuer) res.status(400).send(\"issuer not found \");\n+    if (!issuer) return res.status(400).send(\"Issuer not found\");\n \n     const { contract, provider } = await getIssuerContract(issuer);\n     req.contract = contract;"
            }
        ]
    },
    {
        "sha": "a49b13f68841ee8fe64ccc6ef3403adbc4bc500d",
        "author": "victormimo",
        "date": "2024-01-25 17:57:49+00:00",
        "message": "Merge pull request #140 from transfer-agent-protocol/kkolze/easier-deployments\n\neasier deployments",
        "files": [
            {
                "filename": "README.md",
                "additions": 9,
                "deletions": 9,
                "patch": "@@ -59,12 +59,6 @@ Copy `.env.example` to `.env` in the root of the project.\n cd tap-cap-table && cp .env.example .env\n ```\n \n-Copy `.env.example` to `.env` inside of the `chain` folder.\n-\n-```sh\n-cd chain && cp .env.example .env\n-```\n-\n In the root folder, pull the official Mongo image, and run the local development database with `docker compose`:\n \n ```sh\n@@ -97,17 +91,23 @@ In our architecture, each transaction is mapped to an external library, which en\n 2. Then, inside of the root directory run\n \n ```sh\n-yarn build\n+yarn deploy-libraries\n ```\n \n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n+```sh\n+yarn deploy-factory\n+```\n+\n+This will deploy the factory as pointed to in the `.env`\n+\n ## Running the cap table server\n \n After the deployment script is completed, start the server with nodemon:\n \n ```sh\n-yarn start\n+yarn dev\n ```\n \n Inspect the database with Mongo Compass. To connect to it, use the same string that we provided in the `.env` file:\n@@ -157,7 +157,7 @@ Inside of `/chain`:\n \n -   Restart anvil\n -   Run `forge clean`\n--   Move back to the root directory, then run `yarn build`\n+-   Move back to the root directory, then run `yarn deploy-libraries`\n \n After, you can seed and deploy the cap table with either of the above options. If the bug persists, please open an issue with an attached screenshot and steps to reproduce.\n "
            },
            {
                "filename": "chain/.env.example",
                "additions": 0,
                "deletions": 13,
                "patch": "@@ -1,13 +0,0 @@\n-# RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n-RPC_URL=http://127.0.0.1:8545\n-\n-# Change this to the chain id of the network you are deploying to\n-# Use 31337 for Anvil's, 32586980208 for Arbitrum's\n-CHAIN_ID=31337\n-\n-# Update with the private key of the account that will be used to deploy the contracts\n-PRIVATE_KEY=UPDATE_ME\n-\n-# Etherscan API keys\n-ETHERSCAN_L2_API_KEY=UPDATE_ME\n-ETHERSCAN_L1_API_KEY=UPDATE_ME"
            },
            {
                "filename": "chain/.gitignore",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -5,12 +5,13 @@ out/\n # Ignores development broadcast logs\n !/broadcast\n /broadcast/*/31337/\n+/broadcast/*/84531/\n+/broadcast/*/84532/\n /broadcast/**/dry-run/\n /broadcast/capTable.s.sol/\n /broadcast/capTableFactory.s.sol/\n \n # Docs\n docs/\n \n-# Dotenv file\n-.env\n+.env\n\\ No newline at end of file"
            },
            {
                "filename": "package.json",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -15,7 +15,7 @@\n         \"typecheck\": \"concurrently --raw yarn:typecheck:*\",\n         \"typecheck:app\": \"tsc --noEmit\",\n         \"prepare\": \"husky install\",\n-        \"build\": \"cd chain && npx tsx ../src/scripts/deployAndLinkLibs.js\",\n+        \"deploy-factory\": \"./scripts/deployFactory.sh\",\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n         \"deseed\": \"npx tsx src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\","
            },
            {
                "filename": "scripts/deployFactory.sh",
                "additions": 11,
                "deletions": 0,
                "patch": "@@ -0,0 +1,11 @@\n+#!/bin/bash\n+\n+source .env\n+\n+TEMP=$PWD/chain/.env\n+cp .env $TEMP\n+trap \"rm $TEMP\" EXIT\n+\n+set -x\n+cd chain\n+forge script script/CapTableFactory.s.sol --broadcast --fork-url $RPC_URL"
            },
            {
                "filename": "src/app.js",
                "additions": 5,
                "deletions": 3,
                "patch": "@@ -1,13 +1,11 @@\n-import { config } from \"dotenv\";\n import express, { json, urlencoded } from \"express\";\n-config();\n-\n import { connectDB } from \"./db/config/mongoose.ts\";\n \n import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n \n // Routes\n import { capTable as capTableRoutes } from \"./routes/capTable.ts\";\n+import { router as factoryRoutes } from \"./routes/factory.ts\";\n import historicalTransactions from \"./routes/historicalTransactions.js\";\n import mainRoutes from \"./routes/index.js\";\n import issuerRoutes from \"./routes/issuer.js\";\n@@ -22,6 +20,9 @@ import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n import mongoose from \"mongoose\";\n import { readIssuerById } from \"./db/operations/read.js\";\n import { getIssuerContract } from \"./utils/caches.ts\";\n+import { setupEnv } from \"./utils/env.js\";\n+\n+setupEnv()\n \n const app = express();\n \n@@ -50,6 +51,7 @@ app.enable(\"trust proxy\");\n \n app.use(\"/\", mainRoutes);\n app.use(\"/cap-table\", capTableRoutes);\n+app.use(\"/factory\", factoryRoutes);\n app.use(\"/issuer\", issuerRoutes);\n app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);"
            },
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 6,
                "deletions": 8,
                "patch": "@@ -1,27 +1,25 @@\n-import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import CAP_TABLE_FACTORY from \"../../chain/out/CapTableFactory.sol/CapTableFactory.json\" assert { type: \"json\" };\n-import { readFactory } from \"../db/operations/read.js\";\n+import { readfactories } from \"../db/operations/read.js\";\n import { toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n+import { setupEnv } from \"../utils/env.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n import getProvider from \"./getProvider.js\";\n \n-config();\n+setupEnv();\n \n async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n \n     const provider = getProvider();\n \n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-\n     console.log(\"\ud83d\uddfd | Wallet address: \", wallet.address);\n \n-    const factory = await readFactory();\n-    const factoryAddress = factory[0].factory_address;\n-\n-    console.log(\"factory \", factory);\n+    const factories = await readfactories();\n+    const factoryAddress = factories[0]?.factory_address;\n+    console.log({factories, factoryAddress});\n \n     if (!factoryAddress) {\n         throw new Error(`\u274c | Factory address not found`);"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,10 +1,10 @@\n-import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n+import { setupEnv } from \"../utils/env.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n import getProvider from \"./getProvider.js\";\n \n-config();\n+setupEnv();\n \n export const getContractInstance = (address) => {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;"
            },
            {
                "filename": "src/chain-operations/getProvider.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,6 +1,7 @@\n-import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n-config();\n+import { setupEnv } from \"../utils/env\";\n+\n+setupEnv();\n \n const RPC_URL = process.env.RPC_URL;\n const CHAIN_ID = process.env.CHAIN_ID;"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,7 +1,7 @@\n-import dotenv from \"dotenv\";\n import mongoose from \"mongoose\";\n+import { setupEnv } from \"../../utils/env\";\n \n-dotenv.config();\n+setupEnv();\n \n const DATABASE_URL = process.env.DATABASE_URL;\n const DATABASE_OVERRIDE = process.env.DATABASE_OVERRIDE;"
            },
            {
                "filename": "src/db/objects/Issuer.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -16,7 +16,7 @@ const IssuerSchema = new mongoose.Schema({\n     address: {},\n     initial_shares_authorized: String,\n     comments: [String],\n-    deployed_to: String,\n+    deployed_to: String,  // Address of its CapTable\n     tx_hash: String,\n     last_processed_block: { type: Number, default: null },\n     is_manifest_created: { type: Boolean, default: false },"
            },
            {
                "filename": "src/db/operations/create.js",
                "additions": 5,
                "deletions": 0,
                "patch": "@@ -1,3 +1,4 @@\n+import Factory from \"../objects/Factory.js\";\n import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n@@ -59,3 +60,7 @@ export const createConvertibleIssuance = (issuanceData) => {\n export const createStockTransfer = (stockTransferData) => {\n     return save(new StockTransfer(stockTransferData));\n };\n+\n+export const createFactory = (factoryData) => {\n+    return save(new Factory(factoryData));\n+}"
            },
            {
                "filename": "src/db/operations/read.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -92,6 +92,6 @@ export const readAllIssuers = async () => {\n     return await find(Issuer);\n }\n \n-export const readFactory = async () => {\n+export const readfactories = async () => {\n     return await find(Factory);\n }\n\\ No newline at end of file"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 12,
                "deletions": 1,
                "patch": "@@ -1,4 +1,5 @@\n import sleep from \"../../utils/sleep.js\";\n+import Factory from \"../objects/Factory.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n@@ -15,7 +16,8 @@ import StockReissuance from \"../objects/transactions/reissuance/StockReissuance.\n import StockRepurchase from \"../objects/transactions/repurchase/StockRepurchase.js\";\n import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import { findByIdAndUpdate } from \"./atomic.ts\";\n+import { findByIdAndUpdate, findOne } from \"./atomic.ts\";\n+import { createFactory } from \"./create.js\";\n \n \n export const web3WaitTime = 5000;\n@@ -101,3 +103,12 @@ export const upsertStockClassAuthorizedSharesAdjustment = async (id, updatedData\n export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n     return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n };\n+\n+export const upsertFactory = async (updatedData) => {\n+    // For now, we only allow a single record in the database\n+    const existing = await findOne(Factory);\n+    if (existing) {\n+        return await findByIdAndUpdate(Factory, existing._id, updatedData, { new: true });\n+    } \n+    return await createFactory(updatedData);\n+}"
            },
            {
                "filename": "src/routes/factory.ts",
                "additions": 22,
                "deletions": 0,
                "patch": "@@ -0,0 +1,22 @@\n+import { Router } from \"express\";\n+import { upsertFactory } from \"../db/operations/update\";\n+\n+export const router = Router();\n+\n+router.get(\"/\", async (req, res) => {\n+    res.send(\"Hello Factory!\");\n+});\n+\n+router.post(\"/register\", async (req, res) => {\n+    /*\n+    Register the factory contracts addresses\n+    */\n+    try {\n+        const {factory_address, implementation_address} = req.body;\n+        const factory = await upsertFactory({factory_address, implementation_address});\n+        res.send({factory});\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+});"
            },
            {
                "filename": "src/scripts/deployAndLinkLibs.js",
                "additions": 4,
                "deletions": 3,
                "patch": "@@ -1,10 +1,10 @@\n-import { config } from \"dotenv\";\n import { spawn } from \"child_process\";\n import fs from \"fs\";\n import path from \"path\";\n import sleep from \"../utils/sleep.js\";\n+import { setupEnv } from \"../utils/env.js\";\n \n-config();\n+setupEnv();\n \n const PRIVATE_KEY = process.env.PRIVATE_KEY;\n const RPC_URL = process.env.RPC_URL;\n@@ -96,7 +96,8 @@ async function deployLib(lib, libs) {\n             \"--json\",\n             ...librariesDepsArgs,\n         ];\n-        // TODO: only log when things break console.log(`Forge command arguments: ${args.join(\" \")}`);\n+        // TODO: only log when things break \n+        // console.log(`Forge command arguments: ${args.join(\" \")}`);\n \n         // Executing the forge command\n         const subprocess = spawn(\"forge\", args);"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -27,7 +27,7 @@ const seedExampleData = async () => {\n     const rec = await Factory.findOne();\n     if (!rec) {\n         throw new Error(\n-            `Manually create the {\"implementation_adress\": ..., \"factory_address\": ...} record \n+            `Manually create the {\"implementation_address\": ..., \"factory_address\": ...} record \n             in \"factories\" collection. Run the \"forge script ...\" command from the comment \n             in \"chain/script/CapTableFactory.s.sol\"`\n         );"
            },
            {
                "filename": "src/utils/env.js",
                "additions": 39,
                "deletions": 0,
                "patch": "@@ -0,0 +1,39 @@\n+import { config } from \"dotenv\";\n+import fs from \"fs\";\n+import pathTools from \"path\";\n+\n+const splitPath = (path) => {\n+    /* \n+    Split the file/dir path into its directory and the rightMost piece\n+     ie /home/user/file.txt --> {dir: \"/home/user\", rightMost: \"file.txt\"}\n+    */\n+    const normalizedPath = path.replace(/\\/+$/, '');\n+    const lastIndex = normalizedPath.lastIndexOf('/');\n+    const dir = normalizedPath.substring(0, lastIndex);\n+    const rightMost = normalizedPath.substring(lastIndex + 1);\n+    return { dir, rightMost };\n+};\n+\n+\n+const getEnvFile = () => {\n+    // Find the .env file by iterating up the PWD. However do not go past the repo root!\n+    const repoRootDirName = \"tap-cap-table\"\n+    const cwd = process.env.PWD\n+    let { dir, rightMost } = splitPath(cwd);\n+    let check = pathTools.join(cwd, \".env\");\n+    while (!fs.existsSync(check)) {\n+        if (rightMost === repoRootDirName) {\n+            throw new Error(`Unable to locate .env in ${check}`);\n+        }\n+        // Check our current dir\n+        check = pathTools.join(dir, \".env\");\n+        // The dir to look in next\n+        ({ dir, rightMost } = splitPath(dir));\n+    }\n+    return check;\n+};\n+\n+\n+export const setupEnv = () => {\n+    config({path: getEnvFile()});\n+};"
            }
        ]
    },
    {
        "sha": "f345d1605e499c5f7fb956fa57c2b24f854d7bc8",
        "author": "kentkolze",
        "date": "2024-01-25 17:53:48+00:00",
        "message": "per victor, deploying the factory deploys the libraries",
        "files": [
            {
                "filename": "package.json",
                "additions": 0,
                "deletions": 1,
                "patch": "@@ -15,7 +15,6 @@\n         \"typecheck\": \"concurrently --raw yarn:typecheck:*\",\n         \"typecheck:app\": \"tsc --noEmit\",\n         \"prepare\": \"husky install\",\n-        \"deploy-libraries\": \"./scripts/deployLibraries.sh\",\n         \"deploy-factory\": \"./scripts/deployFactory.sh\",\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n         \"deseed\": \"npx tsx src/db/scripts/deseed.js\","
            },
            {
                "filename": "scripts/deployLibraries.sh",
                "additions": 0,
                "deletions": 9,
                "patch": "@@ -1,9 +0,0 @@\n-#!/bin/bash\n-\n-TEMP=$PWD/chain/.env\n-cp .env $TEMP\n-trap \"rm $TEMP\" EXIT\n-\n-set -x\n-cd chain\n-npx tsx ../src/scripts/deployAndLinkLibs.js"
            }
        ]
    },
    {
        "sha": "41dd72427eab4d36a096e979abb181ce928c5fef",
        "author": "kentkolze",
        "date": "2024-01-24 21:36:59+00:00",
        "message": "ability to register factory addresses through service",
        "files": [
            {
                "filename": "src/app.js",
                "additions": 2,
                "deletions": 0,
                "patch": "@@ -5,6 +5,7 @@ import { startEventProcessing, stopEventProcessing } from \"./chain-operations/tr\n \n // Routes\n import { capTable as capTableRoutes } from \"./routes/capTable.ts\";\n+import { router as factoryRoutes } from \"./routes/factory.ts\";\n import historicalTransactions from \"./routes/historicalTransactions.js\";\n import mainRoutes from \"./routes/index.js\";\n import issuerRoutes from \"./routes/issuer.js\";\n@@ -50,6 +51,7 @@ app.enable(\"trust proxy\");\n \n app.use(\"/\", mainRoutes);\n app.use(\"/cap-table\", capTableRoutes);\n+app.use(\"/factory\", factoryRoutes);\n app.use(\"/issuer\", issuerRoutes);\n app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);"
            },
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 4,
                "deletions": 6,
                "patch": "@@ -1,7 +1,7 @@\n import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import CAP_TABLE_FACTORY from \"../../chain/out/CapTableFactory.sol/CapTableFactory.json\" assert { type: \"json\" };\n-import { readFactory } from \"../db/operations/read.js\";\n+import { readfactories } from \"../db/operations/read.js\";\n import { toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import { setupEnv } from \"../utils/env.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n@@ -15,13 +15,11 @@ async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     const provider = getProvider();\n \n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-\n     console.log(\"\ud83d\uddfd | Wallet address: \", wallet.address);\n \n-    const factory = await readFactory();\n-    const factoryAddress = factory[0].factory_address;\n-\n-    console.log(\"factory \", factory);\n+    const factories = await readfactories();\n+    const factoryAddress = factories[0]?.factory_address;\n+    console.log({factories, factoryAddress});\n \n     if (!factoryAddress) {\n         throw new Error(`\u274c | Factory address not found`);"
            },
            {
                "filename": "src/db/objects/Issuer.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -16,7 +16,7 @@ const IssuerSchema = new mongoose.Schema({\n     address: {},\n     initial_shares_authorized: String,\n     comments: [String],\n-    deployed_to: String,\n+    deployed_to: String,  // Address of its CapTable\n     tx_hash: String,\n     last_processed_block: { type: Number, default: null },\n     is_manifest_created: { type: Boolean, default: false },"
            },
            {
                "filename": "src/db/operations/create.js",
                "additions": 5,
                "deletions": 0,
                "patch": "@@ -1,3 +1,4 @@\n+import Factory from \"../objects/Factory.js\";\n import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n@@ -59,3 +60,7 @@ export const createConvertibleIssuance = (issuanceData) => {\n export const createStockTransfer = (stockTransferData) => {\n     return save(new StockTransfer(stockTransferData));\n };\n+\n+export const createFactory = (factoryData) => {\n+    return save(new Factory(factoryData));\n+}"
            },
            {
                "filename": "src/db/operations/read.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -92,6 +92,6 @@ export const readAllIssuers = async () => {\n     return await find(Issuer);\n }\n \n-export const readFactory = async () => {\n+export const readfactories = async () => {\n     return await find(Factory);\n }\n\\ No newline at end of file"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 12,
                "deletions": 1,
                "patch": "@@ -1,4 +1,5 @@\n import sleep from \"../../utils/sleep.js\";\n+import Factory from \"../objects/Factory.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n@@ -15,7 +16,8 @@ import StockReissuance from \"../objects/transactions/reissuance/StockReissuance.\n import StockRepurchase from \"../objects/transactions/repurchase/StockRepurchase.js\";\n import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import { findByIdAndUpdate } from \"./atomic.ts\";\n+import { findByIdAndUpdate, findOne } from \"./atomic.ts\";\n+import { createFactory } from \"./create.js\";\n \n \n export const web3WaitTime = 5000;\n@@ -101,3 +103,12 @@ export const upsertStockClassAuthorizedSharesAdjustment = async (id, updatedData\n export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n     return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n };\n+\n+export const upsertFactory = async (updatedData) => {\n+    // For now, we only allow a single record in the database\n+    const existing = await findOne(Factory);\n+    if (existing) {\n+        return await findByIdAndUpdate(Factory, existing._id, updatedData, { new: true });\n+    } \n+    return await createFactory(updatedData);\n+}"
            },
            {
                "filename": "src/routes/factory.ts",
                "additions": 22,
                "deletions": 0,
                "patch": "@@ -0,0 +1,22 @@\n+import { Router } from \"express\";\n+import { upsertFactory } from \"../db/operations/update\";\n+\n+export const router = Router();\n+\n+router.get(\"/\", async (req, res) => {\n+    res.send(\"Hello Factory!\");\n+});\n+\n+router.post(\"/register\", async (req, res) => {\n+    /*\n+    Register the factory contracts addresses\n+    */\n+    try {\n+        const {factory_address, implementation_address} = req.body;\n+        const factory = await upsertFactory({factory_address, implementation_address});\n+        res.send({factory});\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+});"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -27,7 +27,7 @@ const seedExampleData = async () => {\n     const rec = await Factory.findOne();\n     if (!rec) {\n         throw new Error(\n-            `Manually create the {\"implementation_adress\": ..., \"factory_address\": ...} record \n+            `Manually create the {\"implementation_address\": ..., \"factory_address\": ...} record \n             in \"factories\" collection. Run the \"forge script ...\" command from the comment \n             in \"chain/script/CapTableFactory.s.sol\"`\n         );"
            }
        ]
    },
    {
        "sha": "5ba8c89060215ae01ee2604529a0240d0c13c19f",
        "author": "kentkolze",
        "date": "2024-01-24 20:33:45+00:00",
        "message": "fix the .env removal on exit",
        "files": [
            {
                "filename": "chain/.gitignore",
                "additions": 2,
                "deletions": 0,
                "patch": "@@ -5,6 +5,8 @@ out/\n # Ignores development broadcast logs\n !/broadcast\n /broadcast/*/31337/\n+/broadcast/*/84531/\n+/broadcast/*/84532/\n /broadcast/**/dry-run/\n /broadcast/capTable.s.sol/\n /broadcast/capTableFactory.s.sol/"
            },
            {
                "filename": "scripts/deployFactory.sh",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -4,7 +4,7 @@ source .env\n \n TEMP=$PWD/chain/.env\n cp .env $TEMP\n-trap \"rm $TEMP\"\n+trap \"rm $TEMP\" EXIT\n \n set -x\n cd chain"
            },
            {
                "filename": "scripts/deployLibraries.sh",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -2,7 +2,7 @@\n \n TEMP=$PWD/chain/.env\n cp .env $TEMP\n-trap \"rm $TEMP\"\n+trap \"rm $TEMP\" EXIT\n \n set -x\n cd chain"
            }
        ]
    },
    {
        "sha": "51bf61bcdfbd6ea54ab7c8ba658200d9e40ddf4b",
        "author": "kentkolze",
        "date": "2024-01-24 19:14:28+00:00",
        "message": "need the .env for foundry. use the bash scripts to temporarily relocate it",
        "files": [
            {
                "filename": "chain/.gitignore",
                "additions": 2,
                "deletions": 0,
                "patch": "@@ -11,3 +11,5 @@ out/\n \n # Docs\n docs/\n+\n+.env\n\\ No newline at end of file"
            },
            {
                "filename": "scripts/deployFactory.sh",
                "additions": 6,
                "deletions": 1,
                "patch": "@@ -1,6 +1,11 @@\n #!/bin/bash\n-set -x\n \n source .env\n+\n+TEMP=$PWD/chain/.env\n+cp .env $TEMP\n+trap \"rm $TEMP\"\n+\n+set -x\n cd chain\n forge script script/CapTableFactory.s.sol --broadcast --fork-url $RPC_URL"
            },
            {
                "filename": "scripts/deployLibraries.sh",
                "additions": 5,
                "deletions": 1,
                "patch": "@@ -1,5 +1,9 @@\n #!/bin/bash\n-set -x\n \n+TEMP=$PWD/chain/.env\n+cp .env $TEMP\n+trap \"rm $TEMP\"\n+\n+set -x\n cd chain\n npx tsx ../src/scripts/deployAndLinkLibs.js"
            }
        ]
    },
    {
        "sha": "34b19b45c07d7a602b0263dd620a077b5ae14b1a",
        "author": "kentkolze",
        "date": "2024-01-24 18:15:47+00:00",
        "message": "fix the readme for the updated commands",
        "files": [
            {
                "filename": "README.md",
                "additions": 8,
                "deletions": 8,
                "patch": "@@ -59,12 +59,6 @@ Copy `.env.example` to `.env` in the root of the project.\n cd tap-cap-table && cp .env.example .env\n ```\n \n-Copy `.env.example` to `.env` inside of the `chain` folder.\n-\n-```sh\n-cd chain && cp .env.example .env\n-```\n-\n In the root folder, pull the official Mongo image, and run the local development database with `docker compose`:\n \n ```sh\n@@ -97,11 +91,17 @@ In our architecture, each transaction is mapped to an external library, which en\n 2. Then, inside of the root directory run\n \n ```sh\n-yarn build\n+yarn deploy-libraries\n ```\n \n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n+```sh\n+yarn deploy-factory\n+```\n+\n+This will deploy the factory as pointed to in the `.env`\n+\n ## Running the cap table server\n \n After the deployment script is completed, start the server with nodemon:\n@@ -157,7 +157,7 @@ Inside of `/chain`:\n \n -   Restart anvil\n -   Run `forge clean`\n--   Move back to the root directory, then run `yarn build`\n+-   Move back to the root directory, then run `yarn deploy-libraries`\n \n After, you can seed and deploy the cap table with either of the above options. If the bug persists, please open an issue with an attached screenshot and steps to reproduce.\n "
            }
        ]
    },
    {
        "sha": "3199894d6137bd3db36b5245ec1633378b598cf5",
        "author": "kentkolze",
        "date": "2024-01-24 18:09:57+00:00",
        "message": "1) unify to single .env at repo root\n2) rename `yarn build` => `yarn deploy-libraries`\n3) create `yarn deploy-factory`",
        "files": [
            {
                "filename": "README.md",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -107,7 +107,7 @@ This will build all libraries and will take at least 5 minutes to complete. Each\n After the deployment script is completed, start the server with nodemon:\n \n ```sh\n-yarn start\n+yarn dev\n ```\n \n Inspect the database with Mongo Compass. To connect to it, use the same string that we provided in the `.env` file:"
            },
            {
                "filename": "chain/.env.example",
                "additions": 0,
                "deletions": 13,
                "patch": "@@ -1,13 +0,0 @@\n-# RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n-RPC_URL=http://127.0.0.1:8545\n-\n-# Change this to the chain id of the network you are deploying to\n-# Use 31337 for Anvil's, 32586980208 for Arbitrum's\n-CHAIN_ID=31337\n-\n-# Update with the private key of the account that will be used to deploy the contracts\n-PRIVATE_KEY=UPDATE_ME\n-\n-# Etherscan API keys\n-ETHERSCAN_L2_API_KEY=UPDATE_ME\n-ETHERSCAN_L1_API_KEY=UPDATE_ME"
            },
            {
                "filename": "chain/.gitignore",
                "additions": 0,
                "deletions": 3,
                "patch": "@@ -11,6 +11,3 @@ out/\n \n # Docs\n docs/\n-\n-# Dotenv file\n-.env"
            },
            {
                "filename": "package.json",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -15,7 +15,8 @@\n         \"typecheck\": \"concurrently --raw yarn:typecheck:*\",\n         \"typecheck:app\": \"tsc --noEmit\",\n         \"prepare\": \"husky install\",\n-        \"build\": \"cd chain && npx tsx ../src/scripts/deployAndLinkLibs.js\",\n+        \"deploy-libraries\": \"./scripts/deployLibraries.sh\",\n+        \"deploy-factory\": \"./scripts/deployFactory.sh\",\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n         \"deseed\": \"npx tsx src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\","
            },
            {
                "filename": "scripts/deployFactory.sh",
                "additions": 6,
                "deletions": 0,
                "patch": "@@ -0,0 +1,6 @@\n+#!/bin/bash\n+set -x\n+\n+source .env\n+cd chain\n+forge script script/CapTableFactory.s.sol --broadcast --fork-url $RPC_URL"
            },
            {
                "filename": "scripts/deployLibraries.sh",
                "additions": 5,
                "deletions": 0,
                "patch": "@@ -0,0 +1,5 @@\n+#!/bin/bash\n+set -x\n+\n+cd chain\n+npx tsx ../src/scripts/deployAndLinkLibs.js"
            },
            {
                "filename": "src/app.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -1,7 +1,4 @@\n-import { config } from \"dotenv\";\n import express, { json, urlencoded } from \"express\";\n-config();\n-\n import { connectDB } from \"./db/config/mongoose.ts\";\n \n import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n@@ -22,6 +19,9 @@ import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n import mongoose from \"mongoose\";\n import { readIssuerById } from \"./db/operations/read.js\";\n import { getIssuerContract } from \"./utils/caches.ts\";\n+import { setupEnv } from \"./utils/env.js\";\n+\n+setupEnv()\n \n const app = express();\n "
            },
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,13 +1,13 @@\n-import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import CAP_TABLE_FACTORY from \"../../chain/out/CapTableFactory.sol/CapTableFactory.json\" assert { type: \"json\" };\n import { readFactory } from \"../db/operations/read.js\";\n import { toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n+import { setupEnv } from \"../utils/env.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n import getProvider from \"./getProvider.js\";\n \n-config();\n+setupEnv();\n \n async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,10 +1,10 @@\n-import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n+import { setupEnv } from \"../utils/env.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n import getProvider from \"./getProvider.js\";\n \n-config();\n+setupEnv();\n \n export const getContractInstance = (address) => {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;"
            },
            {
                "filename": "src/chain-operations/getProvider.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,6 +1,7 @@\n-import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n-config();\n+import { setupEnv } from \"../utils/env\";\n+\n+setupEnv();\n \n const RPC_URL = process.env.RPC_URL;\n const CHAIN_ID = process.env.CHAIN_ID;"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,7 +1,7 @@\n-import dotenv from \"dotenv\";\n import mongoose from \"mongoose\";\n+import { setupEnv } from \"../../utils/env\";\n \n-dotenv.config();\n+setupEnv();\n \n const DATABASE_URL = process.env.DATABASE_URL;\n const DATABASE_OVERRIDE = process.env.DATABASE_OVERRIDE;"
            },
            {
                "filename": "src/scripts/deployAndLinkLibs.js",
                "additions": 4,
                "deletions": 3,
                "patch": "@@ -1,10 +1,10 @@\n-import { config } from \"dotenv\";\n import { spawn } from \"child_process\";\n import fs from \"fs\";\n import path from \"path\";\n import sleep from \"../utils/sleep.js\";\n+import { setupEnv } from \"../utils/env.js\";\n \n-config();\n+setupEnv();\n \n const PRIVATE_KEY = process.env.PRIVATE_KEY;\n const RPC_URL = process.env.RPC_URL;\n@@ -96,7 +96,8 @@ async function deployLib(lib, libs) {\n             \"--json\",\n             ...librariesDepsArgs,\n         ];\n-        // TODO: only log when things break console.log(`Forge command arguments: ${args.join(\" \")}`);\n+        // TODO: only log when things break \n+        // console.log(`Forge command arguments: ${args.join(\" \")}`);\n \n         // Executing the forge command\n         const subprocess = spawn(\"forge\", args);"
            },
            {
                "filename": "src/utils/env.js",
                "additions": 39,
                "deletions": 0,
                "patch": "@@ -0,0 +1,39 @@\n+import { config } from \"dotenv\";\n+import fs from \"fs\";\n+import pathTools from \"path\";\n+\n+const splitPath = (path) => {\n+    /* \n+    Split the file/dir path into its directory and the rightMost piece\n+     ie /home/user/file.txt --> {dir: \"/home/user\", rightMost: \"file.txt\"}\n+    */\n+    const normalizedPath = path.replace(/\\/+$/, '');\n+    const lastIndex = normalizedPath.lastIndexOf('/');\n+    const dir = normalizedPath.substring(0, lastIndex);\n+    const rightMost = normalizedPath.substring(lastIndex + 1);\n+    return { dir, rightMost };\n+};\n+\n+\n+const getEnvFile = () => {\n+    // Find the .env file by iterating up the PWD. However do not go past the repo root!\n+    const repoRootDirName = \"tap-cap-table\"\n+    const cwd = process.env.PWD\n+    let { dir, rightMost } = splitPath(cwd);\n+    let check = pathTools.join(cwd, \".env\");\n+    while (!fs.existsSync(check)) {\n+        if (rightMost === repoRootDirName) {\n+            throw new Error(`Unable to locate .env in ${check}`);\n+        }\n+        // Check our current dir\n+        check = pathTools.join(dir, \".env\");\n+        // The dir to look in next\n+        ({ dir, rightMost } = splitPath(dir));\n+    }\n+    return check;\n+};\n+\n+\n+export const setupEnv = () => {\n+    config({path: getEnvFile()});\n+};"
            }
        ]
    },
    {
        "sha": "909aaf43c68613c09df6f843e4c967346d30aafa",
        "author": "victormimo",
        "date": "2024-01-20 05:45:00+00:00",
        "message": "Merge pull request #137 from transfer-agent-protocol/dev\n\nDev to main",
        "files": [
            {
                "filename": "Dockerfile.dev",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"node\", \"src/server.js\"]\n+CMD [\"npx tsx\", \"src/server.js\"]"
            },
            {
                "filename": "Dockerfile.prod",
                "additions": 15,
                "deletions": 0,
                "patch": "@@ -0,0 +1,15 @@\n+# Use the official node image as a parent image\n+FROM node:18\n+\n+# Set the working directory\n+WORKDIR /app\n+\n+# COPY ./chain/out ./chain/out\n+COPY . .\n+\n+# Install dependencies and setup\n+RUN yarn install\n+\n+EXPOSE 8080\n+# Specify the command to run on container start\n+CMD [\"npx tsx\", \"src/server.js --finalized-only\"]"
            }
        ]
    },
    {
        "sha": "b025ede35c777f0052dde59352bd4b838c153851",
        "author": "victormimo",
        "date": "2024-01-20 05:44:29+00:00",
        "message": "Merge pull request #134 from transfer-agent-protocol/vmimo/update-docker-files\n\nfeat (separating docker files)",
        "files": [
            {
                "filename": "Dockerfile.dev",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"node\", \"src/server.js\"]\n+CMD [\"npx tsx\", \"src/server.js\"]"
            },
            {
                "filename": "Dockerfile.prod",
                "additions": 15,
                "deletions": 0,
                "patch": "@@ -0,0 +1,15 @@\n+# Use the official node image as a parent image\n+FROM node:18\n+\n+# Set the working directory\n+WORKDIR /app\n+\n+# COPY ./chain/out ./chain/out\n+COPY . .\n+\n+# Install dependencies and setup\n+RUN yarn install\n+\n+EXPOSE 8080\n+# Specify the command to run on container start\n+CMD [\"npx tsx\", \"src/server.js --finalized-only\"]"
            }
        ]
    },
    {
        "sha": "5b8fedcbaba9dcec5bfa20267dbd67036b878ca3",
        "author": "victormimo",
        "date": "2024-01-19 20:33:42+00:00",
        "message": "Merge pull request #112 from transfer-agent-protocol/dev\n\nDev to Main",
        "files": [
            {
                "filename": ".env.example",
                "additions": 20,
                "deletions": 8,
                "patch": "@@ -1,9 +1,21 @@\n+# Offchain db connection string for mongodb\n DATABASE_URL=\"mongodb://tap:tap@localhost:27017/mongo?authSource=admin&retryWrites=true&w=majority\"\n-OPTIMISM_GOERLI_RPC_URL=UPDATE_ME\n-LOCAL_RPC_URL=http://127.0.0.1:8545\n-PRIVATE_KEY_FAKE_ACCOUNT=UPDATE_ME\n-PRIVATE_KEY_POET_TEST=UPDATE_ME\n-ETHERSCAN_OPTIMISM_API_KEY=UPDATE_ME\n-ETHERSCAN_ETHEREUM_API_KEY=UPDATE_ME\n-CHAIN=local\n-# CHAIN=\"optimism-goerli\"\n+DATABASE_REPLSET=\"0\"  # set to \"1\" if using --replSet option in mongo. this allows transactions\n+\n+# RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n+RPC_URL=http://127.0.0.1:8545\n+\n+# Change this to the chain id of the network you are deploying to\n+# Use 31337 for Anvil's, 32586980208 for Arbitrum's\n+CHAIN_ID=31337\n+\n+# Update with the private key of the account that will be used to deploy the contracts\n+PRIVATE_KEY=UPDATE_ME\n+\n+# Etherscan API keys\n+ETHERSCAN_L2_API_KEY=UPDATE_ME\n+ETHERSCAN_L1_API_KEY=UPDATE_ME\n+\n+# Server port\n+PORT=8080\n+"
            },
            {
                "filename": "LICENSE",
                "additions": 3,
                "deletions": 1,
                "patch": "@@ -1,4 +1,6 @@\n-Copyright 2023 Poet Network Inc.\n+MIT License\n+\n+Copyright (c) [2023] [Victor Mimo, Alex Palmer]\n \n Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n "
            },
            {
                "filename": "README.md",
                "additions": 52,
                "deletions": 25,
                "patch": "@@ -2,42 +2,44 @@\n \n Developed by:\n \n-- [Poet](https://poet.network/)\n-- [Plural Energy](https://www.pluralenergy.co/)\n-- [Fairmint](https://www.fairmint.com/)\n+# [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n+\n+-   [Poet](https://poet.network/)\n+-   [Plural Energy](https://www.pluralenergy.co/)\n+-   [Fairmint](https://www.fairmint.com/)\n \n This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety. In development, it's meant to be run in a Docker container with a local MongoDB instance. While in active development, it's meant to be run with [Anvil](https://book.getfoundry.sh/anvil/) and [Forge](https://book.getfoundry.sh/forge/).\n \n <div align=\"center\">\n-  <a href=\"https://github.com/poet-network/tap-cap-table/blob/main/LICENSE\">\n-    <img alt=\"License\" src=\"https://img.shields.io/github/license/poet-network/tap-cap-table\">\n+  <a href=\"https://github.com/transfer-agent-protocol/tap-cap-table/blob/main/LICENSE\">\n+    <img alt=\"License\" src=\"https://img.shields.io/github/license/transfer-agent-protocol/tap-cap-table\">\n   </a>\n </div>\n \n ## Dependencies\n \n-- [Docker](https://docs.docker.com/get-docker/)\n+-   [Docker](https://docs.docker.com/get-docker/)\n \n-- [Foundry](https://getfoundry.sh/)\n+-   [Foundry](https://getfoundry.sh/)\n \n ```sh\n curl -L https://foundry.paradigm.xyz | bash\n ```\n \n-- [Mongo Compass](https://www.mongodb.com/try/download/compass)\n+-   [Mongo Compass](https://www.mongodb.com/try/download/compass)\n \n-- [Postman App](https://www.postman.com/downloads/)\n+-   [Postman App](https://www.postman.com/downloads/)\n \n-- [Node.js v18.16.0](https://nodejs.org/en/download/)\n+-   [Node.js v18.16.0](https://nodejs.org/en/download/)\n \n-- [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n+-   [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n \n We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo) for the local development database. You can find the [Docker Compose file](./docker-compose.yml) in the root of this repository.\n \n ## Official links\n \n-- [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n-- [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n+-   [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n+-   [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n \n ## Getting started\n \n@@ -87,17 +89,18 @@ Install dependencies and setup [Foundry](https://book.getfoundry.sh/) and `forge\n yarn install && yarn setup\n ```\n \n-## Deploying external libraries\n-\n-In our architecture, each transaction is mapped to an external library, which ensures bytecode limits are never met.\n+## Deploying the cap table smart contracts\n \n-To deploy these libraries:\n+In our architecture, each transaction is mapped to an external library, which ensures bytecode limits are never met. To deploy these libraries:\n \n 1. Ensure you have Anvil running in the `/chain` directory\n-2. Then, inside of the root directory run `yarn build`\n+2. Then, inside of the root directory run\n \n-This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n+```sh\n+yarn build\n+```\n \n+This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n ## Running the cap table server\n \n@@ -152,16 +155,40 @@ We're shipping code fast. If you run into an issue, particularly one that result\n \n Inside of `/chain`:\n \n-- Restart anvil\n-- Run `forge clean`\n-- Move back to the root directory, then run `yarn build`\n+-   Restart anvil\n+-   Run `forge clean`\n+-   Move back to the root directory, then run `yarn build`\n \n After, you can seed and deploy the cap table with either of the above options. If the bug persists, please open an issue with an attached screenshot and steps to reproduce.\n \n-## Testing\n+## Testing Web3\n+\n+Run all smart contracts tests\n+\n+`yarn test`\n+\n+## Testing Web2\n+\n+### Unit tests\n+\n+Run all javascript unit tests with jest\n+\n+`yarn test-js`\n+\n+### Integration tests\n+\n+Deploy a cap table to local anvil server through a local web2 server. The chain event listener is also run to ensure the events are properly mirrored into the mongo database. NOTE: running this deletes your local mongo collections first\n+\n+`yarn test-js-integration`\n+\n+Integration test setup from no active processes:\n \n-To run tests for the smart contracts, run `yarn test`.\n-This will run all the tests defined in the test suite and output the results to the console.\n+-   Terminal 1: `docker compose up`\n+-   Terminal 2: `anvil`\n+-   Terminal 3: `cd chain && forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast`\n+    -   In MongoDB compass, create/update `implementation_address` and `factory_address` in `jest-integration.factories`\n+        -   If the `jest-integration` MongoDB databases dont exist: `cd .. && yarn test-js-integration`\n+    -   Run `yarn test-js-integration`!\n \n ## Contributing\n "
            },
            {
                "filename": "chain/.env.example",
                "additions": 13,
                "deletions": 6,
                "patch": "@@ -1,6 +1,13 @@\n-OPTIMISM_GOERLI_RPC_URL=UPDATE_ME\n-LOCAL_RPC_URL=http://127.0.0.1:8545\n-PRIVATE_KEY_FAKE_ACCOUNT=UPDATE_ME\n-PRIVATE_KEY_POET_TEST=UPDATE_ME\n-ETHERSCAN_OPTIMISM_API_KEY=UPDATE_ME\n-ETHERSCAN_ETHEREUM_API_KEY=UPDATE_ME\n+# RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n+RPC_URL=http://127.0.0.1:8545\n+\n+# Change this to the chain id of the network you are deploying to\n+# Use 31337 for Anvil's, 32586980208 for Arbitrum's\n+CHAIN_ID=31337\n+\n+# Update with the private key of the account that will be used to deploy the contracts\n+PRIVATE_KEY=UPDATE_ME\n+\n+# Etherscan API keys\n+ETHERSCAN_L2_API_KEY=UPDATE_ME\n+ETHERSCAN_L1_API_KEY=UPDATE_ME"
            },
            {
                "filename": "chain/foundry.toml",
                "additions": 2,
                "deletions": 3,
                "patch": "@@ -8,11 +8,10 @@ cbor_metadata = false\n \n \n [rpc_endpoints]\n-optimism_goerli = \"${OPTIMISM_GOERLI_RPC_URL}\"\n-local = \"${LOCAL_RPC_URL}\"\n+rpc_url = \"${RPC_URL}\"\n \n [etherscan]\n-optimism_goerli_etherscan = { key = \"${ETHERSCAN_OPTIMISM_API_KEY}\", chain = \"goerli\" }\n+optimism_goerli_etherscan = { key = \"${ETHERSCAN_L2_API_KEY}\", chain = \"goerli\" }\n \n \n "
            },
            {
                "filename": "chain/script/CapTableFactory.s.sol",
                "additions": 6,
                "deletions": 6,
                "patch": "@@ -9,26 +9,26 @@ import \"../src/CapTableFactory.sol\";\n \n /// @dev Test deployment using `forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast`\n contract DeployCapTableFactoryDeployLocalScript is Script {\n-    uint256 deployerPrivateKeyFakeAccount;\n+    uint256 deployerPrivateKey;\n \n     function setUp() public {\n         console.log(\"Upgrading CapTableFactory with CapTable implementation\");\n \n-        deployerPrivateKeyFakeAccount = vm.envUint(\"PRIVATE_KEY_FAKE_ACCOUNT\");\n+        deployerPrivateKey = vm.envUint(\"PRIVATE_KEY\");\n     }\n \n     function run() external {\n         console.log(\"Deploying CapTableFactory and CapTable implementation\");\n \n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTable capTable = new CapTable();\n         console.log(\"CapTable implementation deployed at:\", address(capTable));\n \n         vm.stopBroadcast(); // End the transaction\n \n         // Deploy CapTableFactory with the address of CapTable implementation\n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTableFactory capTableFactory = new CapTableFactory(address(capTable));\n         console.log(\"CapTableFactory deployed at:\", address(capTableFactory));\n@@ -38,15 +38,15 @@ contract DeployCapTableFactoryDeployLocalScript is Script {\n \n     /// @dev Run using `forge tx script/CapTableFactory.s.sol upgradeCapTable [0x...] --fork-url http://localhost:8545 --broadcast`\n     function upgradeCapTable(address factory) external {\n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTable capTable = new CapTable();\n         console.log(\"CapTable implementation deployed at:\", address(capTable));\n \n         vm.stopBroadcast(); // End the transaction\n \n         // Upgrade CapTableFactory with the address of CapTable implementation\n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTableFactory capTableFactory = CapTableFactory(factory);\n "
            },
            {
                "filename": "chain/src/CapTable.sol",
                "additions": 6,
                "deletions": 0,
                "patch": "@@ -423,6 +423,12 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n         return stockClasses.length;\n     }\n \n+    /// @inheritdoc ICapTable\n+    function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40) {\n+        ActivePosition storage position = positions.activePositions[stakeholderId][securityId];\n+        return (position.stock_class_id, position.quantity, position.share_price, position.timestamp);\n+    }\n+\n     /* Role Based Access Control */\n     modifier onlyOperator() {\n         /// @notice Admins are also considered Operators"
            },
            {
                "filename": "chain/src/interfaces/ICapTable.sol",
                "additions": 3,
                "deletions": 0,
                "patch": "@@ -74,6 +74,9 @@ interface ICapTable {\n \n     function getTotalActiveSecuritiesCount() external view returns (uint256);\n \n+    // Function to get the timestamp of an active position\n+    function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40);\n+\n     function issueStock(StockIssuanceParams calldata params) external;\n \n     function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external;"
            },
            {
                "filename": "jest.config.js",
                "additions": 203,
                "deletions": 0,
                "patch": "@@ -0,0 +1,203 @@\n+/**\n+ * For a detailed explanation regarding each configuration property, visit:\n+ * https://jestjs.io/docs/configuration\n+ */\n+\n+// This allows us to avoid messing with the state of people's standard dev database\n+process.env['DATABASE_OVERRIDE'] = 'jest-integration';\n+\n+/** @type {import('jest').Config} */\n+const config = {\n+  // All imported modules in your tests should be mocked automatically\n+  // automock: false,\n+\n+  // Stop running tests after `n` failures\n+  // bail: 0,\n+\n+  // The directory where Jest should store its cached dependency information\n+  // cacheDirectory: \"/tmp/jest_rs\",\n+\n+  // Automatically clear mock calls, instances, contexts and results before every test\n+  // clearMocks: false,\n+\n+  // Indicates whether the coverage information should be collected while executing the test\n+  // collectCoverage: false,\n+\n+  // An array of glob patterns indicating a set of files for which coverage information should be collected\n+  // collectCoverageFrom: undefined,\n+\n+  // The directory where Jest should output its coverage files\n+  // coverageDirectory: \"coverage\",\n+\n+  // An array of regexp pattern strings used to skip coverage collection\n+  // coveragePathIgnorePatterns: [\n+  //   \"/node_modules/\"\n+  // ],\n+\n+  // Indicates which provider should be used to instrument code for coverage\n+  // coverageProvider: \"babel\",\n+\n+  // A list of reporter names that Jest uses when writing coverage reports\n+  // coverageReporters: [\n+  //   \"json\",\n+  //   \"text\",\n+  //   \"lcov\",\n+  //   \"clover\"\n+  // ],\n+\n+  // An object that configures minimum threshold enforcement for coverage results\n+  // coverageThreshold: undefined,\n+\n+  // A path to a custom dependency extractor\n+  // dependencyExtractor: undefined,\n+\n+  // Make calling deprecated APIs throw helpful error messages\n+  // errorOnDeprecated: false,\n+\n+  // The default configuration for fake timers\n+  // fakeTimers: {\n+  //   \"enableGlobally\": false\n+  // },\n+\n+  // Force coverage collection from ignored files using an array of glob patterns\n+  // forceCoverageMatch: [],\n+\n+  // A path to a module which exports an async function that is triggered once before all test suites\n+  // globalSetup: undefined,\n+\n+  // A path to a module which exports an async function that is triggered once after all test suites\n+  // globalTeardown: undefined,\n+\n+  // A set of global variables that need to be available in all test environments\n+  // globals: {},\n+\n+  // The maximum amount of workers used to run your tests. Can be specified as % or a number. E.g. maxWorkers: 10% will use 10% of your CPU amount + 1 as the maximum worker number. maxWorkers: 2 will use a maximum of 2 workers.\n+  // maxWorkers: \"50%\",\n+\n+  // An array of directory names to be searched recursively up from the requiring module's location\n+  // moduleDirectories: [\n+  //   \"node_modules\"\n+  // ],\n+\n+  // An array of file extensions your modules use\n+  // moduleFileExtensions: [\n+  //   \"js\",\n+  //   \"mjs\",\n+  //   \"cjs\",\n+  //   \"jsx\",\n+  //   \"ts\",\n+  //   \"tsx\",\n+  //   \"json\",\n+  //   \"node\"\n+  // ],\n+\n+  // A map from regular expressions to module names or to arrays of module names that allow to stub out resources with a single module\n+  // moduleNameMapper: {},\n+\n+  // An array of regexp pattern strings, matched against all module paths before considered 'visible' to the module loader\n+  // modulePathIgnorePatterns: [],\n+\n+  // Activates notifications for test results\n+  // notify: false,\n+\n+  // An enum that specifies notification mode. Requires { notify: true }\n+  // notifyMode: \"failure-change\",\n+\n+  // A preset that is used as a base for Jest's configuration\n+  preset: \"ts-jest\",\n+\n+  // Run tests from one or more projects\n+  // projects: undefined,\n+\n+  // Use this configuration option to add custom reporters to Jest\n+  // reporters: undefined,\n+\n+  // Automatically reset mock state before every test\n+  // resetMocks: false,\n+\n+  // Reset the module registry before running each individual test\n+  // resetModules: false,\n+\n+  // A path to a custom resolver\n+  // resolver: undefined,\n+\n+  // Automatically restore mock state and implementation before every test\n+  // restoreMocks: false,\n+\n+  // The root directory that Jest should scan for tests and modules within\n+  // rootDir: undefined,\n+\n+  // A list of paths to directories that Jest should use to search for files in\n+  // roots: [\n+  //   \"<rootDir>\"\n+  // ],\n+\n+  // Allows you to use a custom runner instead of Jest's default test runner\n+  // runner: \"jest-runner\",\n+\n+  // The paths to modules that run some code to configure or set up the testing environment before each test\n+  // setupFiles: [],\n+\n+  // A list of paths to modules that run some code to configure or set up the testing framework before each test\n+  // setupFilesAfterEnv: [],\n+\n+  // The number of seconds after which a test is considered as slow and reported as such in the results.\n+  // slowTestThreshold: 5,\n+\n+  // A list of paths to snapshot serializer modules Jest should use for snapshot testing\n+  // snapshotSerializers: [],\n+\n+  // The test environment that will be used for testing\n+  // testEnvironment: \"jest-environment-node\",\n+\n+  // Options that will be passed to the testEnvironment\n+  // testEnvironmentOptions: {},\n+\n+  // Adds a location field to test results\n+  // testLocationInResults: false,\n+\n+  // The glob patterns Jest uses to detect test files\n+  // testMatch: [\n+  //   \"**/__tests__/**/*.[jt]s?(x)\",\n+  //   \"**/?(*.)+(spec|test).[tj]s?(x)\"\n+  // ],\n+\n+  // An array of regexp pattern strings that are matched against all test paths, matched tests are skipped\n+  // testPathIgnorePatterns: [\n+  //   \"/node_modules/\"\n+  // ],\n+\n+  // The regexp pattern or array of patterns that Jest uses to detect test files\n+  // testRegex: [],\n+\n+  // This option allows the use of a custom results processor\n+  // testResultsProcessor: undefined,\n+\n+  // This option allows use of a custom test runner\n+  // testRunner: \"jest-circus/runner\",\n+\n+  // A map from regular expressions to paths to transformers\n+  transform: {\n+    \".*\\\\.(tsx?|js)$\": \"ts-jest\",\n+  },\n+\n+  // An array of regexp pattern strings that are matched against all source file paths, matched files will skip transformation\n+  // transformIgnorePatterns: [\n+  //   \"/node_modules/\",\n+  //   \"\\\\.pnp\\\\.[^\\\\/]+$\"\n+  // ],\n+\n+  // An array of regexp pattern strings that are matched against all modules before the module loader will automatically return a mock for them\n+  // unmockedModulePathPatterns: undefined,\n+\n+  // Indicates whether each individual test should be reported during the run\n+  verbose: true,\n+\n+  // An array of regexp patterns that are matched against all source file paths before re-running tests in watch mode\n+  // watchPathIgnorePatterns: [],\n+\n+  // Whether to use watchman for file crawling\n+  // watchman: true,\n+};\n+\n+export default config;"
            },
            {
                "filename": "package.json",
                "additions": 15,
                "deletions": 17,
                "patch": "@@ -2,32 +2,26 @@\n     \"name\": \"tap-cap-table\",\n     \"version\": \"1.0.0-alpha.0\",\n     \"private\": true,\n-    \"author\": \"Alex Palmer <alex@poet.network>, Victor Mimo <victor@poet.network>\",\n+    \"author\": \"Alex Palmer, Victor Mimo\",\n     \"license\": \"MIT\",\n-    \"description\": \"Transfer Agent Protocol compliant cap table\",\n+    \"description\": \"Transfer Agent Protocol onchain cap table\",\n     \"type\": \"module\",\n     \"scripts\": {\n-        \"start\": \"nodemon src/server.js\",\n+        \"prod\": \"npx tsx src/server.js --finalized-only\",\n+        \"dev\": \"npx tsx watch src/server.js\",\n         \"eslint\": \"eslint --cache --cache-location=node_modules/.cache/.eslintcache --fix\",\n         \"lint\": \"yarn run eslint . --ext .js,.jsx,.ts,.tsx\",\n         \"format\": \"prettier '**/*' --ignore-unknown --write\",\n         \"typecheck\": \"concurrently --raw yarn:typecheck:*\",\n         \"typecheck:app\": \"tsc --noEmit\",\n-        \"typecheck:cypress\": \"tsc --noEmit -p cypress/tsconfig.json\",\n         \"prepare\": \"husky install\",\n-        \"build\": \"cd chain && node ../src/scripts/deployAndLinkLibs.js\",\n+        \"build\": \"cd chain && npx tsx ../src/scripts/deployAndLinkLibs.js\",\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n-        \"deseed\": \"node src/db/scripts/deseed.js\",\n+        \"deseed\": \"npx tsx src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\",\n+        \"test-js\": \"jest --testPathPattern src/tests/unit\",\n+        \"test-js-integration\": \"jest --testPathPattern src/tests/integration\",\n         \"validate-poet-files\": \"cd ocf && yarn validate-poet-files\",\n-        \"test-onchain-cap-table-local\": \"node src/chain-operations/capTable.cjs local\",\n-        \"test-onchain-cap-table-optimism-goerli\": \"node src/chain-operations/capTable.cjs optimism-goerli\",\n-        \"test-onchain-cap-table-factory-local\": \"node src/chain-operations/capTableFactory.cjs local\",\n-        \"test-onchain-cap-table-factory-optimism-goerli\": \"node src/chain-operations/capTableFactory.cjs optimism-goerli\",\n-        \"forge-deploy-captable-local\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-optimism-goerli\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-factory-local\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-factory-optimism-goerli\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n         \"export-manifest\": \"cd src/db/samples && zip -r notPoet.zip notPoet && mv notPoet.zip $HOME/Downloads\"\n     },\n     \"dependencies\": {\n@@ -40,7 +34,9 @@\n         \"ethers\": \"^6.7.1\",\n         \"express\": \"^4.18.2\",\n         \"mongoose\": \"^7.4.2\",\n+        \"npx\": \"^10.2.2\",\n         \"solc\": \"^0.8.20\",\n+        \"tsx\": \"^4.7.0\",\n         \"xstate\": \"^4.38.2\",\n         \"yauzl\": \"^2.10.0\"\n     },\n@@ -50,18 +46,20 @@\n         \"maintained node versions\"\n     ],\n     \"devDependencies\": {\n-        \"@types/node\": \"^20.3.2\",\n+        \"@types/jest\": \"^29.5.11\",\n+        \"@types/node\": \"^20.11.5\",\n         \"@types/uuid\": \"^9.0.2\",\n         \"eslint\": \"^8.21.0\",\n         \"eslint-config-next\": \"^13.1.6\",\n         \"eslint-config-prettier\": \"^8.5.0\",\n         \"eslint-plugin-import\": \"^2.26.0\",\n         \"husky\": \"^8.0.1\",\n+        \"jest\": \"^29.7.0\",\n         \"lint-staged\": \"^13.0.3\",\n-        \"nodemon\": \"^3.0.1\",\n         \"prettier\": \"^2.7.1\",\n         \"solhint\": \"^3.4.1\",\n-        \"typescript\": \"^5.1.6\",\n+        \"ts-jest\": \"^29.1.1\",\n+        \"typescript\": \"^5.3.3\",\n         \"uuid\": \"^9.0.0\"\n     }\n }"
            },
            {
                "filename": "src/app.js",
                "additions": 96,
                "deletions": 0,
                "patch": "@@ -0,0 +1,96 @@\n+import { config } from \"dotenv\";\n+import express, { json, urlencoded } from \"express\";\n+config();\n+\n+import { connectDB } from \"./db/config/mongoose.ts\";\n+\n+import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n+\n+// Routes\n+import { capTable as capTableRoutes } from \"./routes/capTable.ts\";\n+import historicalTransactions from \"./routes/historicalTransactions.js\";\n+import mainRoutes from \"./routes/index.js\";\n+import issuerRoutes from \"./routes/issuer.js\";\n+import stakeholderRoutes from \"./routes/stakeholder.js\";\n+import stockClassRoutes from \"./routes/stockClass.js\";\n+import stockLegendRoutes from \"./routes/stockLegend.js\";\n+import stockPlanRoutes from \"./routes/stockPlan.js\";\n+import transactionRoutes from \"./routes/transactions.js\";\n+import valuationRoutes from \"./routes/valuation.js\";\n+import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n+\n+import mongoose from \"mongoose\";\n+import { readIssuerById } from \"./db/operations/read.js\";\n+import { getIssuerContract } from \"./utils/caches.ts\";\n+\n+const app = express();\n+\n+const PORT = process.env.PORT;\n+\n+// Middleware to get or create contract instance\n+// the listener is first started on deployment, then here as a backup\n+const contractMiddleware = async (req, res, next) => {\n+    if (!req.body.issuerId) {\n+        console.log(\"\u274c | No issuer ID\");\n+        res.status(400).send(\"issuerId is required\");\n+    }\n+\n+    // fetch issuer to ensure it exists\n+    const issuer = await readIssuerById(req.body.issuerId);\n+    if (!issuer) res.status(400).send(\"issuer not found \");\n+\n+    const { contract, provider } = await getIssuerContract(issuer);\n+    req.contract = contract;\n+    req.provider = provider;\n+    next();\n+};\n+app.use(urlencoded({ limit: \"50mb\", extended: true }));\n+app.use(json({ limit: \"50mb\" }));\n+app.enable(\"trust proxy\");\n+\n+app.use(\"/\", mainRoutes);\n+app.use(\"/cap-table\", capTableRoutes);\n+app.use(\"/issuer\", issuerRoutes);\n+app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n+app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n+// No middleware required since these are only created offchain\n+app.use(\"/stock-legend\", stockLegendRoutes);\n+app.use(\"/stock-plan\", stockPlanRoutes);\n+app.use(\"/valuation\", valuationRoutes);\n+app.use(\"/vesting-terms\", vestingTermsRoutes);\n+app.use(\"/historical-transactions\", historicalTransactions);\n+\n+// transactions\n+app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n+\n+export const startServer = async (finalizedOnly) => {\n+    /*\n+    processTo can be \"latest\" or \"finalized\". Latest helps during testing bc we dont have to wait for blocks to finalize\n+    */\n+\n+    // Connect to MongoDB\n+    const dbConn = await connectDB();\n+\n+    const server = app.listen(PORT, async () => {\n+        console.log(`\ud83d\ude80  Server successfully launched at ${PORT}`);\n+        // Asynchronous job to track web3 events in web2\n+        startEventProcessing(finalizedOnly, dbConn);\n+    });\n+\n+    return server;\n+};\n+\n+export const shutdownServer = async (server) => {\n+    if (server) {\n+        console.log(\"Shutting down app server...\");\n+        server.close();\n+    }\n+\n+    console.log(\"Waiting for event processing to stop...\");\n+    await stopEventProcessing();\n+\n+    if (mongoose.connection?.readyState === mongoose.STATES.connected) {\n+        console.log(\"Disconnecting from mongo...\");\n+        await mongoose.disconnect();\n+    }\n+};"
            },
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 16,
                "deletions": 69,
                "patch": "@@ -2,108 +2,55 @@ import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import CAP_TABLE_FACTORY from \"../../chain/out/CapTableFactory.sol/CapTableFactory.json\" assert { type: \"json\" };\n+import { readFactory } from \"../db/operations/read.js\";\n import { toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n-import { readFactory } from \"../db/operations/read.js\";\n+import getProvider from \"./getProvider.js\";\n \n config();\n \n-async function deployCapTableLocal(issuerId, issuerName, initial_shares_authorized) {\n-    // Replace with your private key and provider endpoint\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n-    const customNetwork = {\n-        chainId: 31337,\n-        name: \"local\",\n-    };\n-    const provider = new ethers.JsonRpcProvider(\"http://127.0.0.1:8545\", customNetwork);\n-    const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-    console.log(\"\ud83d\uddfd | Wallet address \", wallet.address);\n-\n-    const factory = await readFactory();\n-    const factoryAddress = factory[0].factory_address;\n-\n-    console.log('factory ', factory)\n-    \n-    if(!factoryAddress) {\n-        throw new Error(`\u274c | Factory address not found`);\n-    }\n-   \n-    const capTableFactory = new ethers.Contract(factoryAddress, CAP_TABLE_FACTORY.abi, wallet);\n-\n-    console.log(\n-        `\u2705 | Issuer id inside of deployment: ${issuerId},\n-\t\t\u2705 | Issuer name inside of deployment: ${issuerName},\n-\t\t\u2705 | With initial shares: ${initial_shares_authorized}`\n-    );\n-\n-    const tx = await capTableFactory.createCapTable(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));\n-    await tx.wait();\n-\n-    const capTableCount = await capTableFactory.getCapTableCount();\n-\n-    const latestCapTableProxyContractAddress = await capTableFactory.capTableProxies(capTableCount - BigInt(1));\n-\n-    const contract = new ethers.Contract(latestCapTableProxyContractAddress, CAP_TABLE.abi, wallet); \n+async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n+    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n \n-    console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n-    console.log(\"cap table contract address \", latestCapTableProxyContractAddress);\n-    const libraries = getTXLibContracts(latestCapTableProxyContractAddress, wallet);\n+    const provider = getProvider();\n \n-    return {\n-        contract,\n-        provider,\n-        address: latestCapTableProxyContractAddress,\n-        libraries,\n-    };\n-}\n-\n-async function deployCapTableOptimismGoerli(issuerId, issuerName, initial_shares_authorized) {\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_POET_TEST;\n-    const provider = new ethers.JsonRpcProvider(process.env.OPTIMISM_GOERLI_RPC_URL);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n \n-    const factory = await readFactory(); // Assumes this function fetches the factory address for Goerli\n+    console.log(\"\ud83d\uddfd | Wallet address: \", wallet.address);\n+\n+    const factory = await readFactory();\n     const factoryAddress = factory[0].factory_address;\n \n-    if(!factoryAddress) {\n+    console.log(\"factory \", factory);\n+\n+    if (!factoryAddress) {\n         throw new Error(`\u274c | Factory address not found`);\n     }\n \n     const capTableFactory = new ethers.Contract(factoryAddress, CAP_TABLE_FACTORY.abi, wallet);\n \n-    console.log(\n-        `\u2705 | Issuer id inside of deployment: ${issuerId},\n-        \u2705 | Issuer name inside of deployment: ${issuerName},\n-        \u2705 | With initial shares: ${initial_shares_authorized}`\n-    );\n-\n     const tx = await capTableFactory.createCapTable(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));\n     await tx.wait();\n \n     const capTableCount = await capTableFactory.getCapTableCount();\n+\n+    console.log(\"\ud83d\udcc4 | Cap table count: \", capTableCount);\n+\n     const latestCapTableProxyContractAddress = await capTableFactory.capTableProxies(capTableCount - BigInt(1));\n \n     const contract = new ethers.Contract(latestCapTableProxyContractAddress, CAP_TABLE.abi, wallet);\n \n     console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n-    console.log(\"cap table contract address \", latestCapTableProxyContractAddress);\n+    console.log(\"\u2705 | Cap table contract address \", latestCapTableProxyContractAddress);\n     const libraries = getTXLibContracts(latestCapTableProxyContractAddress, wallet);\n \n     return {\n         contract,\n         provider,\n         address: latestCapTableProxyContractAddress,\n         libraries,\n+        deployHash: tx.hash,\n     };\n }\n \n-async function deployCapTable(chain, issuerId, issuerName, initial_shares_authorized) {\n-    if (chain === \"local\") {\n-        return deployCapTableLocal(issuerId, issuerName, initial_shares_authorized);\n-    } else if (chain === \"optimism-goerli\") {\n-        return deployCapTableOptimismGoerli(issuerId, issuerName, initial_shares_authorized);\n-    } else {\n-        throw new Error(`\u274c | Unsupported chain: ${chain}`);\n-    }\n-}\n export default deployCapTable;"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 8,
                "deletions": 39,
                "patch": "@@ -1,50 +1,19 @@\n-import { ethers } from \"ethers\";\n import { config } from \"dotenv\";\n+import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n+import getProvider from \"./getProvider.js\";\n \n config();\n \n+export const getContractInstance = (address) => {\n+    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n \n-async function getLocalContractInstance(address) {\n-    const CONTRACT_ADDRESS_LOCAL = address;\n-\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n-    const LOCAL_RPC_URL = process.env.LOCAL_RPC_URL;\n-\n-    const customNetwork = {\n-        chainId: 31337,\n-        name: \"local\",\n-    };\n-\n-    const provider = new ethers.JsonRpcProvider(LOCAL_RPC_URL, customNetwork);\n-    const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-    const contract = new ethers.Contract(CONTRACT_ADDRESS_LOCAL, CAP_TABLE.abi, wallet);\n-    const libraries = getTXLibContracts(contract.target, wallet)\n-    return { contract, provider, libraries };\n-}\n-\n-async function getOptimismGoerliContractInstance(address) {\n-    const CONTRACT_ADDRESS_OPTIMISM_GOERLI = address;\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_POET_TEST;\n+    const provider = getProvider();\n \n-    const provider = new ethers.JsonRpcProvider(process.env.OPTIMISM_GOERLI_RPC_URL);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-    const contract = new ethers.Contract(CONTRACT_ADDRESS_OPTIMISM_GOERLI, abi, wallet);\n-    const libraries = getTXLibContracts(contract.target, wallet)\n+    const contract = new ethers.Contract(address, CAP_TABLE.abi, wallet);\n+    const libraries = getTXLibContracts(contract.target, wallet);\n \n-\n-    return { contract, provider, libraries};\n-}\n-\n-async function getContractInstance(chain, address) {\n-    if (chain === \"local\") {\n-        return getLocalContractInstance(address);\n-    } else if (chain === \"optimism-goerli\") {\n-        return getOptimismGoerliContractInstance(address);\n-    } else {\n-        throw new Error(`Unsupported chain: ${chain}`);\n-    }\n+    return { contract, provider, libraries };\n }\n-\n-export default getContractInstance;"
            },
            {
                "filename": "src/chain-operations/getProvider.js",
                "additions": 26,
                "deletions": 0,
                "patch": "@@ -0,0 +1,26 @@\n+import { config } from \"dotenv\";\n+import { ethers } from \"ethers\";\n+config();\n+\n+const RPC_URL = process.env.RPC_URL;\n+const CHAIN_ID = process.env.CHAIN_ID;\n+\n+const LOCAL_RPC = \"http://127.0.0.1:8545\";\n+\n+const getProvider = () => {\n+    let provider;\n+    if (RPC_URL === LOCAL_RPC) {\n+        console.log(\"\ud83d\udd17 | Connecting to local network: \", RPC_URL);\n+        const customNetwork = {\n+            chainId: parseInt(CHAIN_ID),\n+            name: \"local\",\n+        };\n+        provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n+    } else {\n+        console.log(\"\ud83d\udd17 | Connecting to network: \", RPC_URL);\n+        provider = new ethers.JsonRpcProvider(RPC_URL);\n+    }\n+    return provider;\n+};\n+\n+export default getProvider;"
            },
            {
                "filename": "src/chain-operations/seed.js",
                "additions": 2,
                "deletions": 4,
                "patch": "@@ -1,12 +1,10 @@\n import { convertAndReflectStakeholderOnchain } from \"../controllers/stakeholderController.js\";\n import { convertAndReflectStockClassOnchain } from \"../controllers/stockClassController.js\";\n-import { convertAndSeedIssuanceStockOnchain, convertAndSeedTransferStockOnchain } from \"../controllers/transactions/seed.js\";\n-import { getAllIssuerDataById } from \"../db/operations/read.js\";\n+import { getAllIssuerDataById, readIssuerById } from \"../db/operations/read.js\";\n+import { preProcessorCache } from \"../utils/caches.ts\";\n import { convertTimeStampToUint40, toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import { convertBytes16ToUUID, convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import { extractArrays } from \"../utils/flattenPreprocessorCache.js\";\n-import { preProcessorCache } from \"../utils/caches.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n import sleep from \"../utils/sleep.js\";\n \n export const verifyIssuerAndSeed = async (contract, id) => {"
            },
            {
                "filename": "src/chain-operations/transactionListener.js",
                "additions": 0,
                "deletions": 123,
                "patch": "@@ -1,123 +0,0 @@\n-import { verifyIssuerAndSeed } from \"./seed.js\";\n-import {\n-    handleStockCancellation,\n-    handleIssuerAuthorizedSharesAdjusted,\n-    handleStockAcceptance,\n-    handleStockReissuance,\n-    handleStockRepurchase,\n-    handleStockRetraction,\n-    handleStockClass,\n-    handleStakeholder,\n-    handleStockIssuance,\n-    handleStockTransfer,\n-    handleStockClassAuthorizedSharesAdjusted,\n-} from \"./transactionHandlers.js\";\n-import { AbiCoder } from \"ethers\";\n-import {\n-    IssuerAuthorizedSharesAdjustment,\n-    StockAcceptance,\n-    StockCancellation,\n-    StockClassAuthorizedSharesAdjustment,\n-    StockIssuance,\n-    StockReissuance,\n-    StockRepurchase,\n-    StockRetraction,\n-    StockTransfer,\n-} from \"./structs.js\";\n-\n-const abiCoder = new AbiCoder();\n-const eventQueue = [];\n-let issuerEventFired = false;\n-\n-const txMapper = {\n-    0: [\"INVALID\"],\n-    1: [\"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\", IssuerAuthorizedSharesAdjustment],\n-    2: [\"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\", StockClassAuthorizedSharesAdjustment],\n-    3: [\"STOCK_ACCEPTANCE\", StockAcceptance],\n-    4: [\"STOCK_CANCELLATION\", StockCancellation],\n-    5: [\"STOCK_ISSUANCE\", StockIssuance],\n-    6: [\"STOCK_REISSUANCE\", StockReissuance],\n-    7: [\"STOCK_REPURCHASE\", StockRepurchase],\n-    8: [\"STOCK_RETRACTION\", StockRetraction],\n-    9: [\"STOCK_TRANSFER\", StockTransfer],\n-};\n-\n-async function startOnchainListeners(contract, provider, issuerId, libraries) {\n-    console.log(\"\ud83c\udf10 | Initiating on-chain event listeners for issuer\", issuerId, \"at address\", contract.target);\n-\n-    libraries.txHelper.on(\"TxCreated\", async (_, txTypeIdx, txData, event) => {\n-        const [type, structType] = txMapper[txTypeIdx];\n-        const decodedData = abiCoder.decode([structType], txData);\n-        const { timestamp } = await provider.getBlock(event.blockNumber);\n-        eventQueue.push({ type, data: decodedData[0], issuerId, timestamp });\n-    });\n-\n-    contract.on(\"StakeholderCreated\", async (id, _) => {\n-        eventQueue.push({ type: \"STAKEHOLDER_CREATED\", data: id });\n-    });\n-\n-    contract.on(\"StockClassCreated\", async (id, _) => {\n-        eventQueue.push({ type: \"STOCK_CLASS_CREATED\", data: id });\n-    });\n-\n-    const issuerCreatedFilter = contract.filters.IssuerCreated;\n-    const issuerEvents = await contract.queryFilter(issuerCreatedFilter);\n-\n-    if (issuerEvents.length > 0 && !issuerEventFired) {\n-        const id = issuerEvents[0].args[0];\n-        console.log(\"IssuerCreated Event Emitted!\", id);\n-\n-        await verifyIssuerAndSeed(contract, id);\n-        issuerEventFired = true;\n-    }\n-\n-    setInterval(processEventQueue, 5000); // Process every 5 seconds\n-}\n-\n-async function processEventQueue() {\n-    const sortedEventQueue = eventQueue.sort((a, b) => a.timestamp - b.timestamp);\n-    while (sortedEventQueue.length > 0) {\n-        const event = eventQueue[0];\n-        switch (event.type) {\n-            case \"STAKEHOLDER_CREATED\":\n-                await handleStakeholder(event.data);\n-                break;\n-            case \"STOCK_CLASS_CREATED\":\n-                await handleStockClass(event.data);\n-                break;\n-            case \"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\":\n-                await handleIssuerAuthorizedSharesAdjusted(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\":\n-                await handleStockClassAuthorizedSharesAdjusted(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_ACCEPTANCE\":\n-                await handleStockAcceptance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_CANCELLATION\":\n-                await handleStockCancellation(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_ISSUANCE\":\n-                await handleStockIssuance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_REISSUANCE\":\n-                await handleStockReissuance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_REPURCHASE\":\n-                await handleStockRepurchase(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_RETRACTION\":\n-                await handleStockRetraction(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_TRANSFER\":\n-                await handleStockTransfer(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"INVALID\":\n-                throw new Error(\"Invalid transaction type\");\n-                break;\n-        }\n-        sortedEventQueue.shift();\n-    }\n-}\n-\n-export default startOnchainListeners;"
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 227,
                "deletions": 0,
                "patch": "@@ -0,0 +1,227 @@\n+import { AbiCoder, EventLog } from \"ethers\";\n+import { connectDB } from \"../db/config/mongoose.ts\";\n+import { withGlobalTransaction } from \"../db/operations/atomic.ts\";\n+import { readAllIssuers } from \"../db/operations/read.js\";\n+import { updateIssuerById } from \"../db/operations/update.js\";\n+import { getIssuerContract } from \"../utils/caches.ts\";\n+import sleep from \"../utils/sleep.js\";\n+import { verifyIssuerAndSeed } from \"./seed.js\";\n+import {\n+    IssuerAuthorizedSharesAdjustment,\n+    StockAcceptance,\n+    StockCancellation,\n+    StockClassAuthorizedSharesAdjustment,\n+    StockIssuance,\n+    StockReissuance,\n+    StockRepurchase,\n+    StockRetraction,\n+    StockTransfer,\n+} from \"./structs.js\";\n+import {\n+    handleIssuerAuthorizedSharesAdjusted,\n+    handleStakeholder,\n+    handleStockAcceptance,\n+    handleStockCancellation,\n+    handleStockClass,\n+    handleStockClassAuthorizedSharesAdjusted,\n+    handleStockIssuance,\n+    handleStockReissuance,\n+    handleStockRepurchase,\n+    handleStockRetraction,\n+    handleStockTransfer,\n+} from \"./transactionHandlers.js\";\n+\n+const abiCoder = new AbiCoder();\n+\n+interface QueuedEvent {\n+    type: string;\n+    timestamp: Date;\n+    data: any;\n+    o: EventLog;\n+}\n+\n+const contractFuncs = new Map([\n+    [\"StakeholderCreated\", handleStakeholder],\n+    [\"StockClassCreated\", handleStockClass],\n+]);\n+\n+const txMapper = {\n+    1: [IssuerAuthorizedSharesAdjustment, handleIssuerAuthorizedSharesAdjusted],\n+    2: [StockClassAuthorizedSharesAdjustment, handleStockClassAuthorizedSharesAdjusted],\n+    3: [StockAcceptance, handleStockAcceptance],\n+    4: [StockCancellation, handleStockCancellation],\n+    5: [StockIssuance, handleStockIssuance],\n+    6: [StockReissuance, handleStockReissuance],\n+    7: [StockRepurchase, handleStockRepurchase],\n+    8: [StockRetraction, handleStockRetraction],\n+    9: [StockTransfer, handleStockTransfer],\n+};\n+// (idx => type name) derived from txMapper\n+export const txTypes = Object.fromEntries(\n+    // @ts-ignore\n+    Object.entries(txMapper).map(([i, [_, f]]) => [i, f.name.replace(\"handle\", \"\")])\n+);\n+// (name => handler) derived from txMapper\n+export const txFuncs = Object.fromEntries(\n+    Object.entries(txMapper).map(([i, [_, f]]) => [txTypes[i], f])\n+);\n+\n+let _keepProcessing = true;\n+let _finishedProcessing = false;\n+\n+export const stopEventProcessing = async () => {\n+    _keepProcessing = false;\n+    while (!_finishedProcessing) {\n+        await sleep(50);\n+    }\n+}\n+\n+export const pollingSleepTime = 1000;\n+\n+export const startEventProcessing = async (finalizedOnly: boolean, dbConn) => {\n+    _keepProcessing = true;\n+    _finishedProcessing = false;\n+    while (_keepProcessing) {\n+        const issuers = await readAllIssuers();\n+\n+        // console.log(`Processing synchronously for ${issuers.length} issuers`);\n+        for (const issuer of issuers) {\n+            if (issuer.deployed_to) {\n+                const { contract, provider, libraries } = await getIssuerContract(issuer);\n+                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper, finalizedOnly);\n+            }\n+        }\n+        await sleep(pollingSleepTime);\n+    }\n+    _finishedProcessing = true;\n+};\n+\n+const processEvents = async (dbConn, contract, provider, issuer, txHelper, finalizedOnly, maxBlocks = 1500, maxEvents = 250) => {\n+    /*\n+    We process up to `maxEvents` across `maxBlocks` to ensure our transaction sizes dont get too big and bog down our db\n+    */\n+    let {_id: issuerId, last_processed_block: lastProcessedBlock, tx_hash: deployedTxHash} = issuer;\n+    const {number: latestBlock} = await provider.getBlock(finalizedOnly ? \"finalized\" : \"latest\");\n+    // console.log(\"Processing for issuer\", {issuerId, lastProcessedBlock, deployedTxHash, latestBlock});\n+    if (lastProcessedBlock === null) {\n+        const receipt = await provider.getTransactionReceipt(deployedTxHash);\n+        if (!receipt) {\n+            console.error(\"Deployment receipt not found\");\n+            return;\n+        }\n+        if (receipt.blockNumber > latestBlock) {\n+            console.log(\"Deployment tx not finalized\", {receipt, lastFinalizedBlock: latestBlock});\n+            return;\n+        }\n+        lastProcessedBlock = await issuerDeployed(issuerId, receipt, contract, dbConn);\n+    }\n+    const startBlock = lastProcessedBlock + 1;\n+    let endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n+    if (startBlock >= endBlock) {\n+        return;\n+    }\n+    \n+    // console.log(\" processing from\", { startBlock, endBlock });\n+    let events: QueuedEvent[] = [];\n+\n+    const contractEvents: EventLog[] = await contract.queryFilter(\"*\", startBlock, endBlock);\n+    for (const event of contractEvents) {\n+        const type = event?.fragment?.name;\n+        if (contractFuncs.has(type)) {\n+            const { timestamp } = await provider.getBlock(event.blockNumber);\n+            events.push({type, timestamp, data: event.args[0], o: event });\n+        }\n+    }\n+\n+    const txEvents: EventLog[] = await txHelper.queryFilter(txHelper.filters.TxCreated, startBlock, endBlock);\n+    for (const event of txEvents) {\n+        if (event.removed) {\n+            continue;\n+        }\n+        const [_len, typeIdx, txData] = event.args;\n+        const [structType, _] = txMapper[typeIdx];\n+        const decodedData = abiCoder.decode([structType], txData);\n+        const { timestamp } = await provider.getBlock(event.blockNumber);\n+        events.push({ type: txTypes[typeIdx], timestamp, data: decodedData[0], o: event });\n+    }\n+\n+    // Nothing to process\n+    if (events.length === 0) {\n+        await updateLastProcessed(issuerId, endBlock);\n+        return;\n+    }\n+\n+    // Process only up to a certain amount\n+    [events, endBlock] = trimEvents(events, maxEvents, endBlock);\n+\n+    await withGlobalTransaction(async () => {\n+        await persistEvents(issuerId, events);\n+        await updateLastProcessed(issuerId, endBlock);\n+    }, dbConn);\n+};\n+\n+const issuerDeployed = async (issuerId, receipt, contract, dbConn) => {\n+    console.log(\"New issuer was deployed\", {issuerId});\n+    const events = await contract.queryFilter(contract.filters.IssuerCreated);\n+    if (events.length === 0) {\n+        throw new Error(`No issuer events found!`);\n+    }\n+    const issuerCreatedEventId = events[0].args[0];\n+    console.log(\"IssuerCreated event captured!\", {issuerCreatedEventId});\n+    const lastProcessedBlock = receipt.blockNumber - 1;\n+    await withGlobalTransaction(async () => {\n+        await verifyIssuerAndSeed(contract, issuerCreatedEventId);\n+        await updateLastProcessed(issuerId, lastProcessedBlock);\n+    }, dbConn);\n+    return lastProcessedBlock;\n+};\n+\n+const persistEvents = async (issuerId, events: QueuedEvent[]) => {\n+    // Persist all the necessary changes for each event gathered in process events\n+    console.log(`${events.length} events to process for issuerId ${issuerId}`);\n+    for (const event of events) {\n+        const {type, data, timestamp} = event;\n+        const txHandleFunc = txFuncs[type];\n+        // console.log(\"persistEvent: \", {type, data, timestamp});\n+        if (txHandleFunc) {\n+            // @ts-ignore\n+            await txHandleFunc(data, issuerId, timestamp);\n+            continue;\n+        }\n+        const contractHandleFunc = contractFuncs.get(type);\n+        if (contractHandleFunc) {\n+            await contractHandleFunc(data);\n+            continue;\n+        }\n+        console.error(\"Invalid transaction type: \", type, event);\n+        throw new Error(`Invalid transaction type: \"${type}\"`);\n+    }\n+};\n+\n+export const trimEvents = (origEvents: QueuedEvent[], maxEvents, endBlock) => {\n+    // Sort for correct execution order\n+    let events = [...origEvents];\n+    events.sort((a, b) => a.o.blockNumber - b.o.blockNumber || a.o.transactionIndex - b.o.transactionIndex || a.o.index - b.o.index);\n+    let index = 0;    \n+    while (index < maxEvents && index < events.length) {\n+        // Include the entire next block\n+        const includeBlock = events[index].o.blockNumber;\n+        index++;\n+        while (index < events.length && events[index].o.blockNumber === includeBlock) {\n+            index++;\n+        }\n+    }\n+    // Nothing to trim!\n+    if (index >= events.length) {\n+        return [events, endBlock];\n+    }\n+    // We processed up to the last events' blockNumber\n+    // `index` is *exclusive* when trimming\n+    const useEvents = [...events.slice(0, index)];\n+    return [useEvents, useEvents[useEvents.length - 1].o.blockNumber];\n+};\n+\n+\n+const updateLastProcessed = async (issuerId, lastProcessedBlock) => {\n+    return updateIssuerById(issuerId, {last_processed_block: lastProcessedBlock});\n+};"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 6,
                "deletions": 5,
                "patch": "@@ -1,19 +1,20 @@\n-import mongoose from \"mongoose\";\n import dotenv from \"dotenv\";\n+import mongoose from \"mongoose\";\n \n dotenv.config();\n \n const DATABASE_URL = process.env.DATABASE_URL;\n+const DATABASE_OVERRIDE = process.env.DATABASE_OVERRIDE;\n \n-const connectDB = async () => {\n+export const connectDB = async () => {\n+    const connectOptions = DATABASE_OVERRIDE ? {dbName: DATABASE_OVERRIDE} : {};\n     try {\n-        await mongoose.connect(DATABASE_URL);\n+        await mongoose.connect(DATABASE_URL, connectOptions);\n         console.log(\"\u2705 | Mongo connected succesfully\");\n+        return mongoose.connection;\n     } catch (error) {\n         console.error(\"\u274c | Error connecting to Mongo\", error.message);\n         // Exit process with failure\n         process.exit(1);\n     }\n };\n-\n-export default connectDB;"
            },
            {
                "filename": "src/db/objects/Issuer.js",
                "additions": 2,
                "deletions": 0,
                "patch": "@@ -17,6 +17,8 @@ const IssuerSchema = new mongoose.Schema({\n     initial_shares_authorized: String,\n     comments: [String],\n     deployed_to: String,\n+    tx_hash: String,\n+    last_processed_block: { type: Number, default: null },\n     is_manifest_created: { type: Boolean, default: false },\n }, { timestamps: true });\n "
            },
            {
                "filename": "src/db/operations/atomic.ts",
                "additions": 100,
                "deletions": 0,
                "patch": "@@ -0,0 +1,100 @@\n+// Store a global mongo session to allows us to bundle CRUD operations into one transaction\n+\n+import { Connection, QueryOptions } from \"mongoose\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+type TQueryOptions = QueryOptions | null;\n+\n+let _globalSession = null;\n+\n+\n+export const setGlobalSession = (session) => {\n+    if (_globalSession !== null) {\n+        throw new Error(\n+            `globalSession is already set! ${_globalSession}. \n+            Nested transactions are not supported`\n+        );\n+    }\n+    _globalSession = session;\n+}\n+\n+export const clearGlobalSession = () => {\n+    _globalSession = null;\n+}\n+\n+const isReplSet = () => {\n+    if (process.env.DATABASE_REPLSET === \"1\") {\n+        return true;\n+    } \n+    return false;\n+}\n+\n+export const withGlobalTransaction = async (func: () => Promise<void>, useConn?: Connection) => {\n+    if (!isReplSet()) {\n+        // Transactions in mongo only work when running with --replSet\n+        //  https://www.mongodb.com/docs/manual/tutorial/convert-standalone-to-replica-set/\n+        return await func();\n+    }\n+\n+    // Wrap a user defined `func` in a global transaction\n+    const dbConn = useConn || await connectDB();\n+    await dbConn.transaction(async (session) => {\n+        setGlobalSession(session);\n+        try {\n+            return await func();\n+        } finally {\n+            clearGlobalSession();\n+        }\n+    });\n+}\n+\n+\n+const includeSession = (options?: TQueryOptions) => {\n+    let useOptions = options || {};\n+    if (_globalSession !== null) {\n+        if (useOptions.session) {\n+            throw new Error(`options.session is already set!: ${useOptions}`);\n+        }\n+        useOptions.session = _globalSession;\n+    }\n+    return useOptions;\n+}\n+\n+/* \n+Wrapped mongoose db calls. All mongo interaction should go through a function below\n+*/\n+\n+// CREATE\n+\n+export const save = (model, options?: TQueryOptions) => {\n+    return model.save(includeSession(options));\n+}\n+\n+// UPDATE\n+\n+export const findByIdAndUpdate = (model, id, updatedData, options?: TQueryOptions) => {\n+    return model.findByIdAndUpdate(id, updatedData, includeSession(options));\n+}\n+\n+// DELETE\n+\n+export const findByIdAndDelete = (model, id, options?: TQueryOptions) => {\n+    return model.findByIdAndDelete(id, includeSession(options));\n+}\n+\n+// QUERY\n+\n+export const findById = (model, id, projection?, options?: TQueryOptions) => {\n+    return model.findById(id, projection, includeSession(options));\n+}\n+\n+export const findOne = (model, filter, projection?, options?: TQueryOptions) => {\n+    return model.findOne(filter, projection, includeSession(options));\n+}\n+\n+export const find = (model, filter, projection?, options?: TQueryOptions) => {\n+    return model.find(filter, projection, includeSession(options));\n+}\n+\n+export const countDocuments = (model, options?: TQueryOptions) => {\n+    return model.countDocuments(includeSession(options));\n+}"
            },
            {
                "filename": "src/db/operations/create.js",
                "additions": 22,
                "deletions": 21,
                "patch": "@@ -1,60 +1,61 @@\n+import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n+import ConvertibleIssuance from \"../objects/transactions/issuance/ConvertibleIssuance.js\";\n+import EquityCompensationIssuance from \"../objects/transactions/issuance/EquityCompensationIssuance.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n+import { save } from \"./atomic.ts\";\n \n export const createIssuer = (issuerData) => {\n-    const issuer = new Issuer(issuerData);\n-    return issuer.save();\n+    return save(new Issuer(issuerData));\n };\n \n export const createStakeholder = (stakeholderData) => {\n-    const stakeholder = new Stakeholder(stakeholderData);\n-    return stakeholder.save();\n+    return save(new Stakeholder(stakeholderData));\n };\n \n export const createStockClass = (stockClassData) => {\n-    const stockClass = new StockClass(stockClassData);\n-    return stockClass.save();\n+    return save(new StockClass(stockClassData));\n };\n \n export const createStockLegendTemplate = (stockLegendTemplateData) => {\n-    const stockLegendTemplate = new StockLegendTemplate(stockLegendTemplateData);\n-    return stockLegendTemplate.save();\n+    return save(new StockLegendTemplate(stockLegendTemplateData));\n };\n \n export const createStockPlan = (stockPlanData) => {\n-    const stockPlan = new StockPlan(stockPlanData);\n-    return stockPlan.save();\n+    return save(new StockPlan(stockPlanData));\n };\n \n export const createValuation = (valuationData) => {\n-    const valuation = new Valuation(valuationData);\n-    return valuation.save();\n+    return save(new Valuation(valuationData));\n };\n \n export const createVestingTerms = (vestingTermsData) => {\n-    const vestingTerms = new VestingTerms(vestingTermsData);\n-    return vestingTerms.save();\n+    return save(new VestingTerms(vestingTermsData));\n };\n \n export const createHistoricalTransaction = (transactionHistoryData) => {\n-    const historicalTransaction = new HistoricalTransaction(transactionHistoryData);\n-    return historicalTransaction.save();\n+    return save(new HistoricalTransaction(transactionHistoryData));\n };\n \n export const createStockIssuance = (stockIssuanceData) => {\n-    const stockIssuance = new StockIssuance(stockIssuanceData);\n-    return stockIssuance.save();\n+    return save(new StockIssuance(stockIssuanceData));\n+};\n+\n+export const createEquityCompensationIssuance = (issuanceData) => {\n+    return save(new EquityCompensationIssuance(issuanceData));\n+};\n+\n+export const createConvertibleIssuance = (issuanceData) => {\n+    return save(new ConvertibleIssuance(issuanceData));\n };\n \n export const createStockTransfer = (stockTransferData) => {\n-    const stockTransfer = new StockTransfer(stockTransferData);\n-    return stockTransfer.save();\n+    return save(new StockTransfer(stockTransferData));\n };"
            },
            {
                "filename": "src/db/operations/delete.js",
                "additions": 9,
                "deletions": 8,
                "patch": "@@ -6,37 +6,38 @@ import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n+import { findByIdAndDelete } from \"./atomic.ts\";\n \n // TODO: since we're doing a time series db that's meant to be immutable, is this needed?\n \n export const deleteIssuerById = (issuerId) => {\n-    return Issuer.findByIdAndDelete(issuerId);\n+    return findByIdAndDelete(Issuer, issuerId);\n };\n \n export const deleteStakeholderById = (stakeholderId) => {\n-    return Stakeholder.findByIdAndDelete(stakeholderId);\n+    return findByIdAndDelete(Stakeholder, stakeholderId);\n };\n \n export const deleteStockClassById = (stockClassId) => {\n-    return StockClass.findByIdAndDelete(stockClassId);\n+    return findByIdAndDelete(StockClass, stockClassId);\n };\n \n export const deleteStockLegendTemplateById = (stockLegendTemplateId) => {\n-    return StockLegendTemplate.findByIdAndDelete(stockLegendTemplateId);\n+    return findByIdAndDelete(StockLegendTemplate, stockLegendTemplateId);\n };\n \n export const deleteStockPlanById = (stockPlanId) => {\n-    return StockPlan.findByIdAndDelete(stockPlanId);\n+    return findByIdAndDelete(StockPlan, stockPlanId);\n };\n \n export const deleteValuationById = (valuationId) => {\n-    return Valuation.findByIdAndDelete(valuationId);\n+    return findByIdAndDelete(Valuation, valuationId);\n };\n \n export const deleteVestingTermsById = (vestingTermsId) => {\n-    return VestingTerms.findByIdAndDelete(vestingTermsId);\n+    return findByIdAndDelete(VestingTerms, vestingTermsId);\n };\n \n export const deleteTransactionById = (transactionId) => {\n-    return StockIssuance.findByIdAndDelete(transactionId);\n+    return findByIdAndDelete(StockIssuance, transactionId);\n };"
            },
            {
                "filename": "src/db/operations/read.js",
                "additions": 25,
                "deletions": 40,
                "patch": "@@ -1,97 +1,84 @@\n+import Factory from \"../objects/Factory.js\";\n+import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import Factory from \"../objects/Factory.js\";\n+import { countDocuments, find, findById } from \"./atomic.ts\";\n \n // READ By ID\n export const readIssuerById = async (id) => {\n-    const issuer = await Issuer.findById(id);\n-    return issuer;\n+    return await findById(Issuer, id);\n };\n \n export const readStakeholderById = async (id) => {\n-    const stakeholder = await Stakeholder.findById(id);\n-    return stakeholder;\n+    return await findById(Stakeholder, id);\n };\n \n export const readStockClassById = async (id) => {\n-    const stockClass = await StockClass.findById(id);\n-    return stockClass;\n+    return await findById(StockClass, id);\n };\n \n export const readStockLegendTemplateById = async (id) => {\n-    const stockLegendTemplate = await StockLegendTemplate.findById(id);\n-    return stockLegendTemplate;\n+    return await findById(StockLegendTemplate, id);\n };\n \n export const readStockPlanById = async (id) => {\n-    const stockPlan = await StockPlan.findById(id);\n-    return stockPlan;\n+    return await findById(StockPlan, id);\n };\n \n export const readValuationById = async (id) => {\n-    const valuation = await Valuation.findById(id);\n-    return valuation;\n+    return await findById(Valuation, id);\n };\n \n export const readVestingTermsById = async (id) => {\n-    const vestingTerms = await VestingTerms.findById(id);\n-    return vestingTerms;\n+    return await findById(VestingTerms, id);\n };\n \n+// READ Multiple\n export const readHistoricalTransactionByIssuerId = async (issuerId) => {\n-    const historicalTransactions = await HistoricalTransaction.find({ issuer: issuerId }).populate(\"transaction\");\n-    return historicalTransactions;\n+    return await find(HistoricalTransaction, { issuer: issuerId }).populate(\"transaction\");\n };\n \n // COUNT\n export const countIssuers = async () => {\n-    const totalIssuers = await Issuer.countDocuments();\n-    return totalIssuers;\n+    return await countDocuments(Issuer);\n };\n \n export const countStakeholders = async () => {\n-    const totalStakeholders = await Stakeholder.countDocuments();\n-    return totalStakeholders;\n+    return await countDocuments(Stakeholder);\n };\n \n export const countStockClasses = async () => {\n-    const totalStockClasses = await StockClass.countDocuments();\n-    return totalStockClasses;\n+    return await countDocuments(StockClass);\n };\n \n export const countStockLegendTemplates = async () => {\n-    const totalTemplates = await StockLegendTemplate.countDocuments();\n-    return totalTemplates;\n+    return await countDocuments(StockLegendTemplate);\n };\n \n export const countStockPlans = async () => {\n-    const totalStockPlans = await StockPlan.countDocuments();\n-    return totalStockPlans;\n+    return await countDocuments(StockPlan);\n };\n \n export const countValuations = async () => {\n-    const totalValuations = await Valuation.countDocuments();\n-    return totalValuations;\n+    return await countDocuments(Valuation);\n };\n \n export const countVestingTerms = async () => {\n-    const totalVestingTerms = await VestingTerms.countDocuments();\n-    return totalVestingTerms;\n+    return await countDocuments(VestingTerms);\n };\n \n export const getAllIssuerDataById = async (issuerId) => {\n-    const issuerStakeholders = await Stakeholder.find({ issuer: issuerId });\n-    const issuerStockClasses = await StockClass.find({ issuer: issuerId });\n-    const issuerStockIssuances = await StockIssuance.find({ issuer: issuerId });\n-    const issuerStockTransfers = await StockTransfer.find({ issuer: issuerId });\n+    const issuerStakeholders = await find(Stakeholder, { issuer: issuerId });\n+    const issuerStockClasses = await find(StockClass, { issuer: issuerId });\n+    const issuerStockIssuances = await find(StockIssuance, { issuer: issuerId });\n+    const issuerStockTransfers = await find(StockTransfer, { issuer: issuerId });\n \n     return {\n         stakeholders: issuerStakeholders,\n@@ -102,11 +89,9 @@ export const getAllIssuerDataById = async (issuerId) => {\n };\n \n export const readAllIssuers = async () => {\n-    const issuers = await Issuer.find();\n-    return issuers;\n+    return await find(Issuer);\n }\n \n export const readFactory = async () => {\n-    const factory = await Factory.find();\n-    return factory;\n+    return await find(Factory);\n }\n\\ No newline at end of file"
            },
            {
                "filename": "src/db/operations/transactions.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,8 +1,8 @@\n import * as Acceptance from \"../objects/transactions/acceptance/index.js\";\n import * as Adjustment from \"../objects/transactions/adjustment/index.js\";\n import * as Cancellation from \"../objects/transactions/cancellation/index.js\";\n-import * as Exercise from \"../objects/transactions/exercise/index.js\";\n import * as Conversion from \"../objects/transactions/conversion/index.js\";\n+import * as Exercise from \"../objects/transactions/exercise/index.js\";\n import * as Issuance from \"../objects/transactions/issuance/index.js\";\n import * as Reissuance from \"../objects/transactions/reissuance/index.js\";\n import * as Release from \"../objects/transactions/release/index.js\";\n@@ -12,6 +12,7 @@ import * as ReturnToPool from \"../objects/transactions/return_to_pool/index.js\";\n import * as Split from \"../objects/transactions/split/index.js\";\n import * as Transfer from \"../objects/transactions/transfer/index.js\";\n import * as Vesting from \"../objects/transactions/vesting/index.js\";\n+import { save } from \"./atomic.ts\";\n \n const typeToModelType = {\n     // Acceptance\n@@ -91,7 +92,7 @@ const addTransactions = async (inputTransactions, issuerId) => {\n         inputTransaction = { ...inputTransaction, issuer: issuerId };\n         const ModelType = typeToModelType[inputTransaction.object_type];\n         if (ModelType) {\n-            const transaction = await new ModelType(inputTransaction).save();\n+            const transaction = await save(new ModelType(inputTransaction));\n             console.log(`${inputTransaction.object_type} transaction added. Details:`, JSON.stringify(transaction, null, 2));\n         } else {\n             console.log(`Unknown object type for transaction:`, JSON.stringify(inputTransaction, null, 2));"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 46,
                "deletions": 38,
                "patch": "@@ -1,95 +1,103 @@\n+import sleep from \"../../utils/sleep.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n-import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n+import StockAcceptance from \"../objects/transactions/acceptance/StockAcceptance.js\";\n+import IssuerAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.js\";\n+import StockClassAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.js\";\n import StockCancellation from \"../objects/transactions/cancellation/StockCancellation.js\";\n-import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n+import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockReissuance from \"../objects/transactions/reissuance/StockReissuance.js\";\n import StockRepurchase from \"../objects/transactions/repurchase/StockRepurchase.js\";\n-import StockAcceptance from \"../objects/transactions/acceptance/StockAcceptance.js\";\n-import StockClassAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.js\";\n-import IssuerAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.js\";\n+import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n+import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n+import { findByIdAndUpdate } from \"./atomic.ts\";\n+\n+\n+export const web3WaitTime = 5000;\n+\n+\n+const retryOnMiss = async (updateFunc, numRetries = 5, waitBase = null) => {\n+    /* kkolze: When polling `latest` instead of `finalized` web3 blocks, web3 can get ahead of mongo \n+      For example, see the `issuer.post(\"/create\"` code: the issuer is created in mongo after deployCapTable is called  \n+      We add retries to ensure the server routes have written to mongo  */\n+    let tried = 0;\n+    const waitMultiplier = waitBase || web3WaitTime;\n+    while (tried <= numRetries) {\n+        const res = await updateFunc();\n+        if (res !== null) {\n+            return res;\n+        }\n+        tried++;\n+        await sleep(tried * waitMultiplier, \"Returned null, retrying in \");\n+    }\n+}\n \n \n export const updateIssuerById = async (id, updatedData) => {\n-    const issuer = await Issuer.findByIdAndUpdate(id, updatedData, { new: true });\n-    return issuer;\n+    return await findByIdAndUpdate(Issuer, id, updatedData, { new: true });\n };\n \n export const updateStakeholderById = async (id, updatedData) => {\n-    const stakeholder = await Stakeholder.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stakeholder;\n+    return await retryOnMiss(async () => findByIdAndUpdate(Stakeholder, id, updatedData, { new: true }));\n };\n \n export const updateStockClassById = async (id, updatedData) => {\n-    const stockClass = await StockClass.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockClass;\n+    return await retryOnMiss(async () => findByIdAndUpdate(StockClass, id, updatedData, { new: true }));\n };\n \n export const updateStockLegendTemplateById = async (id, updatedData) => {\n-    const stockLegendTemplate = await StockLegendTemplate.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockLegendTemplate;\n+    return await findByIdAndUpdate(StockLegendTemplate, id, updatedData, { new: true });\n };\n \n export const updateStockPlanById = async (id, updatedData) => {\n-    const stockPlan = await StockPlan.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockPlan;\n+    return await findByIdAndUpdate(StockPlan, id, updatedData, { new: true });\n };\n \n export const updateValuationById = async (id, updatedData) => {\n-    const valuation = await Valuation.findByIdAndUpdate(id, updatedData, { new: true });\n-    return valuation;\n+    return await findByIdAndUpdate(Valuation, id, updatedData, { new: true });\n };\n \n export const updateVestingTermsById = async (id, updatedData) => {\n-    const vestingTerms = await VestingTerms.findByIdAndUpdate(id, updatedData, { new: true });\n-    return vestingTerms;\n+    return await findByIdAndUpdate(VestingTerms, id, updatedData, { new: true });\n };\n \n export const upsertStockIssuanceById = async (id, updatedData) => {\n-    const stockIssuance = await StockIssuance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockIssuance;\n+    return await findByIdAndUpdate(StockIssuance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockTransferById = async (id, updatedData) => {\n-    const stockTransfer = await StockTransfer.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockTransfer;\n+    return await findByIdAndUpdate(StockTransfer, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockCancellationById = async (id, updatedData) => {\n-    const stockCancellation = await StockCancellation.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockCancellation;\n+    return await findByIdAndUpdate(StockCancellation, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockRetractionById = async (id, updatedData) => {\n-    const stockRetraction = await StockRetraction.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockRetraction;\n+    return await findByIdAndUpdate(StockRetraction, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockReissuanceById = async (id, updatedData) => {\n-    const stockReissuance = await StockReissuance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockReissuance;\n+    return await findByIdAndUpdate(StockReissuance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockRepurchaseById = async (id, updatedData) => {\n-    const stockRepurchase = await StockRepurchase.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockRepurchase;\n+    return await findByIdAndUpdate(StockRepurchase, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockAcceptanceById = async (id, updatedData) => {\n-    const stockAcceptance = await StockAcceptance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockAcceptance;\n+    return await findByIdAndUpdate(StockAcceptance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockClassAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await StockClassAuthorizedSharesAdjustment.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockClassAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n };\n-export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await IssuerAuthorizedSharesAdjustment.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n \n-}\n+export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n+    return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n+};"
            },
            {
                "filename": "src/db/scripts/deseed.js",
                "additions": 4,
                "deletions": 40,
                "patch": "@@ -1,47 +1,11 @@\n-import mongoose from \"mongoose\";\n-import connectDB from \"../config/mongoose.js\";\n-import Issuer from \"../objects/Issuer.js\";\n-import Stakeholder from \"../objects/Stakeholder.js\";\n-import StockClass from \"../objects/StockClass.js\";\n-import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n-import StockPlan from \"../objects/StockPlan.js\";\n-import Valuation from \"../objects/Valuation.js\";\n-import VestingTerms from \"../objects/VestingTerms.js\";\n-import { typeToModelType } from \"../operations/transactions.js\"; // Import the typeToModelType object to delete all transactions\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n+import { deseedDatabase } from \"../../tests/integration/utils.ts\";\n \n-const deleteAllTransactions = async () => {\n-    for (const ModelType of Object.values(typeToModelType)) {\n-        await ModelType.deleteMany({});\n-    }\n-};\n-\n-const deleteAll = async () => {\n-    // Delete all documents from the collections\n-    await Issuer.deleteMany({});\n-    await Stakeholder.deleteMany({});\n-    await StockClass.deleteMany({});\n-    await StockLegendTemplate.deleteMany({});\n-    await StockPlan.deleteMany({});\n-    await Valuation.deleteMany({});\n-    await VestingTerms.deleteMany({});\n-    await HistoricalTransaction.deleteMany({});\n-    await deleteAllTransactions(); // Delete all transactions\n-};\n-\n-const deseedDatabase = async () => {\n+const runDeseed = async () => {\n     try {\n-        connectDB();\n-\n-        await deleteAll();\n-\n-        console.log(\"\u2705 Database deseeded successfully\");\n-\n-        // Close the database connection\n-        await mongoose.connection.close();\n+        await deseedDatabase();\n     } catch (err) {\n         console.log(\"\u274c Error deseeding database:\", err);\n     }\n };\n \n-deseedDatabase();\n+runDeseed();"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 89,
                "deletions": 0,
                "patch": "@@ -0,0 +1,89 @@\n+import { Router } from \"express\";\n+import Issuer from \"../db/objects/Issuer\";\n+import Stakeholder from \"../db/objects/Stakeholder\";\n+import StockClass from \"../db/objects/StockClass\";\n+import { StockIssuance } from \"../db/objects/transactions/issuance\";\n+import { getIssuerContract } from \"../utils/caches\";\n+import { decimalScaleValue } from \"../utils/convertToFixedPointDecimals\";\n+import { convertUUIDToBytes16 } from \"../utils/convertUUID\";\n+\n+export const capTable = Router();\n+\n+capTable.get(\"/\", async (req, res) => {\n+    res.send(\"Hello Cap Table!\");\n+});\n+\n+capTable.get(\"/latest\", async (req, res) => {\n+    /* \n+    TODO: handle this in the polling process? or maybe just cache it once in a while?\n+     It will get slow once we have 50+ stakeholders\n+    */\n+    const issuerId = req.query.issuerId;\n+    try {\n+        const stakeholders = await Stakeholder.find({issuer: issuerId});\n+        const stockClasses = await StockClass.find({issuer: issuerId});\n+        const issuer = await Issuer.findById(issuerId);\n+        // Grouping by stakeholder_id and stock_class_id, grab the records with the largest createdAt time\n+        const issuances = await StockIssuance.aggregate([\n+            {\n+                $group: {\n+                    _id: {\n+                        stakeholder_id: \"$stakeholder_id\",\n+                        stock_class_id: \"$stock_class_id\"\n+                    },\n+                    maxDate: { $max: \"$createdAt\" }\n+                }\n+            },\n+            {\n+                $lookup: {\n+                    from: \"stockissuances\",\n+                    let: { stakeholder_id: \"$_id.stakeholder_id\", stock_class_id: \"$_id.stock_class_id\", maxDate: \"$maxDate\" },\n+                    pipeline: [\n+                        {\n+                            $match: {\n+                                $expr: {\n+                                    $and: [\n+                                        { $eq: [\"$stakeholder_id\", \"$$stakeholder_id\"] },\n+                                        { $eq: [\"$stock_class_id\", \"$$stock_class_id\"] },\n+                                        { $eq: [\"$createdAt\", \"$$maxDate\"] }\n+                                    ]\n+                                }\n+                            }\n+                        }\n+                    ],\n+                    as: \"issuanceData\"\n+                }\n+            },\n+            { $unwind: \"$issuanceData\" },\n+            { $replaceRoot: { newRoot: \"$issuanceData\" } }\n+        ]);\n+      \n+        // We need to hit web3 to see which are actually valid\n+        const { contract } = await getIssuerContract(issuer);\n+        let holdings = [];\n+        const stakeholderMap = Object.fromEntries(stakeholders.map((x) => { return [x._id, x]; }));\n+        const stockClassMap = Object.fromEntries(stockClasses.map((x) => { return [x._id, x]; }));\n+        for (const issuance of issuances) {\n+            const { stakeholder_id, security_id, stock_class_id } = issuance;\n+            const [_, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+                convertUUIDToBytes16(stakeholder_id),\n+                convertUUIDToBytes16(security_id),\n+            );\n+            if (quantity == 0) {\n+                continue;\n+            }\n+            holdings.push({\n+                issuance,\n+                stockClass: stockClassMap[stock_class_id],\n+                stakeholder: stakeholderMap[stakeholder_id],\n+                quantity: Number(quantity) / decimalScaleValue,\n+                sharePrice: Number(sharePrice) / decimalScaleValue,\n+                timestamp: Number(timestamp) * 1000,\n+            });\n+        }\n+        res.send({ holdings, stockClasses, issuer });\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+})"
            },
            {
                "filename": "src/routes/index.js",
                "additions": 6,
                "deletions": 9,
                "patch": "@@ -1,14 +1,16 @@\n import { Router } from \"express\";\n import deployCapTable from \"../chain-operations/deployCapTable.js\";\n+import { updateIssuerById } from \"../db/operations/update.js\";\n import seedDB from \"../db/scripts/seed.js\";\n-import { contractCache } from \"../utils/caches.js\";\n import { convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import processManifest from \"../utils/processManifest.js\";\n-import { updateIssuerById } from \"../db/operations/update.js\";\n-import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n \n const router = Router();\n \n+router.get(\"/\", async (req, res) => {\n+    console.log(\"Welcome to TAP\");\n+    res.status(200).send(`Welcome to the future of Transfer Agents \ud83d\udcb8`);\n+});\n \n router.post(\"/mint-cap-table\", async (req, res) => {\n     try {\n@@ -17,14 +19,9 @@ router.post(\"/mint-cap-table\", async (req, res) => {\n         const issuer = await seedDB(manifest);\n \n         const issuerIdBytes16 = convertUUIDToBytes16(issuer._id);\n-        const { contract, address, provider, libraries } = await deployCapTable(req.chain, issuerIdBytes16, issuer.legal_name, issuer.initial_shares_authorized);\n+        const { contract, address, provider, libraries } = await deployCapTable(issuerIdBytes16, issuer.legal_name, issuer.initial_shares_authorized);\n \n         const savedIssuerWithDeployedTo = await updateIssuerById(issuer._id, { deployed_to: address });\n-\n-        // add contract to the cache and start listener\n-        contractCache[issuer._id] = { contract, provider, libraries };\n-        await startOnchainListeners(contract, provider, issuer._id, libraries);\n-\n         res.status(200).send({ issuer: savedIssuerWithDeployedTo });\n     } catch (error) {\n         console.error(`error: ${error}`);"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 2,
                "deletions": 11,
                "patch": "@@ -8,9 +8,6 @@ import { countIssuers, readIssuerById } from \"../db/operations/read.js\";\n import { convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n-import { contractCache } from \"../utils/caches.js\";\n-import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n-\n const issuer = Router();\n \n issuer.get(\"/\", async (req, res) => {\n@@ -42,8 +39,6 @@ issuer.get(\"/total-number\", async (req, res) => {\n });\n \n issuer.post(\"/create\", async (req, res) => {\n-    const { chain } = req;\n-\n     try {\n         // OCF doesn't allow extra fields in their validation\n         const incomingIssuerToValidate = {\n@@ -58,20 +53,16 @@ issuer.post(\"/create\", async (req, res) => {\n \n         const issuerIdBytes16 = convertUUIDToBytes16(incomingIssuerToValidate.id);\n         console.log(\"\ud83d\udcbe | Issuer id in bytes16 \", issuerIdBytes16);\n-        const { contract, provider, address, libraries } = await deployCapTable(\n-            chain,\n+        const { address, deployHash } = await deployCapTable(\n             issuerIdBytes16,\n             incomingIssuerToValidate.legal_name,\n             incomingIssuerToValidate.initial_shares_authorized\n         );\n \n-        // add contract to the cache and start listener\n-        contractCache[incomingIssuerToValidate.id] = { contract, provider, libraries };\n-        startOnchainListeners(contract, provider, incomingIssuerToValidate.id, libraries);\n-\n         const incomingIssuerForDB = {\n             ...incomingIssuerToValidate,\n             deployed_to: address,\n+            tx_hash: deployHash,\n         };\n \n         const issuer = await createIssuer(incomingIssuerForDB);"
            },
            {
                "filename": "src/routes/stakeholder.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -10,8 +10,8 @@ import {\n \n import stakeholderSchema from \"../../ocf/schema/objects/Stakeholder.schema.json\" assert { type: \"json\" };\n import { createStakeholder } from \"../db/operations/create.js\";\n-import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n import { readIssuerById } from \"../db/operations/read.js\";\n+import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n const stakeholder = Router();\n "
            },
            {
                "filename": "src/routes/transactions.js",
                "additions": 59,
                "deletions": 2,
                "patch": "@@ -9,6 +9,8 @@ import stockRepurchaseSchema from \"../../ocf/schema/objects/transactions/repurch\n import stockRetractionSchema from \"../../ocf/schema/objects/transactions/retraction/StockRetraction.schema.json\" assert { type: \"json\" };\n import stockClassAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n import issuerAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n+import equityCompensationIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/EquityCompensationIssuance.schema.json\" assert { type: \"json\" };\n+import convertibleIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/ConvertibleIssuance.schema.json\" assert { type: \"json\" };\n \n import { convertAndAdjustIssuerAuthorizedSharesOnChain } from \"../controllers/issuerController.js\";\n import { convertAndAdjustStockClassAuthorizedSharesOnchain } from \"../controllers/stockClassController.js\";\n@@ -19,6 +21,8 @@ import { convertAndCreateReissuanceStockOnchain } from \"../controllers/transacti\n import { convertAndCreateRepurchaseStockOnchain } from \"../controllers/transactions/repurchaseController.js\";\n import { convertAndCreateRetractionStockOnchain } from \"../controllers/transactions/retractionController.js\";\n import { convertAndCreateTransferStockOnchain } from \"../controllers/transactions/transferController.js\";\n+import { createEquityCompensationIssuance } from \"../db/operations/create.js\";\n+import { createConvertibleIssuance } from \"../db/operations/create.js\";\n \n import { readIssuerById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n@@ -272,8 +276,6 @@ transactions.post(\"/adjust/stock-class/authorized-shares\", async (req, res) => {\n \n         console.log(\"stockClassAuthorizedSharesAdjustment\", stockClassAuthorizedSharesAdjustment);\n \n-        // delete incomingStockClassAdjustment.stockClassId;\n-\n         // NOTE: schema validation does not include stakeholder, stockClassId, however these properties are needed on to be passed on chain\n         await validateInputAgainstOCF(stockClassAuthorizedSharesAdjustment, stockClassAuthorizedSharesAdjustmentSchema);\n \n@@ -289,4 +291,59 @@ transactions.post(\"/adjust/stock-class/authorized-shares\", async (req, res) => {\n     }\n });\n \n+transactions.post(\"/issuance/equity-compensation\", async (req, res) => {\n+    const { issuerId, data } = req.body;\n+\n+    try {\n+        // ensuring issuer exists\n+        await readIssuerById(issuerId);\n+\n+        const incomingEquityCompensationIssuance = {\n+            id: uuid(), // for OCF Validation\n+            security_id: uuid(), // for OCF Validation,\n+            date: new Date().toISOString().slice(0, 10), // for OCF Validation\n+            object_type: \"TX_EQUITY_COMPENSATION_ISSUANCE\",\n+            ...data,\n+        };\n+        await validateInputAgainstOCF(incomingEquityCompensationIssuance, equityCompensationIssuanceSchema);\n+\n+        // save to DB\n+        const createdIssuance = await createEquityCompensationIssuance(incomingEquityCompensationIssuance);\n+\n+        res.status(200).send({ equityCompensationIssuance: createdIssuance });\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+})\n+\n+\n+transactions.post(\"/issuance/convertible\", async (req, res) => {\n+    const { issuerId, data } = req.body;\n+\n+    try {\n+        // ensuring issuer exists\n+        await readIssuerById(issuerId);\n+\n+        const incomingConvertibleIssuance = {\n+            id: uuid(), // for OCF Validation\n+            security_id: uuid(), // for OCF Validation\n+            date: new Date().toISOString().slice(0, 10), // for OCF Validation\n+            object_type: \"TX_CONVERTIBLE_ISSUANCE\",\n+            ...data,\n+        };\n+\n+        console.log('incomingConvertibleIssuance', incomingConvertibleIssuance)\n+        await validateInputAgainstOCF(incomingConvertibleIssuance, convertibleIssuanceSchema);\n+\n+        // save to DB\n+        const createdIssuance = await createConvertibleIssuance(incomingConvertibleIssuance);\n+\n+        res.status(200).send({ convertibleIssuance: createdIssuance });\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+})\n+\n export default transactions;"
            },
            {
                "filename": "src/scripts/deployAndLinkLibs.js",
                "additions": 39,
                "deletions": 13,
                "patch": "@@ -1,13 +1,14 @@\n-import { spawn } from \"child_process\";\n import { config } from \"dotenv\";\n+import { spawn } from \"child_process\";\n import fs from \"fs\";\n import path from \"path\";\n import sleep from \"../utils/sleep.js\";\n \n config();\n \n-const privateKey = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n-\n+const PRIVATE_KEY = process.env.PRIVATE_KEY;\n+const RPC_URL = process.env.RPC_URL;\n+const CHAIN_ID = process.env.CHAIN_ID;\n const rootDirectory = \"./src/lib\";\n const excludeDirectory = \"./src/lib/transactions\";\n \n@@ -70,37 +71,62 @@ function getAllLibraries(dirPath) {\n \n async function deployLib(lib, libs) {\n     return new Promise((resolve, reject) => {\n-        console.log(`Deploying ${lib.libraryName || lib.fileName}\\n`);\n-        const librariesDepsArgs = lib.deps.map((idx) => [\"--libraries\", `${libs[idx].path}:${libs[idx].libraryName}:${libs[idx].address}`]).flat();\n+        console.log(`Starting deployment for ${lib.libraryName || lib.fileName}`);\n+\n+        // Logging the dependencies and their addresses\n+        const librariesDepsArgs = lib.deps\n+            .map((idx) => {\n+                console.log(`Dependency: ${libs[idx].libraryName}, Address: ${libs[idx].address}`);\n+                return [\"--libraries\", `${libs[idx].path}:${libs[idx].libraryName}:${libs[idx].address}`];\n+            })\n+            .flat();\n         console.log(\"librariesDepsArgs:\", librariesDepsArgs);\n+\n+        // Forge command arguments\n         const args = [\n             \"c\",\n             \"-r\",\n-            \"http://127.0.0.1:8545\",\n+            // Using the RPC_URL from .env\n+            RPC_URL,\n             \"--chain\",\n-            31337,\n+            CHAIN_ID,\n             \"--private-key\",\n-            privateKey,\n+            PRIVATE_KEY,\n             `${lib.path}:${lib.libraryName || lib.fileName}`,\n-            // \"--via-ir\",\n             \"--json\",\n             ...librariesDepsArgs,\n         ];\n+        // TODO: only log when things break console.log(`Forge command arguments: ${args.join(\" \")}`);\n+\n+        // Executing the forge command\n         const subprocess = spawn(\"forge\", args);\n \n         subprocess.stdout.on(\"data\", (data) => {\n-            lib.address = JSON.parse(data).deployedTo;\n-            console.log(`${lib.fileName} Deployed Successfully - Address: ${lib.address}\\n`);\n+            console.log(`stdout: ${data}`);\n+            try {\n+                lib.address = JSON.parse(data).deployedTo;\n+                console.log(`${lib.fileName} Deployed Successfully - Address: ${lib.address}`);\n+            } catch (err) {\n+                console.error(`Error parsing JSON from stdout: ${err}`);\n+            }\n         });\n \n         subprocess.stderr.on(\"data\", (data) => {\n-            reject(new Error(`${lib.fileName}::stderr: ${data}`));\n+            const errorDetails = data.toString(); // Convert Buffer to string\n+            console.error(`stderr: ${errorDetails}`);\n+        });\n+\n+        subprocess.on(\"error\", (error) => {\n+            console.error(`Spawn error: ${error}`);\n+            reject(new Error(`Error executing forge command for ${lib.fileName}`));\n         });\n \n         subprocess.on(\"close\", (code) => {\n             if (code !== 0) {\n-                reject(new Error(`${lib.fileName}::child process exited with code ${code}`));\n+                console.error(`${lib.fileName} deployment failed with exit code ${code}`);\n+                reject(new Error(`${lib.fileName} deployment failed. Details in the logs above.`));\n             } else {\n+                console.log(`${lib.fileName} deployment process exited successfully`);\n                 resolve();\n             }\n         });"
            },
            {
                "filename": "src/scripts/sampleData.js",
                "additions": 19,
                "deletions": 1,
                "patch": "@@ -1,5 +1,5 @@\n export const issuer = {\n-    legal_name: \"Poet Network Inc.\",\n+    legal_name: \"Banana Peppers Inc.\",\n     formation_date: \"2022-08-23\",\n     country_of_formation: \"US\",\n     country_subdivision_of_formation: \"DE\",\n@@ -174,6 +174,24 @@ export const stakeholder2 = (issuerId) => {\n         },\n     };\n };\n+\n+export const stakeholder3 = (issuerId) => {\n+    return {\n+        issuerId,\n+        data: {\n+            name: {\n+                legal_name: \"Kent Kolze\",\n+                first_name: \"Kent\",\n+                last_name: \"Kolze\",\n+            },\n+            issuer_assigned_id: \"\",\n+            stakeholder_type: \"INDIVIDUAL\",\n+            current_relationship: \"EMPLOYEE\",\n+            comments: [],\n+        },\n+    };\n+};\n+\n export const stockClass = (issuerId) => {\n     return {\n         issuerId,"
            },
            {
                "filename": "src/scripts/testAcceptance.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockAccept } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testCancellation.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -1,7 +1,7 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { issuer, stakeholder1, stakeholder2, stockCancel, stockClass, stockIssuance, stockTransfer } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { stockCancel } from \"./sampleData.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testIssuance.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,9 +1,9 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import Issuer from \"../db/objects/Issuer.js\";\n import Stakeholder from \"../db/objects/Stakeholder.js\";\n import StockClass from \"../db/objects/StockClass.js\";\n-import axios from \"axios\";\n import { stockIssuance } from \"./sampleData.js\";\n-import connectDB from \"../db/config/mongoose.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testIssuerAdjustment.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -1,7 +1,7 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { stockClass, stockClassAuthorizedSharesAdjust, issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testMintingCapTable.js",
                "additions": 15,
                "deletions": 15,
                "patch": "@@ -10,32 +10,32 @@ const main = async () => {\n \n     console.log(\"\u2705 | Issuer response \", issuerResponse.data);\n \n-    // await sleep(3000);\n+    await sleep(3000);\n \n-    // console.log(\"\u23f3 | Creating first stakeholder\");\n+    console.log(\"\u23f3 | Creating first stakeholder\");\n \n-    // // create two stakeholders\n-    // const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerResponse.data.issuer._id));\n+    // create two stakeholders\n+    const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerResponse.data.issuer._id));\n \n-    // console.log(\"\u2705 | stakeholder1Response\", stakeholder1Response.data);\n-    // console.log(\"\u2705 | finished\");\n+    console.log(\"\u2705 | stakeholder1Response\", stakeholder1Response.data);\n+    console.log(\"\u2705 | finished\");\n \n-    // await sleep(3000);\n+    await sleep(3000);\n \n-    // console.log(\"\u23f3 | Creating second stakeholder\u2026\");\n+    console.log(\"\u23f3 | Creating second stakeholder\u2026\");\n \n-    // const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerResponse.data.issuer._id));\n+    const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerResponse.data.issuer._id));\n \n-    // console.log(\"\u2705 | stakeholder2Response\", stakeholder2Response.data);\n+    console.log(\"\u2705 | stakeholder2Response\", stakeholder2Response.data);\n \n-    // await sleep(3000);\n+    await sleep(3000);\n \n-    // console.log(\"\u23f3| Creating stock class\");\n+    console.log(\"\u23f3| Creating stock class\");\n \n-    // // create stockClass\n-    // const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerResponse.data.issuer._id));\n+    // create stockClass\n+    const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerResponse.data.issuer._id));\n \n-    // console.log(\"\u2705 | stockClassResponse\", stockClassResponse.data);\n+    console.log(\"\u2705 | stockClassResponse\", stockClassResponse.data);\n };\n \n main()"
            },
            {
                "filename": "src/scripts/testReissuance.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockReissue } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testRepurchase.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,7 +1,7 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockRepurchase } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testRetraction.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockRetract } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n // Connect to MongoDB\n connectDB();\n "
            },
            {
                "filename": "src/scripts/testStockClassAdjustment.js",
                "additions": 4,
                "deletions": 3,
                "patch": "@@ -1,7 +1,8 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { stockClass, stockClassAuthorizedSharesAdjust, issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { stockClassAuthorizedSharesAdjust } from \"./sampleData.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/server.js",
                "additions": 7,
                "deletions": 93,
                "patch": "@@ -1,97 +1,11 @@\n-import { config } from \"dotenv\";\n-import express, { json, urlencoded } from \"express\";\n-config();\n+import { startServer } from \"./app.js\";\n \n-import connectDB from \"./db/config/mongoose.js\";\n+// Function to check if the flag is present\n+const isFlagPresent = (flag) => process.argv.includes(flag);\n \n-import getContractInstance from \"./chain-operations/getContractInstances.js\";\n-import startOnchainListeners from \"./chain-operations/transactionListener.js\";\n+// Setting the default value of the flag to true\n+const finalizedOnly = isFlagPresent(\"--finalized-only\");\n \n-// Routes\n-import historicalTransactions from \"./routes/historicalTransactions.js\";\n-import mainRoutes from \"./routes/index.js\";\n-import issuerRoutes from \"./routes/issuer.js\";\n-import stakeholderRoutes from \"./routes/stakeholder.js\";\n-import stockClassRoutes from \"./routes/stockClass.js\";\n-import stockLegendRoutes from \"./routes/stockLegend.js\";\n-import stockPlanRoutes from \"./routes/stockPlan.js\";\n-import transactionRoutes from \"./routes/transactions.js\";\n-import valuationRoutes from \"./routes/valuation.js\";\n-import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n+console.log(\"Finalized Only:\", finalizedOnly);\n \n-import { readIssuerById, readAllIssuers } from \"./db/operations/read.js\";\n-import { contractCache } from \"./utils/caches.js\";\n-\n-const app = express();\n-\n-// Connect to MongoDB\n-connectDB();\n-\n-const PORT = process.env.PORT;\n-const CHAIN = process.env.CHAIN;\n-\n-// Middlewares\n-const chainMiddleware = (req, res, next) => {\n-    req.chain = CHAIN;\n-    next();\n-};\n-\n-// Middleware to get or create contract instance\n-// the listener is first started on deployment, then here as a backup\n-const contractMiddleware = async (req, res, next) => {\n-    if (!req.body.issuerId) {\n-        console.log(\"\u274c | No issuer ID\");\n-        res.status(400).send(\"issuerId is required\");\n-    }\n-\n-    // fetch issuer to ensure it exists\n-    const issuer = await readIssuerById(req.body.issuerId);\n-    if (!issuer) res.status(400).send(\"issuer not found \");\n-\n-    // Check if contract instance already exists in cache\n-    if (!contractCache[req.body.issuerId]) {\n-        const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to);\n-        contractCache[req.body.issuerId] = { contract, provider, libraries };\n-\n-        // Initialize listener for this contract\n-        startOnchainListeners(contract, provider, req.body.issuerId, libraries);\n-    }\n-\n-    req.contract = contractCache[req.body.issuerId].contract;\n-    req.provider = contractCache[req.body.issuerId].provider;\n-    next();\n-};\n-app.use(urlencoded({ limit: \"50mb\", extended: true }));\n-app.use(json({ limit: \"50mb\" }));\n-app.enable(\"trust proxy\");\n-\n-app.use(\"/\", chainMiddleware, mainRoutes);\n-app.use(\"/issuer\", chainMiddleware, issuerRoutes);\n-app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n-app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n-// No middleware required since these are only created offchain\n-app.use(\"/stock-legend\", stockLegendRoutes);\n-app.use(\"/stock-plan\", stockPlanRoutes);\n-app.use(\"/valuation\", valuationRoutes);\n-app.use(\"/vesting-terms\", vestingTermsRoutes);\n-app.use(\"/historical-transactions\", historicalTransactions);\n-\n-// transactions\n-app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n-\n-app.listen(PORT, async () => {\n-    console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n-     // Fetch all issuers\n-     const issuers = await readAllIssuers();\n-     if (issuers && issuers.length > 0) {\n-         for (const issuer of issuers) {\n-             if (issuer.deployed_to) {\n-                 // Create a new contract instance for each issuer\n-                 const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to);\n- \n-                 // Initialize listener for this contract\n-                 startOnchainListeners(contract, provider, issuer._id, libraries);\n-             }\n-         }\n-     }\n-});\n+startServer(finalizedOnly);"
            },
            {
                "filename": "src/state-machines/process.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,6 +1,6 @@\n import { interpret } from \"xstate\";\n+import { preProcessorCache } from \"../utils/caches.ts\";\n import { parentMachine } from \"./parent.js\";\n-import { preProcessorCache } from \"../utils/caches.js\";\n \n /*\n     @dev: Parent-Child machines are created to calculate current context then deleted."
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 122,
                "deletions": 0,
                "patch": "@@ -0,0 +1,122 @@\n+import axios from \"axios\";\n+import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n+import Factory from \"../../db/objects/Factory\";\n+import { web3WaitTime } from \"../../db/operations/update\";\n+import { issuer as exampleIssuer, stakeholder1, stakeholder2, stakeholder3, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n+import sleep from \"../../utils/sleep\";\n+import { SERVER_BASE, runLocalServer, shutdownLocalServer } from \"./utils\";\n+\n+\n+// Pro-tip: set this to iterate faster in dev after `seedExampleData` finishes\n+const HARDCODED_ISSUER_ID = null;\n+\n+beforeAll(async () => {\n+    await runLocalServer(!HARDCODED_ISSUER_ID);\n+}, 10000);\n+\n+afterAll(shutdownLocalServer, 10000);\n+\n+const WAIT_TIME = 1000;\n+\n+const allowPropagate = async () => {\n+    // Ensure ethers has enough time to catch up\n+    await sleep(WAIT_TIME);\n+}\n+\n+const seedExampleData = async () => {\n+    const rec = await Factory.findOne();\n+    if (!rec) {\n+        throw new Error(\n+            `Manually create the {\"implementation_adress\": ..., \"factory_address\": ...} record \n+            in \"factories\" collection. Run the \"forge script ...\" command from the comment \n+            in \"chain/script/CapTableFactory.s.sol\"`\n+        );\n+    }\n+\n+    const issuerResponse = await axios.post(`${SERVER_BASE}/issuer/create`, exampleIssuer);\n+    const issuerId = issuerResponse.data.issuer._id;\n+    console.log(\"\u2705 | Issuer response \", issuerId, issuerResponse.data);\n+    await allowPropagate();\n+       \n+    const stakeholder1Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder1(issuerId));\n+    const s1Id = stakeholder1Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder1Response\", s1Id, stakeholder1Response.data);\n+    await allowPropagate();\n+    \n+    const stakeholder2Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder2(issuerId));\n+    const s2Id = stakeholder2Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder2Response\", s2Id, stakeholder2Response.data);\n+    await allowPropagate();\n+    \n+    const stakeholder3Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder3(issuerId));\n+    const s3Id = stakeholder3Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder3Response\", s3Id, stakeholder3Response.data);\n+    await allowPropagate();\n+    \n+    const stockClassResponse = await axios.post(`${SERVER_BASE}/stock-class/create`, stockClass(issuerId));\n+    const stockClassId = stockClassResponse.data.stockClass._id;\n+    console.log(\"\u2705 | stockClassResponse\", stockClassId, stockClassResponse.data);\n+    await allowPropagate();\n+    \n+    const stockIssuanceResponse = await axios.post(\n+        `${SERVER_BASE}/transactions/issuance/stock`,\n+        stockIssuance(issuerId, s1Id, stockClassId, \"500\", \"1.2\")\n+    );\n+    const issuance = stockIssuanceResponse.data.stockIssuance;\n+    console.log(\"\u2705 | stockIssuanceResponse\", issuance);\n+    await allowPropagate();\n+    \n+    // TODO: Victor acceptance of issuance?\n+    // const { security_id } = issuance;\n+    // const stockIssuanceAcceptanceResp = await axios.post(\n+    //     `${SERVER_BASE}/transactions/accept/stock`,\n+    //     stockAccept(issuerId, s1Id, stockClassId, security_id, [\"Accepted\"])\n+    // );\n+    // console.log(\"\u2705 | Stock issuance acceptance response\", stockIssuanceAcceptanceResp.data);\n+    // await allowPropagate();\n+    \n+    const stockTransfer1Response = await axios.post(\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n+        stockTransfer(issuerId, \"200\", s1Id, s2Id, stockClassId, \"4.20\")\n+    );\n+    console.log(\"\u2705 | stockTransfer1Response\", stockTransfer1Response.data);\n+    await allowPropagate();\n+    \n+    // TODO: Victor acceptance of transfer1?\n+    // const stockTransferAcceptanceResp = await axios.post(\n+    //     `${SERVER_BASE}/transactions/accept/stock`,\n+    //     stockAccept(issuerId, s2Id, stockClassId, security_id, [\"Accepted\"])\n+    // );\n+    // console.log(\"\u2705 | Stock transfer acceptance response\", stockTransferAcceptanceResp.data);\n+    // await allowPropagate();\n+    \n+    const stockTransfer2Response = await axios.post(\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n+        stockTransfer(issuerId, \"300\", s1Id, s3Id, stockClassId, \"10.66\")\n+    );\n+    console.log(\"\u2705 | stockTransfer2Response\", stockTransfer2Response.data);\n+    await allowPropagate();\n+\n+    // TODO: acceptance of transfer2?\n+\n+    // Allow time for poller process to catch up\n+    await sleep(pollingSleepTime + web3WaitTime + 2000);\n+\n+    return issuerId;\n+}\n+\n+const checkRecs = async (issuerId) => {\n+    const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n+    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice, name: stakeholder.name.legal_name}; });\n+    portions.sort((a, b) => b.quantity - a.quantity);\n+    expect(portions).toStrictEqual([\n+        {quantity: 300, sharePrice: 10.66, name: \"Kent Kolze\"},\n+        {quantity: 200, sharePrice: 4.2, name: \"Victor Mimo\"},\n+    ]);\n+}\n+\n+test('end to end with event processing', async () => {\n+    const issuerId = HARDCODED_ISSUER_ID || await seedExampleData();\n+    await checkRecs(issuerId);\n+    \n+}, WAIT_TIME * 100);"
            },
            {
                "filename": "src/tests/integration/utils.ts",
                "additions": 58,
                "deletions": 0,
                "patch": "@@ -0,0 +1,58 @@\n+import { shutdownServer, startServer } from \"../../app\";\n+import { connectDB } from \"../../db/config/mongoose\";\n+import HistoricalTransaction from \"../../db/objects/HistoricalTransaction\";\n+import Issuer from \"../../db/objects/Issuer\";\n+import Stakeholder from \"../../db/objects/Stakeholder\";\n+import StockClass from \"../../db/objects/StockClass\";\n+import StockLegendTemplate from \"../../db/objects/StockLegendTemplate\";\n+import StockPlan from \"../../db/objects/StockPlan\";\n+import Valuation from \"../../db/objects/Valuation\";\n+import VestingTerms from \"../../db/objects/VestingTerms\";\n+import { typeToModelType } from \"../../db/operations/transactions\"; // Import the typeToModelType object to delete all transactions\n+\n+export const SERVER_BASE = `http://localhost:${process.env.PORT}`;\n+\n+let _server = null;\n+\n+export const runLocalServer = async (deseed) => {\n+    if (deseed) {\n+        await deseedDatabase();\n+    }\n+    console.log(\"starting server\");\n+    _server = await startServer(false);\n+}\n+\n+\n+export const shutdownLocalServer = async () => {\n+    console.log(\"shutting down server\");\n+    await shutdownServer(_server);\n+}\n+\n+\n+const deleteAllTransactions = async () => {\n+    for (const ModelType of Object.values(typeToModelType)) {\n+        // @ts-ignore\n+        await ModelType.deleteMany({});\n+    }\n+};\n+\n+const deleteAll = async () => {\n+    // Delete all documents from the collections\n+    await Issuer.deleteMany({});\n+    await Stakeholder.deleteMany({});\n+    await StockClass.deleteMany({});\n+    await StockLegendTemplate.deleteMany({});\n+    await StockPlan.deleteMany({});\n+    await Valuation.deleteMany({});\n+    await VestingTerms.deleteMany({});\n+    await HistoricalTransaction.deleteMany({});\n+    await deleteAllTransactions(); // Delete all transactions\n+};\n+\n+export const deseedDatabase = async () => {\n+    const connection = await connectDB();\n+    console.log(\"Deseeding from database: \", connection.name);\n+    await deleteAll();\n+    console.log(\"\u2705 Database deseeded successfully\");\n+    await connection.close();\n+};"
            },
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 31,
                "deletions": 0,
                "patch": "@@ -0,0 +1,31 @@\n+import { trimEvents, txFuncs, txTypes } from \"../../chain-operations/transactionPoller\";\n+\n+// TODO: if starts failing again run: yarn add --dev jest-esm-transformer\n+// https://jestjs.io/docs/using-matchers for more docs on `expect`\n+\n+const myEvents = [5, 6, 6, 6, 7, 7, 7].map((x, i) => { return {i, o: {blockNumber: x}}; });\n+\n+test('trimEvents partial', () => {\n+    // @ts-ignore\n+    const [events, block] = trimEvents(myEvents, 2, 10);\n+    expect(events.length).toBe(4);\n+    expect(events).toStrictEqual(myEvents.slice(0, 4));\n+    expect(block).toBe(6);\n+});\n+\n+test('trimEvents full', () => {\n+    // We allow more than maxEvents in order to include all events of the last block\n+    for (const maxEvents of [5, 6, 7, 15]) {\n+        // @ts-ignore\n+        const [events, block] = trimEvents(myEvents, maxEvents, 10);\n+        expect(events.length).toBe(myEvents.length);\n+        expect(events).toStrictEqual(myEvents);\n+        expect(block).toBe(10);\n+    }\n+});\n+\n+test('txMapper to maps', () => {\n+    // @ts-ignore\n+    expect(txTypes[3n]).toBe(\"StockAcceptance\");\n+    expect(txFuncs[\"StockAcceptance\"].name).toBe(\"handleStockAcceptance\");\n+});"
            },
            {
                "filename": "src/utils/caches.js",
                "additions": 0,
                "deletions": 10,
                "patch": "@@ -1,10 +0,0 @@\n-// Centralized contract manager/cache\n-export const contractCache = {};\n-\n-/*\n-issuerId = {\n-        activePositions: {...},\n-        activeSecurityIdsByStockClass: {...},\n-    };\n-*/\n-export const preProcessorCache = {};"
            },
            {
                "filename": "src/utils/caches.ts",
                "additions": 26,
                "deletions": 0,
                "patch": "@@ -0,0 +1,26 @@\n+import { getContractInstance } from \"../chain-operations/getContractInstances.js\";\n+\n+interface CachePayload {\n+    contract: any;\n+    provider: any;\n+    libraries: any;\n+}\n+\n+// Centralized contract manager/cache\n+const contractCache: {[key: string]: CachePayload} = {};\n+\n+export const getIssuerContract = async (issuer): Promise<CachePayload> => {\n+    if (!contractCache[issuer._id]) {\n+        const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to); \n+        contractCache[issuer._id] = { contract, provider, libraries };\n+    }\n+    return contractCache[issuer._id];\n+}\n+\n+/*\n+issuerId = {\n+        activePositions: {...},\n+        activeSecurityIdsByStockClass: {...},\n+    };\n+*/\n+export const preProcessorCache = {};"
            },
            {
                "filename": "src/utils/convertToFixedPointDecimals.js",
                "additions": 4,
                "deletions": 2,
                "patch": "@@ -1,16 +1,18 @@\n import { toBigInt } from \"ethers\";\n \n+export const decimalScaleValue = 1e10;\n+\n // Convert a price to a BigInt\n function toScaledBigNumber(price) {\n-    return toBigInt(Math.round(price * 1e10).toString());\n+    return toBigInt(Math.round(price * decimalScaleValue).toString());\n }\n \n // TODO: might not be refactored correctly from ethers v5 to v6\n // Convert a BigInt back to a decimal price\n function toDecimal(scaledPriceBigInt) {\n     if (typeof scaledPriceBigInt === \"bigint\") {\n         const numberString = scaledPriceBigInt.toString();\n-        return parseFloat(numberString / 1e10).toString();\n+        return parseFloat(numberString / decimalScaleValue).toString();\n     } else {\n         return scaledPriceBigInt;\n     }"
            },
            {
                "filename": "src/utils/sleep.js",
                "additions": 4,
                "deletions": 1,
                "patch": "@@ -1,4 +1,7 @@\n-function sleep(ms) {\n+function sleep(ms, logPrefix = null) {\n+    if (logPrefix) {\n+        console.log(`${logPrefix} ${(ms / 1000).toFixed(1)} seconds`);\n+    }\n     return new Promise((resolve) => setTimeout(resolve, ms));\n }\n "
            },
            {
                "filename": "tsconfig.json",
                "additions": 109,
                "deletions": 0,
                "patch": "@@ -0,0 +1,109 @@\n+{\n+  \"compilerOptions\": {\n+    /* Visit https://aka.ms/tsconfig to read more about this file */\n+\n+    /* Projects */\n+    // \"incremental\": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */\n+    // \"composite\": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */\n+    // \"tsBuildInfoFile\": \"./.tsbuildinfo\",              /* Specify the path to .tsbuildinfo incremental compilation file. */\n+    // \"disableSourceOfProjectReferenceRedirect\": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */\n+    // \"disableSolutionSearching\": true,                 /* Opt a project out of multi-project reference checking when editing. */\n+    // \"disableReferencedProjectLoad\": true,             /* Reduce the number of projects loaded automatically by TypeScript. */\n+\n+    /* Language and Environment */\n+    \"target\": \"ESNext\",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */\n+    // \"lib\": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */\n+    // \"jsx\": \"preserve\",                                /* Specify what JSX code is generated. */\n+    // \"experimentalDecorators\": true,                   /* Enable experimental support for legacy experimental decorators. */\n+    // \"emitDecoratorMetadata\": true,                    /* Emit design-type metadata for decorated declarations in source files. */\n+    // \"jsxFactory\": \"\",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */\n+    // \"jsxFragmentFactory\": \"\",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */\n+    // \"jsxImportSource\": \"\",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */\n+    // \"reactNamespace\": \"\",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */\n+    // \"noLib\": true,                                    /* Disable including any library files, including the default lib.d.ts. */\n+    // \"useDefineForClassFields\": true,                  /* Emit ECMAScript-standard-compliant class fields. */\n+    // \"moduleDetection\": \"auto\",                        /* Control what method is used to detect module-format JS files. */\n+\n+    /* Modules */\n+    \"module\": \"ESNext\",                                /* Specify what module code is generated. */\n+    // \"rootDir\": \"./\",                                  /* Specify the root folder within your source files. */\n+    \"moduleResolution\": \"node\",                     /* Specify how TypeScript looks up a file from a given module specifier. */\n+    // \"baseUrl\": \"./\",                                  /* Specify the base directory to resolve non-relative module names. */\n+    // \"paths\": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */\n+    // \"rootDirs\": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */\n+    // \"typeRoots\": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */\n+    // \"types\": [],                                      /* Specify type package names to be included without being referenced in a source file. */\n+    // \"allowUmdGlobalAccess\": true,                     /* Allow accessing UMD globals from modules. */\n+    // \"moduleSuffixes\": [],                             /* List of file name suffixes to search when resolving a module. */\n+    \"allowImportingTsExtensions\": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */\n+    // \"resolvePackageJsonExports\": true,                /* Use the package.json 'exports' field when resolving package imports. */\n+    // \"resolvePackageJsonImports\": true,                /* Use the package.json 'imports' field when resolving imports. */\n+    // \"customConditions\": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */\n+    // \"resolveJsonModule\": true,                        /* Enable importing .json files. */\n+    // \"allowArbitraryExtensions\": true,                 /* Enable importing files with any extension, provided a declaration file is present. */\n+    // \"noResolve\": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */\n+\n+    /* JavaScript Support */\n+    \"allowJs\": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */\n+    // \"checkJs\": true,                                  /* Enable error reporting in type-checked JavaScript files. */\n+    // \"maxNodeModuleJsDepth\": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */\n+\n+    /* Emit */\n+    // \"declaration\": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */\n+    // \"declarationMap\": true,                           /* Create sourcemaps for d.ts files. */\n+    // \"emitDeclarationOnly\": true,                      /* Only output d.ts files and not JavaScript files. */\n+    \"sourceMap\": true,                                /* Create source map files for emitted JavaScript files. */\n+    // \"inlineSourceMap\": true,                          /* Include sourcemap files inside the emitted JavaScript. */\n+    // \"outFile\": \"./\",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */\n+    // \"outDir\": \"./\",                                   /* Specify an output folder for all emitted files. */\n+    // \"removeComments\": true,                           /* Disable emitting comments. */\n+    // \"noEmit\": true,                                   /* Disable emitting files from a compilation. */\n+    // \"importHelpers\": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */\n+    // \"importsNotUsedAsValues\": \"remove\",               /* Specify emit/checking behavior for imports that are only used for types. */\n+    // \"downlevelIteration\": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */\n+    // \"sourceRoot\": \"\",                                 /* Specify the root path for debuggers to find the reference source code. */\n+    // \"mapRoot\": \"\",                                    /* Specify the location where debugger should locate map files instead of generated locations. */\n+    // \"inlineSources\": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */\n+    // \"emitBOM\": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */\n+    // \"newLine\": \"crlf\",                                /* Set the newline character for emitting files. */\n+    // \"stripInternal\": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */\n+    // \"noEmitHelpers\": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */\n+    // \"noEmitOnError\": true,                            /* Disable emitting files if any type checking errors are reported. */\n+    // \"preserveConstEnums\": true,                       /* Disable erasing 'const enum' declarations in generated code. */\n+    // \"declarationDir\": \"./\",                           /* Specify the output directory for generated declaration files. */\n+    // \"preserveValueImports\": true,                     /* Preserve unused imported values in the JavaScript output that would otherwise be removed. */\n+\n+    /* Interop Constraints */\n+    // \"isolatedModules\": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */\n+    // \"verbatimModuleSyntax\": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */\n+    // \"allowSyntheticDefaultImports\": true,             /* Allow 'import x from y' when a module doesn't have a default export. */\n+    \"esModuleInterop\": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */\n+    // \"preserveSymlinks\": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */\n+    \"forceConsistentCasingInFileNames\": true,            /* Ensure that casing is correct in imports. */\n+\n+    /* Type Checking */\n+    \"strict\": false,                                      /* Enable all strict type-checking options. */\n+    // \"noImplicitAny\": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */\n+    // \"strictNullChecks\": true,                         /* When type checking, take into account 'null' and 'undefined'. */\n+    // \"strictFunctionTypes\": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */\n+    // \"strictBindCallApply\": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */\n+    // \"strictPropertyInitialization\": true,             /* Check for class properties that are declared but not set in the constructor. */\n+    // \"noImplicitThis\": true,                           /* Enable error reporting when 'this' is given the type 'any'. */\n+    // \"useUnknownInCatchVariables\": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */\n+    // \"alwaysStrict\": true,                             /* Ensure 'use strict' is always emitted. */\n+    // \"noUnusedLocals\": true,                           /* Enable error reporting when local variables aren't read. */\n+    // \"noUnusedParameters\": true,                       /* Raise an error when a function parameter isn't read. */\n+    // \"exactOptionalPropertyTypes\": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */\n+    // \"noImplicitReturns\": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */\n+    // \"noFallthroughCasesInSwitch\": true,               /* Enable error reporting for fallthrough cases in switch statements. */\n+    // \"noUncheckedIndexedAccess\": true,                 /* Add 'undefined' to a type when accessed using an index. */\n+    // \"noImplicitOverride\": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */\n+    // \"noPropertyAccessFromIndexSignature\": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */\n+    // \"allowUnusedLabels\": true,                        /* Disable error reporting for unused labels. */\n+    // \"allowUnreachableCode\": true,                     /* Disable error reporting for unreachable code. */\n+\n+    /* Completeness */\n+    // \"skipDefaultLibCheck\": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */\n+    \"skipLibCheck\": true                                 /* Skip type checking all .d.ts files. */\n+  }\n+}"
            },
            {
                "filename": "yarn.lock",
                "additions": 5230,
                "deletions": 892,
                "patch": null
            }
        ]
    },
    {
        "sha": "eaecd49d53573605af2f411a7c384d25ae568032",
        "author": "victormimo",
        "date": "2024-01-19 20:33:00+00:00",
        "message": "separating docker files",
        "files": [
            {
                "filename": "Dockerfile.dev",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -12,4 +12,4 @@ RUN yarn install\n \n EXPOSE 8080\n # Specify the command to run on container start\n-CMD [\"node\", \"src/server.js\"]\n+CMD [\"npx tsx\", \"src/server.js\"]"
            },
            {
                "filename": "Dockerfile.prod",
                "additions": 15,
                "deletions": 0,
                "patch": "@@ -0,0 +1,15 @@\n+# Use the official node image as a parent image\n+FROM node:18\n+\n+# Set the working directory\n+WORKDIR /app\n+\n+# COPY ./chain/out ./chain/out\n+COPY . .\n+\n+# Install dependencies and setup\n+RUN yarn install\n+\n+EXPOSE 8080\n+# Specify the command to run on container start\n+CMD [\"npx tsx\", \"src/server.js --finalized-only\"]"
            }
        ]
    },
    {
        "sha": "43c1b4ef4651f230c8abfe9ebac628a68b82a209",
        "author": "kentplural",
        "date": "2024-01-19 19:41:14+00:00",
        "message": "Merge pull request #116 from transfer-agent-protocol/kkolze/resilient-event-processor\n\nKkolze/resilient event processor",
        "files": [
            {
                "filename": ".env.example",
                "additions": 3,
                "deletions": 1,
                "patch": "@@ -1,5 +1,6 @@\n # Offchain db connection string for mongodb\n DATABASE_URL=\"mongodb://tap:tap@localhost:27017/mongo?authSource=admin&retryWrites=true&w=majority\"\n+DATABASE_REPLSET=\"0\"  # set to \"1\" if using --replSet option in mongo. this allows transactions\n \n # RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n RPC_URL=http://127.0.0.1:8545\n@@ -16,4 +17,5 @@ ETHERSCAN_L2_API_KEY=UPDATE_ME\n ETHERSCAN_L1_API_KEY=UPDATE_ME\n \n # Server port\n-PORT=8080\n\\ No newline at end of file\n+PORT=8080\n+"
            },
            {
                "filename": "README.md",
                "additions": 43,
                "deletions": 20,
                "patch": "@@ -2,9 +2,11 @@\n \n Developed by:\n \n-- [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n-- [Plural Energy](https://www.pluralenergy.co/)\n-- [Fairmint](https://www.fairmint.com/)\n+# [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n+\n+-   [Poet](https://poet.network/)\n+-   [Plural Energy](https://www.pluralenergy.co/)\n+-   [Fairmint](https://www.fairmint.com/)\n \n This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety. In development, it's meant to be run in a Docker container with a local MongoDB instance. While in active development, it's meant to be run with [Anvil](https://book.getfoundry.sh/anvil/) and [Forge](https://book.getfoundry.sh/forge/).\n \n@@ -16,28 +18,28 @@ This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap\n \n ## Dependencies\n \n-- [Docker](https://docs.docker.com/get-docker/)\n+-   [Docker](https://docs.docker.com/get-docker/)\n \n-- [Foundry](https://getfoundry.sh/)\n+-   [Foundry](https://getfoundry.sh/)\n \n ```sh\n curl -L https://foundry.paradigm.xyz | bash\n ```\n \n-- [Mongo Compass](https://www.mongodb.com/try/download/compass)\n+-   [Mongo Compass](https://www.mongodb.com/try/download/compass)\n \n-- [Postman App](https://www.postman.com/downloads/)\n+-   [Postman App](https://www.postman.com/downloads/)\n \n-- [Node.js v18.16.0](https://nodejs.org/en/download/)\n+-   [Node.js v18.16.0](https://nodejs.org/en/download/)\n \n-- [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n+-   [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n \n We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo) for the local development database. You can find the [Docker Compose file](./docker-compose.yml) in the root of this repository.\n \n ## Official links\n \n-- [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n-- [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n+-   [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n+-   [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n \n ## Getting started\n \n@@ -100,9 +102,6 @@ yarn build\n \n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n-To deploy the cap table smart contracts to a testnet, update `RPC_URL` and `CHAIN_ID` in the `.env` file, then run the same `yarn build` command.\n-\n-\n ## Running the cap table server\n \n After the deployment script is completed, start the server with nodemon:\n@@ -156,16 +155,40 @@ We're shipping code fast. If you run into an issue, particularly one that result\n \n Inside of `/chain`:\n \n-- Restart anvil\n-- Run `forge clean`\n-- Move back to the root directory, then run `yarn build`\n+-   Restart anvil\n+-   Run `forge clean`\n+-   Move back to the root directory, then run `yarn build`\n \n After, you can seed and deploy the cap table with either of the above options. If the bug persists, please open an issue with an attached screenshot and steps to reproduce.\n \n-## Testing\n+## Testing Web3\n+\n+Run all smart contracts tests\n+\n+`yarn test`\n+\n+## Testing Web2\n+\n+### Unit tests\n+\n+Run all javascript unit tests with jest\n+\n+`yarn test-js`\n+\n+### Integration tests\n+\n+Deploy a cap table to local anvil server through a local web2 server. The chain event listener is also run to ensure the events are properly mirrored into the mongo database. NOTE: running this deletes your local mongo collections first\n+\n+`yarn test-js-integration`\n+\n+Integration test setup from no active processes:\n \n-To run tests for the smart contracts, run `yarn test`.\n-This will run all the tests defined in the test suite and output the results to the console.\n+-   Terminal 1: `docker compose up`\n+-   Terminal 2: `anvil`\n+-   Terminal 3: `cd chain && forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast`\n+    -   In MongoDB compass, create/update `implementation_address` and `factory_address` in `jest-integration.factories`\n+        -   If the `jest-integration` MongoDB databases dont exist: `cd .. && yarn test-js-integration`\n+    -   Run `yarn test-js-integration`!\n \n ## Contributing\n "
            },
            {
                "filename": "chain/src/CapTable.sol",
                "additions": 6,
                "deletions": 0,
                "patch": "@@ -423,6 +423,12 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n         return stockClasses.length;\n     }\n \n+    /// @inheritdoc ICapTable\n+    function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40) {\n+        ActivePosition storage position = positions.activePositions[stakeholderId][securityId];\n+        return (position.stock_class_id, position.quantity, position.share_price, position.timestamp);\n+    }\n+\n     /* Role Based Access Control */\n     modifier onlyOperator() {\n         /// @notice Admins are also considered Operators"
            },
            {
                "filename": "chain/src/interfaces/ICapTable.sol",
                "additions": 3,
                "deletions": 0,
                "patch": "@@ -74,6 +74,9 @@ interface ICapTable {\n \n     function getTotalActiveSecuritiesCount() external view returns (uint256);\n \n+    // Function to get the timestamp of an active position\n+    function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40);\n+\n     function issueStock(StockIssuanceParams calldata params) external;\n \n     function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external;"
            },
            {
                "filename": "jest.config.js",
                "additions": 203,
                "deletions": 0,
                "patch": "@@ -0,0 +1,203 @@\n+/**\n+ * For a detailed explanation regarding each configuration property, visit:\n+ * https://jestjs.io/docs/configuration\n+ */\n+\n+// This allows us to avoid messing with the state of people's standard dev database\n+process.env['DATABASE_OVERRIDE'] = 'jest-integration';\n+\n+/** @type {import('jest').Config} */\n+const config = {\n+  // All imported modules in your tests should be mocked automatically\n+  // automock: false,\n+\n+  // Stop running tests after `n` failures\n+  // bail: 0,\n+\n+  // The directory where Jest should store its cached dependency information\n+  // cacheDirectory: \"/tmp/jest_rs\",\n+\n+  // Automatically clear mock calls, instances, contexts and results before every test\n+  // clearMocks: false,\n+\n+  // Indicates whether the coverage information should be collected while executing the test\n+  // collectCoverage: false,\n+\n+  // An array of glob patterns indicating a set of files for which coverage information should be collected\n+  // collectCoverageFrom: undefined,\n+\n+  // The directory where Jest should output its coverage files\n+  // coverageDirectory: \"coverage\",\n+\n+  // An array of regexp pattern strings used to skip coverage collection\n+  // coveragePathIgnorePatterns: [\n+  //   \"/node_modules/\"\n+  // ],\n+\n+  // Indicates which provider should be used to instrument code for coverage\n+  // coverageProvider: \"babel\",\n+\n+  // A list of reporter names that Jest uses when writing coverage reports\n+  // coverageReporters: [\n+  //   \"json\",\n+  //   \"text\",\n+  //   \"lcov\",\n+  //   \"clover\"\n+  // ],\n+\n+  // An object that configures minimum threshold enforcement for coverage results\n+  // coverageThreshold: undefined,\n+\n+  // A path to a custom dependency extractor\n+  // dependencyExtractor: undefined,\n+\n+  // Make calling deprecated APIs throw helpful error messages\n+  // errorOnDeprecated: false,\n+\n+  // The default configuration for fake timers\n+  // fakeTimers: {\n+  //   \"enableGlobally\": false\n+  // },\n+\n+  // Force coverage collection from ignored files using an array of glob patterns\n+  // forceCoverageMatch: [],\n+\n+  // A path to a module which exports an async function that is triggered once before all test suites\n+  // globalSetup: undefined,\n+\n+  // A path to a module which exports an async function that is triggered once after all test suites\n+  // globalTeardown: undefined,\n+\n+  // A set of global variables that need to be available in all test environments\n+  // globals: {},\n+\n+  // The maximum amount of workers used to run your tests. Can be specified as % or a number. E.g. maxWorkers: 10% will use 10% of your CPU amount + 1 as the maximum worker number. maxWorkers: 2 will use a maximum of 2 workers.\n+  // maxWorkers: \"50%\",\n+\n+  // An array of directory names to be searched recursively up from the requiring module's location\n+  // moduleDirectories: [\n+  //   \"node_modules\"\n+  // ],\n+\n+  // An array of file extensions your modules use\n+  // moduleFileExtensions: [\n+  //   \"js\",\n+  //   \"mjs\",\n+  //   \"cjs\",\n+  //   \"jsx\",\n+  //   \"ts\",\n+  //   \"tsx\",\n+  //   \"json\",\n+  //   \"node\"\n+  // ],\n+\n+  // A map from regular expressions to module names or to arrays of module names that allow to stub out resources with a single module\n+  // moduleNameMapper: {},\n+\n+  // An array of regexp pattern strings, matched against all module paths before considered 'visible' to the module loader\n+  // modulePathIgnorePatterns: [],\n+\n+  // Activates notifications for test results\n+  // notify: false,\n+\n+  // An enum that specifies notification mode. Requires { notify: true }\n+  // notifyMode: \"failure-change\",\n+\n+  // A preset that is used as a base for Jest's configuration\n+  preset: \"ts-jest\",\n+\n+  // Run tests from one or more projects\n+  // projects: undefined,\n+\n+  // Use this configuration option to add custom reporters to Jest\n+  // reporters: undefined,\n+\n+  // Automatically reset mock state before every test\n+  // resetMocks: false,\n+\n+  // Reset the module registry before running each individual test\n+  // resetModules: false,\n+\n+  // A path to a custom resolver\n+  // resolver: undefined,\n+\n+  // Automatically restore mock state and implementation before every test\n+  // restoreMocks: false,\n+\n+  // The root directory that Jest should scan for tests and modules within\n+  // rootDir: undefined,\n+\n+  // A list of paths to directories that Jest should use to search for files in\n+  // roots: [\n+  //   \"<rootDir>\"\n+  // ],\n+\n+  // Allows you to use a custom runner instead of Jest's default test runner\n+  // runner: \"jest-runner\",\n+\n+  // The paths to modules that run some code to configure or set up the testing environment before each test\n+  // setupFiles: [],\n+\n+  // A list of paths to modules that run some code to configure or set up the testing framework before each test\n+  // setupFilesAfterEnv: [],\n+\n+  // The number of seconds after which a test is considered as slow and reported as such in the results.\n+  // slowTestThreshold: 5,\n+\n+  // A list of paths to snapshot serializer modules Jest should use for snapshot testing\n+  // snapshotSerializers: [],\n+\n+  // The test environment that will be used for testing\n+  // testEnvironment: \"jest-environment-node\",\n+\n+  // Options that will be passed to the testEnvironment\n+  // testEnvironmentOptions: {},\n+\n+  // Adds a location field to test results\n+  // testLocationInResults: false,\n+\n+  // The glob patterns Jest uses to detect test files\n+  // testMatch: [\n+  //   \"**/__tests__/**/*.[jt]s?(x)\",\n+  //   \"**/?(*.)+(spec|test).[tj]s?(x)\"\n+  // ],\n+\n+  // An array of regexp pattern strings that are matched against all test paths, matched tests are skipped\n+  // testPathIgnorePatterns: [\n+  //   \"/node_modules/\"\n+  // ],\n+\n+  // The regexp pattern or array of patterns that Jest uses to detect test files\n+  // testRegex: [],\n+\n+  // This option allows the use of a custom results processor\n+  // testResultsProcessor: undefined,\n+\n+  // This option allows use of a custom test runner\n+  // testRunner: \"jest-circus/runner\",\n+\n+  // A map from regular expressions to paths to transformers\n+  transform: {\n+    \".*\\\\.(tsx?|js)$\": \"ts-jest\",\n+  },\n+\n+  // An array of regexp pattern strings that are matched against all source file paths, matched files will skip transformation\n+  // transformIgnorePatterns: [\n+  //   \"/node_modules/\",\n+  //   \"\\\\.pnp\\\\.[^\\\\/]+$\"\n+  // ],\n+\n+  // An array of regexp pattern strings that are matched against all modules before the module loader will automatically return a mock for them\n+  // unmockedModulePathPatterns: undefined,\n+\n+  // Indicates whether each individual test should be reported during the run\n+  verbose: true,\n+\n+  // An array of regexp patterns that are matched against all source file paths before re-running tests in watch mode\n+  // watchPathIgnorePatterns: [],\n+\n+  // Whether to use watchman for file crawling\n+  // watchman: true,\n+};\n+\n+export default config;"
            },
            {
                "filename": "package.json",
                "additions": 14,
                "deletions": 6,
                "patch": "@@ -7,17 +7,21 @@\n     \"description\": \"Transfer Agent Protocol onchain cap table\",\n     \"type\": \"module\",\n     \"scripts\": {\n-        \"start\": \"nodemon src/server.js\",\n+        \"prod\": \"npx tsx src/server.js --finalized-only\",\n+        \"dev\": \"npx tsx watch src/server.js\",\n         \"eslint\": \"eslint --cache --cache-location=node_modules/.cache/.eslintcache --fix\",\n         \"lint\": \"yarn run eslint . --ext .js,.jsx,.ts,.tsx\",\n         \"format\": \"prettier '**/*' --ignore-unknown --write\",\n         \"typecheck\": \"concurrently --raw yarn:typecheck:*\",\n         \"typecheck:app\": \"tsc --noEmit\",\n         \"prepare\": \"husky install\",\n-        \"build\": \"cd chain && node ../src/scripts/deployAndLinkLibs.js\",\n+        \"build\": \"cd chain && npx tsx ../src/scripts/deployAndLinkLibs.js\",\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n-        \"deseed\": \"node src/db/scripts/deseed.js\",\n+        \"deseed\": \"npx tsx src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\",\n+        \"test-js\": \"jest --testPathPattern src/tests/unit\",\n+        \"test-js-integration\": \"jest --testPathPattern src/tests/integration\",\n+        \"validate-poet-files\": \"cd ocf && yarn validate-poet-files\",\n         \"export-manifest\": \"cd src/db/samples && zip -r notPoet.zip notPoet && mv notPoet.zip $HOME/Downloads\"\n     },\n     \"dependencies\": {\n@@ -30,7 +34,9 @@\n         \"ethers\": \"^6.7.1\",\n         \"express\": \"^4.18.2\",\n         \"mongoose\": \"^7.4.2\",\n+        \"npx\": \"^10.2.2\",\n         \"solc\": \"^0.8.20\",\n+        \"tsx\": \"^4.7.0\",\n         \"xstate\": \"^4.38.2\",\n         \"yauzl\": \"^2.10.0\"\n     },\n@@ -40,18 +46,20 @@\n         \"maintained node versions\"\n     ],\n     \"devDependencies\": {\n-        \"@types/node\": \"^20.3.2\",\n+        \"@types/jest\": \"^29.5.11\",\n+        \"@types/node\": \"^20.11.5\",\n         \"@types/uuid\": \"^9.0.2\",\n         \"eslint\": \"^8.21.0\",\n         \"eslint-config-next\": \"^13.1.6\",\n         \"eslint-config-prettier\": \"^8.5.0\",\n         \"eslint-plugin-import\": \"^2.26.0\",\n         \"husky\": \"^8.0.1\",\n+        \"jest\": \"^29.7.0\",\n         \"lint-staged\": \"^13.0.3\",\n-        \"nodemon\": \"^3.0.1\",\n         \"prettier\": \"^2.7.1\",\n         \"solhint\": \"^3.4.1\",\n-        \"typescript\": \"^5.1.6\",\n+        \"ts-jest\": \"^29.1.1\",\n+        \"typescript\": \"^5.3.3\",\n         \"uuid\": \"^9.0.0\"\n     }\n }"
            },
            {
                "filename": "src/app.js",
                "additions": 96,
                "deletions": 0,
                "patch": "@@ -0,0 +1,96 @@\n+import { config } from \"dotenv\";\n+import express, { json, urlencoded } from \"express\";\n+config();\n+\n+import { connectDB } from \"./db/config/mongoose.ts\";\n+\n+import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n+\n+// Routes\n+import { capTable as capTableRoutes } from \"./routes/capTable.ts\";\n+import historicalTransactions from \"./routes/historicalTransactions.js\";\n+import mainRoutes from \"./routes/index.js\";\n+import issuerRoutes from \"./routes/issuer.js\";\n+import stakeholderRoutes from \"./routes/stakeholder.js\";\n+import stockClassRoutes from \"./routes/stockClass.js\";\n+import stockLegendRoutes from \"./routes/stockLegend.js\";\n+import stockPlanRoutes from \"./routes/stockPlan.js\";\n+import transactionRoutes from \"./routes/transactions.js\";\n+import valuationRoutes from \"./routes/valuation.js\";\n+import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n+\n+import mongoose from \"mongoose\";\n+import { readIssuerById } from \"./db/operations/read.js\";\n+import { getIssuerContract } from \"./utils/caches.ts\";\n+\n+const app = express();\n+\n+const PORT = process.env.PORT;\n+\n+// Middleware to get or create contract instance\n+// the listener is first started on deployment, then here as a backup\n+const contractMiddleware = async (req, res, next) => {\n+    if (!req.body.issuerId) {\n+        console.log(\"\u274c | No issuer ID\");\n+        res.status(400).send(\"issuerId is required\");\n+    }\n+\n+    // fetch issuer to ensure it exists\n+    const issuer = await readIssuerById(req.body.issuerId);\n+    if (!issuer) res.status(400).send(\"issuer not found \");\n+\n+    const { contract, provider } = await getIssuerContract(issuer);\n+    req.contract = contract;\n+    req.provider = provider;\n+    next();\n+};\n+app.use(urlencoded({ limit: \"50mb\", extended: true }));\n+app.use(json({ limit: \"50mb\" }));\n+app.enable(\"trust proxy\");\n+\n+app.use(\"/\", mainRoutes);\n+app.use(\"/cap-table\", capTableRoutes);\n+app.use(\"/issuer\", issuerRoutes);\n+app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n+app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n+// No middleware required since these are only created offchain\n+app.use(\"/stock-legend\", stockLegendRoutes);\n+app.use(\"/stock-plan\", stockPlanRoutes);\n+app.use(\"/valuation\", valuationRoutes);\n+app.use(\"/vesting-terms\", vestingTermsRoutes);\n+app.use(\"/historical-transactions\", historicalTransactions);\n+\n+// transactions\n+app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n+\n+export const startServer = async (finalizedOnly) => {\n+    /*\n+    processTo can be \"latest\" or \"finalized\". Latest helps during testing bc we dont have to wait for blocks to finalize\n+    */\n+\n+    // Connect to MongoDB\n+    const dbConn = await connectDB();\n+\n+    const server = app.listen(PORT, async () => {\n+        console.log(`\ud83d\ude80  Server successfully launched at ${PORT}`);\n+        // Asynchronous job to track web3 events in web2\n+        startEventProcessing(finalizedOnly, dbConn);\n+    });\n+\n+    return server;\n+};\n+\n+export const shutdownServer = async (server) => {\n+    if (server) {\n+        console.log(\"Shutting down app server...\");\n+        server.close();\n+    }\n+\n+    console.log(\"Waiting for event processing to stop...\");\n+    await stopEventProcessing();\n+\n+    if (mongoose.connection?.readyState === mongoose.STATES.connected) {\n+        console.log(\"Disconnecting from mongo...\");\n+        await mongoose.disconnect();\n+    }\n+};"
            },
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -49,6 +49,7 @@ async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n         provider,\n         address: latestCapTableProxyContractAddress,\n         libraries,\n+        deployHash: tx.hash,\n     };\n }\n "
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 2,
                "deletions": 4,
                "patch": "@@ -1,12 +1,12 @@\n-import { ethers } from \"ethers\";\n import { config } from \"dotenv\";\n+import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n import getProvider from \"./getProvider.js\";\n \n config();\n \n-async function getContractInstance(address) {\n+export const getContractInstance = (address) => {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n \n     const provider = getProvider();\n@@ -17,5 +17,3 @@ async function getContractInstance(address) {\n \n     return { contract, provider, libraries };\n }\n-\n-export default getContractInstance;"
            },
            {
                "filename": "src/chain-operations/seed.js",
                "additions": 2,
                "deletions": 3,
                "patch": "@@ -1,11 +1,10 @@\n import { convertAndReflectStakeholderOnchain } from \"../controllers/stakeholderController.js\";\n import { convertAndReflectStockClassOnchain } from \"../controllers/stockClassController.js\";\n-import { getAllIssuerDataById } from \"../db/operations/read.js\";\n+import { getAllIssuerDataById, readIssuerById } from \"../db/operations/read.js\";\n+import { preProcessorCache } from \"../utils/caches.ts\";\n import { convertTimeStampToUint40, toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import { convertBytes16ToUUID, convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import { extractArrays } from \"../utils/flattenPreprocessorCache.js\";\n-import { preProcessorCache } from \"../utils/caches.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n import sleep from \"../utils/sleep.js\";\n \n export const verifyIssuerAndSeed = async (contract, id) => {"
            },
            {
                "filename": "src/chain-operations/transactionListener.js",
                "additions": 0,
                "deletions": 123,
                "patch": "@@ -1,123 +0,0 @@\n-import { verifyIssuerAndSeed } from \"./seed.js\";\n-import {\n-    handleStockCancellation,\n-    handleIssuerAuthorizedSharesAdjusted,\n-    handleStockAcceptance,\n-    handleStockReissuance,\n-    handleStockRepurchase,\n-    handleStockRetraction,\n-    handleStockClass,\n-    handleStakeholder,\n-    handleStockIssuance,\n-    handleStockTransfer,\n-    handleStockClassAuthorizedSharesAdjusted,\n-} from \"./transactionHandlers.js\";\n-import { AbiCoder } from \"ethers\";\n-import {\n-    IssuerAuthorizedSharesAdjustment,\n-    StockAcceptance,\n-    StockCancellation,\n-    StockClassAuthorizedSharesAdjustment,\n-    StockIssuance,\n-    StockReissuance,\n-    StockRepurchase,\n-    StockRetraction,\n-    StockTransfer,\n-} from \"./structs.js\";\n-\n-const abiCoder = new AbiCoder();\n-const eventQueue = [];\n-let issuerEventFired = false;\n-\n-const txMapper = {\n-    0: [\"INVALID\"],\n-    1: [\"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\", IssuerAuthorizedSharesAdjustment],\n-    2: [\"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\", StockClassAuthorizedSharesAdjustment],\n-    3: [\"STOCK_ACCEPTANCE\", StockAcceptance],\n-    4: [\"STOCK_CANCELLATION\", StockCancellation],\n-    5: [\"STOCK_ISSUANCE\", StockIssuance],\n-    6: [\"STOCK_REISSUANCE\", StockReissuance],\n-    7: [\"STOCK_REPURCHASE\", StockRepurchase],\n-    8: [\"STOCK_RETRACTION\", StockRetraction],\n-    9: [\"STOCK_TRANSFER\", StockTransfer],\n-};\n-\n-async function startOnchainListeners(contract, provider, issuerId, libraries) {\n-    console.log(\"\ud83c\udf10 | Initiating on-chain event listeners for issuer\", issuerId, \"at address\", contract.target);\n-\n-    libraries.txHelper.on(\"TxCreated\", async (_, txTypeIdx, txData, event) => {\n-        const [type, structType] = txMapper[txTypeIdx];\n-        const decodedData = abiCoder.decode([structType], txData);\n-        const { timestamp } = await provider.getBlock(event.blockNumber);\n-        eventQueue.push({ type, data: decodedData[0], issuerId, timestamp });\n-    });\n-\n-    contract.on(\"StakeholderCreated\", async (id, _) => {\n-        eventQueue.push({ type: \"STAKEHOLDER_CREATED\", data: id });\n-    });\n-\n-    contract.on(\"StockClassCreated\", async (id, _) => {\n-        eventQueue.push({ type: \"STOCK_CLASS_CREATED\", data: id });\n-    });\n-\n-    const issuerCreatedFilter = contract.filters.IssuerCreated;\n-    const issuerEvents = await contract.queryFilter(issuerCreatedFilter);\n-\n-    if (issuerEvents.length > 0 && !issuerEventFired) {\n-        const id = issuerEvents[0].args[0];\n-        console.log(\"IssuerCreated Event Emitted!\", id);\n-\n-        await verifyIssuerAndSeed(contract, id);\n-        issuerEventFired = true;\n-    }\n-\n-    setInterval(processEventQueue, 5000); // Process every 5 seconds\n-}\n-\n-async function processEventQueue() {\n-    const sortedEventQueue = eventQueue.sort((a, b) => a.timestamp - b.timestamp);\n-    while (sortedEventQueue.length > 0) {\n-        const event = eventQueue[0];\n-        switch (event.type) {\n-            case \"STAKEHOLDER_CREATED\":\n-                await handleStakeholder(event.data);\n-                break;\n-            case \"STOCK_CLASS_CREATED\":\n-                await handleStockClass(event.data);\n-                break;\n-            case \"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\":\n-                await handleIssuerAuthorizedSharesAdjusted(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\":\n-                await handleStockClassAuthorizedSharesAdjusted(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_ACCEPTANCE\":\n-                await handleStockAcceptance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_CANCELLATION\":\n-                await handleStockCancellation(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_ISSUANCE\":\n-                await handleStockIssuance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_REISSUANCE\":\n-                await handleStockReissuance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_REPURCHASE\":\n-                await handleStockRepurchase(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_RETRACTION\":\n-                await handleStockRetraction(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_TRANSFER\":\n-                await handleStockTransfer(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"INVALID\":\n-                throw new Error(\"Invalid transaction type\");\n-                break;\n-        }\n-        sortedEventQueue.shift();\n-    }\n-}\n-\n-export default startOnchainListeners;"
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 227,
                "deletions": 0,
                "patch": "@@ -0,0 +1,227 @@\n+import { AbiCoder, EventLog } from \"ethers\";\n+import { connectDB } from \"../db/config/mongoose.ts\";\n+import { withGlobalTransaction } from \"../db/operations/atomic.ts\";\n+import { readAllIssuers } from \"../db/operations/read.js\";\n+import { updateIssuerById } from \"../db/operations/update.js\";\n+import { getIssuerContract } from \"../utils/caches.ts\";\n+import sleep from \"../utils/sleep.js\";\n+import { verifyIssuerAndSeed } from \"./seed.js\";\n+import {\n+    IssuerAuthorizedSharesAdjustment,\n+    StockAcceptance,\n+    StockCancellation,\n+    StockClassAuthorizedSharesAdjustment,\n+    StockIssuance,\n+    StockReissuance,\n+    StockRepurchase,\n+    StockRetraction,\n+    StockTransfer,\n+} from \"./structs.js\";\n+import {\n+    handleIssuerAuthorizedSharesAdjusted,\n+    handleStakeholder,\n+    handleStockAcceptance,\n+    handleStockCancellation,\n+    handleStockClass,\n+    handleStockClassAuthorizedSharesAdjusted,\n+    handleStockIssuance,\n+    handleStockReissuance,\n+    handleStockRepurchase,\n+    handleStockRetraction,\n+    handleStockTransfer,\n+} from \"./transactionHandlers.js\";\n+\n+const abiCoder = new AbiCoder();\n+\n+interface QueuedEvent {\n+    type: string;\n+    timestamp: Date;\n+    data: any;\n+    o: EventLog;\n+}\n+\n+const contractFuncs = new Map([\n+    [\"StakeholderCreated\", handleStakeholder],\n+    [\"StockClassCreated\", handleStockClass],\n+]);\n+\n+const txMapper = {\n+    1: [IssuerAuthorizedSharesAdjustment, handleIssuerAuthorizedSharesAdjusted],\n+    2: [StockClassAuthorizedSharesAdjustment, handleStockClassAuthorizedSharesAdjusted],\n+    3: [StockAcceptance, handleStockAcceptance],\n+    4: [StockCancellation, handleStockCancellation],\n+    5: [StockIssuance, handleStockIssuance],\n+    6: [StockReissuance, handleStockReissuance],\n+    7: [StockRepurchase, handleStockRepurchase],\n+    8: [StockRetraction, handleStockRetraction],\n+    9: [StockTransfer, handleStockTransfer],\n+};\n+// (idx => type name) derived from txMapper\n+export const txTypes = Object.fromEntries(\n+    // @ts-ignore\n+    Object.entries(txMapper).map(([i, [_, f]]) => [i, f.name.replace(\"handle\", \"\")])\n+);\n+// (name => handler) derived from txMapper\n+export const txFuncs = Object.fromEntries(\n+    Object.entries(txMapper).map(([i, [_, f]]) => [txTypes[i], f])\n+);\n+\n+let _keepProcessing = true;\n+let _finishedProcessing = false;\n+\n+export const stopEventProcessing = async () => {\n+    _keepProcessing = false;\n+    while (!_finishedProcessing) {\n+        await sleep(50);\n+    }\n+}\n+\n+export const pollingSleepTime = 1000;\n+\n+export const startEventProcessing = async (finalizedOnly: boolean, dbConn) => {\n+    _keepProcessing = true;\n+    _finishedProcessing = false;\n+    while (_keepProcessing) {\n+        const issuers = await readAllIssuers();\n+\n+        // console.log(`Processing synchronously for ${issuers.length} issuers`);\n+        for (const issuer of issuers) {\n+            if (issuer.deployed_to) {\n+                const { contract, provider, libraries } = await getIssuerContract(issuer);\n+                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper, finalizedOnly);\n+            }\n+        }\n+        await sleep(pollingSleepTime);\n+    }\n+    _finishedProcessing = true;\n+};\n+\n+const processEvents = async (dbConn, contract, provider, issuer, txHelper, finalizedOnly, maxBlocks = 1500, maxEvents = 250) => {\n+    /*\n+    We process up to `maxEvents` across `maxBlocks` to ensure our transaction sizes dont get too big and bog down our db\n+    */\n+    let {_id: issuerId, last_processed_block: lastProcessedBlock, tx_hash: deployedTxHash} = issuer;\n+    const {number: latestBlock} = await provider.getBlock(finalizedOnly ? \"finalized\" : \"latest\");\n+    // console.log(\"Processing for issuer\", {issuerId, lastProcessedBlock, deployedTxHash, latestBlock});\n+    if (lastProcessedBlock === null) {\n+        const receipt = await provider.getTransactionReceipt(deployedTxHash);\n+        if (!receipt) {\n+            console.error(\"Deployment receipt not found\");\n+            return;\n+        }\n+        if (receipt.blockNumber > latestBlock) {\n+            console.log(\"Deployment tx not finalized\", {receipt, lastFinalizedBlock: latestBlock});\n+            return;\n+        }\n+        lastProcessedBlock = await issuerDeployed(issuerId, receipt, contract, dbConn);\n+    }\n+    const startBlock = lastProcessedBlock + 1;\n+    let endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n+    if (startBlock >= endBlock) {\n+        return;\n+    }\n+    \n+    // console.log(\" processing from\", { startBlock, endBlock });\n+    let events: QueuedEvent[] = [];\n+\n+    const contractEvents: EventLog[] = await contract.queryFilter(\"*\", startBlock, endBlock);\n+    for (const event of contractEvents) {\n+        const type = event?.fragment?.name;\n+        if (contractFuncs.has(type)) {\n+            const { timestamp } = await provider.getBlock(event.blockNumber);\n+            events.push({type, timestamp, data: event.args[0], o: event });\n+        }\n+    }\n+\n+    const txEvents: EventLog[] = await txHelper.queryFilter(txHelper.filters.TxCreated, startBlock, endBlock);\n+    for (const event of txEvents) {\n+        if (event.removed) {\n+            continue;\n+        }\n+        const [_len, typeIdx, txData] = event.args;\n+        const [structType, _] = txMapper[typeIdx];\n+        const decodedData = abiCoder.decode([structType], txData);\n+        const { timestamp } = await provider.getBlock(event.blockNumber);\n+        events.push({ type: txTypes[typeIdx], timestamp, data: decodedData[0], o: event });\n+    }\n+\n+    // Nothing to process\n+    if (events.length === 0) {\n+        await updateLastProcessed(issuerId, endBlock);\n+        return;\n+    }\n+\n+    // Process only up to a certain amount\n+    [events, endBlock] = trimEvents(events, maxEvents, endBlock);\n+\n+    await withGlobalTransaction(async () => {\n+        await persistEvents(issuerId, events);\n+        await updateLastProcessed(issuerId, endBlock);\n+    }, dbConn);\n+};\n+\n+const issuerDeployed = async (issuerId, receipt, contract, dbConn) => {\n+    console.log(\"New issuer was deployed\", {issuerId});\n+    const events = await contract.queryFilter(contract.filters.IssuerCreated);\n+    if (events.length === 0) {\n+        throw new Error(`No issuer events found!`);\n+    }\n+    const issuerCreatedEventId = events[0].args[0];\n+    console.log(\"IssuerCreated event captured!\", {issuerCreatedEventId});\n+    const lastProcessedBlock = receipt.blockNumber - 1;\n+    await withGlobalTransaction(async () => {\n+        await verifyIssuerAndSeed(contract, issuerCreatedEventId);\n+        await updateLastProcessed(issuerId, lastProcessedBlock);\n+    }, dbConn);\n+    return lastProcessedBlock;\n+};\n+\n+const persistEvents = async (issuerId, events: QueuedEvent[]) => {\n+    // Persist all the necessary changes for each event gathered in process events\n+    console.log(`${events.length} events to process for issuerId ${issuerId}`);\n+    for (const event of events) {\n+        const {type, data, timestamp} = event;\n+        const txHandleFunc = txFuncs[type];\n+        // console.log(\"persistEvent: \", {type, data, timestamp});\n+        if (txHandleFunc) {\n+            // @ts-ignore\n+            await txHandleFunc(data, issuerId, timestamp);\n+            continue;\n+        }\n+        const contractHandleFunc = contractFuncs.get(type);\n+        if (contractHandleFunc) {\n+            await contractHandleFunc(data);\n+            continue;\n+        }\n+        console.error(\"Invalid transaction type: \", type, event);\n+        throw new Error(`Invalid transaction type: \"${type}\"`);\n+    }\n+};\n+\n+export const trimEvents = (origEvents: QueuedEvent[], maxEvents, endBlock) => {\n+    // Sort for correct execution order\n+    let events = [...origEvents];\n+    events.sort((a, b) => a.o.blockNumber - b.o.blockNumber || a.o.transactionIndex - b.o.transactionIndex || a.o.index - b.o.index);\n+    let index = 0;    \n+    while (index < maxEvents && index < events.length) {\n+        // Include the entire next block\n+        const includeBlock = events[index].o.blockNumber;\n+        index++;\n+        while (index < events.length && events[index].o.blockNumber === includeBlock) {\n+            index++;\n+        }\n+    }\n+    // Nothing to trim!\n+    if (index >= events.length) {\n+        return [events, endBlock];\n+    }\n+    // We processed up to the last events' blockNumber\n+    // `index` is *exclusive* when trimming\n+    const useEvents = [...events.slice(0, index)];\n+    return [useEvents, useEvents[useEvents.length - 1].o.blockNumber];\n+};\n+\n+\n+const updateLastProcessed = async (issuerId, lastProcessedBlock) => {\n+    return updateIssuerById(issuerId, {last_processed_block: lastProcessedBlock});\n+};"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 6,
                "deletions": 5,
                "patch": "@@ -1,19 +1,20 @@\n-import mongoose from \"mongoose\";\n import dotenv from \"dotenv\";\n+import mongoose from \"mongoose\";\n \n dotenv.config();\n \n const DATABASE_URL = process.env.DATABASE_URL;\n+const DATABASE_OVERRIDE = process.env.DATABASE_OVERRIDE;\n \n-const connectDB = async () => {\n+export const connectDB = async () => {\n+    const connectOptions = DATABASE_OVERRIDE ? {dbName: DATABASE_OVERRIDE} : {};\n     try {\n-        await mongoose.connect(DATABASE_URL);\n+        await mongoose.connect(DATABASE_URL, connectOptions);\n         console.log(\"\u2705 | Mongo connected succesfully\");\n+        return mongoose.connection;\n     } catch (error) {\n         console.error(\"\u274c | Error connecting to Mongo\", error.message);\n         // Exit process with failure\n         process.exit(1);\n     }\n };\n-\n-export default connectDB;"
            },
            {
                "filename": "src/db/objects/Issuer.js",
                "additions": 2,
                "deletions": 0,
                "patch": "@@ -17,6 +17,8 @@ const IssuerSchema = new mongoose.Schema({\n     initial_shares_authorized: String,\n     comments: [String],\n     deployed_to: String,\n+    tx_hash: String,\n+    last_processed_block: { type: Number, default: null },\n     is_manifest_created: { type: Boolean, default: false },\n }, { timestamps: true });\n "
            },
            {
                "filename": "src/db/operations/atomic.ts",
                "additions": 100,
                "deletions": 0,
                "patch": "@@ -0,0 +1,100 @@\n+// Store a global mongo session to allows us to bundle CRUD operations into one transaction\n+\n+import { Connection, QueryOptions } from \"mongoose\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+type TQueryOptions = QueryOptions | null;\n+\n+let _globalSession = null;\n+\n+\n+export const setGlobalSession = (session) => {\n+    if (_globalSession !== null) {\n+        throw new Error(\n+            `globalSession is already set! ${_globalSession}. \n+            Nested transactions are not supported`\n+        );\n+    }\n+    _globalSession = session;\n+}\n+\n+export const clearGlobalSession = () => {\n+    _globalSession = null;\n+}\n+\n+const isReplSet = () => {\n+    if (process.env.DATABASE_REPLSET === \"1\") {\n+        return true;\n+    } \n+    return false;\n+}\n+\n+export const withGlobalTransaction = async (func: () => Promise<void>, useConn?: Connection) => {\n+    if (!isReplSet()) {\n+        // Transactions in mongo only work when running with --replSet\n+        //  https://www.mongodb.com/docs/manual/tutorial/convert-standalone-to-replica-set/\n+        return await func();\n+    }\n+\n+    // Wrap a user defined `func` in a global transaction\n+    const dbConn = useConn || await connectDB();\n+    await dbConn.transaction(async (session) => {\n+        setGlobalSession(session);\n+        try {\n+            return await func();\n+        } finally {\n+            clearGlobalSession();\n+        }\n+    });\n+}\n+\n+\n+const includeSession = (options?: TQueryOptions) => {\n+    let useOptions = options || {};\n+    if (_globalSession !== null) {\n+        if (useOptions.session) {\n+            throw new Error(`options.session is already set!: ${useOptions}`);\n+        }\n+        useOptions.session = _globalSession;\n+    }\n+    return useOptions;\n+}\n+\n+/* \n+Wrapped mongoose db calls. All mongo interaction should go through a function below\n+*/\n+\n+// CREATE\n+\n+export const save = (model, options?: TQueryOptions) => {\n+    return model.save(includeSession(options));\n+}\n+\n+// UPDATE\n+\n+export const findByIdAndUpdate = (model, id, updatedData, options?: TQueryOptions) => {\n+    return model.findByIdAndUpdate(id, updatedData, includeSession(options));\n+}\n+\n+// DELETE\n+\n+export const findByIdAndDelete = (model, id, options?: TQueryOptions) => {\n+    return model.findByIdAndDelete(id, includeSession(options));\n+}\n+\n+// QUERY\n+\n+export const findById = (model, id, projection?, options?: TQueryOptions) => {\n+    return model.findById(id, projection, includeSession(options));\n+}\n+\n+export const findOne = (model, filter, projection?, options?: TQueryOptions) => {\n+    return model.findOne(filter, projection, includeSession(options));\n+}\n+\n+export const find = (model, filter, projection?, options?: TQueryOptions) => {\n+    return model.find(filter, projection, includeSession(options));\n+}\n+\n+export const countDocuments = (model, options?: TQueryOptions) => {\n+    return model.countDocuments(includeSession(options));\n+}"
            },
            {
                "filename": "src/db/operations/create.js",
                "additions": 16,
                "deletions": 27,
                "patch": "@@ -1,72 +1,61 @@\n+import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n+import ConvertibleIssuance from \"../objects/transactions/issuance/ConvertibleIssuance.js\";\n+import EquityCompensationIssuance from \"../objects/transactions/issuance/EquityCompensationIssuance.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n-import EquityCompensationIssuance from \"../objects/transactions/issuance/EquityCompensationIssuance.js\";\n-import ConvertibleIssuance from \"../objects/transactions/issuance/ConvertibleIssuance.js\";\n+import { save } from \"./atomic.ts\";\n \n export const createIssuer = (issuerData) => {\n-    const issuer = new Issuer(issuerData);\n-    return issuer.save();\n+    return save(new Issuer(issuerData));\n };\n \n export const createStakeholder = (stakeholderData) => {\n-    const stakeholder = new Stakeholder(stakeholderData);\n-    return stakeholder.save();\n+    return save(new Stakeholder(stakeholderData));\n };\n \n export const createStockClass = (stockClassData) => {\n-    const stockClass = new StockClass(stockClassData);\n-    return stockClass.save();\n+    return save(new StockClass(stockClassData));\n };\n \n export const createStockLegendTemplate = (stockLegendTemplateData) => {\n-    const stockLegendTemplate = new StockLegendTemplate(stockLegendTemplateData);\n-    return stockLegendTemplate.save();\n+    return save(new StockLegendTemplate(stockLegendTemplateData));\n };\n \n export const createStockPlan = (stockPlanData) => {\n-    const stockPlan = new StockPlan(stockPlanData);\n-    return stockPlan.save();\n+    return save(new StockPlan(stockPlanData));\n };\n \n export const createValuation = (valuationData) => {\n-    const valuation = new Valuation(valuationData);\n-    return valuation.save();\n+    return save(new Valuation(valuationData));\n };\n \n export const createVestingTerms = (vestingTermsData) => {\n-    const vestingTerms = new VestingTerms(vestingTermsData);\n-    return vestingTerms.save();\n+    return save(new VestingTerms(vestingTermsData));\n };\n \n export const createHistoricalTransaction = (transactionHistoryData) => {\n-    const historicalTransaction = new HistoricalTransaction(transactionHistoryData);\n-    return historicalTransaction.save();\n+    return save(new HistoricalTransaction(transactionHistoryData));\n };\n \n export const createStockIssuance = (stockIssuanceData) => {\n-    const stockIssuance = new StockIssuance(stockIssuanceData);\n-    return stockIssuance.save();\n+    return save(new StockIssuance(stockIssuanceData));\n };\n \n export const createEquityCompensationIssuance = (issuanceData) => {\n-    const equityCompensationIssuance = new EquityCompensationIssuance(issuanceData);\n-    return equityCompensationIssuance.save();\n+    return save(new EquityCompensationIssuance(issuanceData));\n };\n \n export const createConvertibleIssuance = (issuanceData) => {\n-    const convertibleIssuance = new ConvertibleIssuance(issuanceData);\n-    return convertibleIssuance.save();\n+    return save(new ConvertibleIssuance(issuanceData));\n };\n \n export const createStockTransfer = (stockTransferData) => {\n-    const stockTransfer = new StockTransfer(stockTransferData);\n-    return stockTransfer.save();\n+    return save(new StockTransfer(stockTransferData));\n };"
            },
            {
                "filename": "src/db/operations/delete.js",
                "additions": 9,
                "deletions": 8,
                "patch": "@@ -6,37 +6,38 @@ import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n+import { findByIdAndDelete } from \"./atomic.ts\";\n \n // TODO: since we're doing a time series db that's meant to be immutable, is this needed?\n \n export const deleteIssuerById = (issuerId) => {\n-    return Issuer.findByIdAndDelete(issuerId);\n+    return findByIdAndDelete(Issuer, issuerId);\n };\n \n export const deleteStakeholderById = (stakeholderId) => {\n-    return Stakeholder.findByIdAndDelete(stakeholderId);\n+    return findByIdAndDelete(Stakeholder, stakeholderId);\n };\n \n export const deleteStockClassById = (stockClassId) => {\n-    return StockClass.findByIdAndDelete(stockClassId);\n+    return findByIdAndDelete(StockClass, stockClassId);\n };\n \n export const deleteStockLegendTemplateById = (stockLegendTemplateId) => {\n-    return StockLegendTemplate.findByIdAndDelete(stockLegendTemplateId);\n+    return findByIdAndDelete(StockLegendTemplate, stockLegendTemplateId);\n };\n \n export const deleteStockPlanById = (stockPlanId) => {\n-    return StockPlan.findByIdAndDelete(stockPlanId);\n+    return findByIdAndDelete(StockPlan, stockPlanId);\n };\n \n export const deleteValuationById = (valuationId) => {\n-    return Valuation.findByIdAndDelete(valuationId);\n+    return findByIdAndDelete(Valuation, valuationId);\n };\n \n export const deleteVestingTermsById = (vestingTermsId) => {\n-    return VestingTerms.findByIdAndDelete(vestingTermsId);\n+    return findByIdAndDelete(VestingTerms, vestingTermsId);\n };\n \n export const deleteTransactionById = (transactionId) => {\n-    return StockIssuance.findByIdAndDelete(transactionId);\n+    return findByIdAndDelete(StockIssuance, transactionId);\n };"
            },
            {
                "filename": "src/db/operations/read.js",
                "additions": 25,
                "deletions": 40,
                "patch": "@@ -1,97 +1,84 @@\n+import Factory from \"../objects/Factory.js\";\n+import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import Factory from \"../objects/Factory.js\";\n+import { countDocuments, find, findById } from \"./atomic.ts\";\n \n // READ By ID\n export const readIssuerById = async (id) => {\n-    const issuer = await Issuer.findById(id);\n-    return issuer;\n+    return await findById(Issuer, id);\n };\n \n export const readStakeholderById = async (id) => {\n-    const stakeholder = await Stakeholder.findById(id);\n-    return stakeholder;\n+    return await findById(Stakeholder, id);\n };\n \n export const readStockClassById = async (id) => {\n-    const stockClass = await StockClass.findById(id);\n-    return stockClass;\n+    return await findById(StockClass, id);\n };\n \n export const readStockLegendTemplateById = async (id) => {\n-    const stockLegendTemplate = await StockLegendTemplate.findById(id);\n-    return stockLegendTemplate;\n+    return await findById(StockLegendTemplate, id);\n };\n \n export const readStockPlanById = async (id) => {\n-    const stockPlan = await StockPlan.findById(id);\n-    return stockPlan;\n+    return await findById(StockPlan, id);\n };\n \n export const readValuationById = async (id) => {\n-    const valuation = await Valuation.findById(id);\n-    return valuation;\n+    return await findById(Valuation, id);\n };\n \n export const readVestingTermsById = async (id) => {\n-    const vestingTerms = await VestingTerms.findById(id);\n-    return vestingTerms;\n+    return await findById(VestingTerms, id);\n };\n \n+// READ Multiple\n export const readHistoricalTransactionByIssuerId = async (issuerId) => {\n-    const historicalTransactions = await HistoricalTransaction.find({ issuer: issuerId }).populate(\"transaction\");\n-    return historicalTransactions;\n+    return await find(HistoricalTransaction, { issuer: issuerId }).populate(\"transaction\");\n };\n \n // COUNT\n export const countIssuers = async () => {\n-    const totalIssuers = await Issuer.countDocuments();\n-    return totalIssuers;\n+    return await countDocuments(Issuer);\n };\n \n export const countStakeholders = async () => {\n-    const totalStakeholders = await Stakeholder.countDocuments();\n-    return totalStakeholders;\n+    return await countDocuments(Stakeholder);\n };\n \n export const countStockClasses = async () => {\n-    const totalStockClasses = await StockClass.countDocuments();\n-    return totalStockClasses;\n+    return await countDocuments(StockClass);\n };\n \n export const countStockLegendTemplates = async () => {\n-    const totalTemplates = await StockLegendTemplate.countDocuments();\n-    return totalTemplates;\n+    return await countDocuments(StockLegendTemplate);\n };\n \n export const countStockPlans = async () => {\n-    const totalStockPlans = await StockPlan.countDocuments();\n-    return totalStockPlans;\n+    return await countDocuments(StockPlan);\n };\n \n export const countValuations = async () => {\n-    const totalValuations = await Valuation.countDocuments();\n-    return totalValuations;\n+    return await countDocuments(Valuation);\n };\n \n export const countVestingTerms = async () => {\n-    const totalVestingTerms = await VestingTerms.countDocuments();\n-    return totalVestingTerms;\n+    return await countDocuments(VestingTerms);\n };\n \n export const getAllIssuerDataById = async (issuerId) => {\n-    const issuerStakeholders = await Stakeholder.find({ issuer: issuerId });\n-    const issuerStockClasses = await StockClass.find({ issuer: issuerId });\n-    const issuerStockIssuances = await StockIssuance.find({ issuer: issuerId });\n-    const issuerStockTransfers = await StockTransfer.find({ issuer: issuerId });\n+    const issuerStakeholders = await find(Stakeholder, { issuer: issuerId });\n+    const issuerStockClasses = await find(StockClass, { issuer: issuerId });\n+    const issuerStockIssuances = await find(StockIssuance, { issuer: issuerId });\n+    const issuerStockTransfers = await find(StockTransfer, { issuer: issuerId });\n \n     return {\n         stakeholders: issuerStakeholders,\n@@ -102,11 +89,9 @@ export const getAllIssuerDataById = async (issuerId) => {\n };\n \n export const readAllIssuers = async () => {\n-    const issuers = await Issuer.find();\n-    return issuers;\n+    return await find(Issuer);\n }\n \n export const readFactory = async () => {\n-    const factory = await Factory.find();\n-    return factory;\n+    return await find(Factory);\n }\n\\ No newline at end of file"
            },
            {
                "filename": "src/db/operations/transactions.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,8 +1,8 @@\n import * as Acceptance from \"../objects/transactions/acceptance/index.js\";\n import * as Adjustment from \"../objects/transactions/adjustment/index.js\";\n import * as Cancellation from \"../objects/transactions/cancellation/index.js\";\n-import * as Exercise from \"../objects/transactions/exercise/index.js\";\n import * as Conversion from \"../objects/transactions/conversion/index.js\";\n+import * as Exercise from \"../objects/transactions/exercise/index.js\";\n import * as Issuance from \"../objects/transactions/issuance/index.js\";\n import * as Reissuance from \"../objects/transactions/reissuance/index.js\";\n import * as Release from \"../objects/transactions/release/index.js\";\n@@ -12,6 +12,7 @@ import * as ReturnToPool from \"../objects/transactions/return_to_pool/index.js\";\n import * as Split from \"../objects/transactions/split/index.js\";\n import * as Transfer from \"../objects/transactions/transfer/index.js\";\n import * as Vesting from \"../objects/transactions/vesting/index.js\";\n+import { save } from \"./atomic.ts\";\n \n const typeToModelType = {\n     // Acceptance\n@@ -91,7 +92,7 @@ const addTransactions = async (inputTransactions, issuerId) => {\n         inputTransaction = { ...inputTransaction, issuer: issuerId };\n         const ModelType = typeToModelType[inputTransaction.object_type];\n         if (ModelType) {\n-            const transaction = await new ModelType(inputTransaction).save();\n+            const transaction = await save(new ModelType(inputTransaction));\n             console.log(`${inputTransaction.object_type} transaction added. Details:`, JSON.stringify(transaction, null, 2));\n         } else {\n             console.log(`Unknown object type for transaction:`, JSON.stringify(inputTransaction, null, 2));"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 46,
                "deletions": 38,
                "patch": "@@ -1,95 +1,103 @@\n+import sleep from \"../../utils/sleep.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n-import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n+import StockAcceptance from \"../objects/transactions/acceptance/StockAcceptance.js\";\n+import IssuerAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.js\";\n+import StockClassAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.js\";\n import StockCancellation from \"../objects/transactions/cancellation/StockCancellation.js\";\n-import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n+import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockReissuance from \"../objects/transactions/reissuance/StockReissuance.js\";\n import StockRepurchase from \"../objects/transactions/repurchase/StockRepurchase.js\";\n-import StockAcceptance from \"../objects/transactions/acceptance/StockAcceptance.js\";\n-import StockClassAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.js\";\n-import IssuerAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.js\";\n+import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n+import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n+import { findByIdAndUpdate } from \"./atomic.ts\";\n+\n+\n+export const web3WaitTime = 5000;\n+\n+\n+const retryOnMiss = async (updateFunc, numRetries = 5, waitBase = null) => {\n+    /* kkolze: When polling `latest` instead of `finalized` web3 blocks, web3 can get ahead of mongo \n+      For example, see the `issuer.post(\"/create\"` code: the issuer is created in mongo after deployCapTable is called  \n+      We add retries to ensure the server routes have written to mongo  */\n+    let tried = 0;\n+    const waitMultiplier = waitBase || web3WaitTime;\n+    while (tried <= numRetries) {\n+        const res = await updateFunc();\n+        if (res !== null) {\n+            return res;\n+        }\n+        tried++;\n+        await sleep(tried * waitMultiplier, \"Returned null, retrying in \");\n+    }\n+}\n \n \n export const updateIssuerById = async (id, updatedData) => {\n-    const issuer = await Issuer.findByIdAndUpdate(id, updatedData, { new: true });\n-    return issuer;\n+    return await findByIdAndUpdate(Issuer, id, updatedData, { new: true });\n };\n \n export const updateStakeholderById = async (id, updatedData) => {\n-    const stakeholder = await Stakeholder.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stakeholder;\n+    return await retryOnMiss(async () => findByIdAndUpdate(Stakeholder, id, updatedData, { new: true }));\n };\n \n export const updateStockClassById = async (id, updatedData) => {\n-    const stockClass = await StockClass.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockClass;\n+    return await retryOnMiss(async () => findByIdAndUpdate(StockClass, id, updatedData, { new: true }));\n };\n \n export const updateStockLegendTemplateById = async (id, updatedData) => {\n-    const stockLegendTemplate = await StockLegendTemplate.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockLegendTemplate;\n+    return await findByIdAndUpdate(StockLegendTemplate, id, updatedData, { new: true });\n };\n \n export const updateStockPlanById = async (id, updatedData) => {\n-    const stockPlan = await StockPlan.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockPlan;\n+    return await findByIdAndUpdate(StockPlan, id, updatedData, { new: true });\n };\n \n export const updateValuationById = async (id, updatedData) => {\n-    const valuation = await Valuation.findByIdAndUpdate(id, updatedData, { new: true });\n-    return valuation;\n+    return await findByIdAndUpdate(Valuation, id, updatedData, { new: true });\n };\n \n export const updateVestingTermsById = async (id, updatedData) => {\n-    const vestingTerms = await VestingTerms.findByIdAndUpdate(id, updatedData, { new: true });\n-    return vestingTerms;\n+    return await findByIdAndUpdate(VestingTerms, id, updatedData, { new: true });\n };\n \n export const upsertStockIssuanceById = async (id, updatedData) => {\n-    const stockIssuance = await StockIssuance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockIssuance;\n+    return await findByIdAndUpdate(StockIssuance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockTransferById = async (id, updatedData) => {\n-    const stockTransfer = await StockTransfer.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockTransfer;\n+    return await findByIdAndUpdate(StockTransfer, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockCancellationById = async (id, updatedData) => {\n-    const stockCancellation = await StockCancellation.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockCancellation;\n+    return await findByIdAndUpdate(StockCancellation, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockRetractionById = async (id, updatedData) => {\n-    const stockRetraction = await StockRetraction.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockRetraction;\n+    return await findByIdAndUpdate(StockRetraction, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockReissuanceById = async (id, updatedData) => {\n-    const stockReissuance = await StockReissuance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockReissuance;\n+    return await findByIdAndUpdate(StockReissuance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockRepurchaseById = async (id, updatedData) => {\n-    const stockRepurchase = await StockRepurchase.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockRepurchase;\n+    return await findByIdAndUpdate(StockRepurchase, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockAcceptanceById = async (id, updatedData) => {\n-    const stockAcceptance = await StockAcceptance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockAcceptance;\n+    return await findByIdAndUpdate(StockAcceptance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockClassAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await StockClassAuthorizedSharesAdjustment.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockClassAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n };\n-export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await IssuerAuthorizedSharesAdjustment.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n \n-}\n+export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n+    return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n+};"
            },
            {
                "filename": "src/db/scripts/deseed.js",
                "additions": 4,
                "deletions": 40,
                "patch": "@@ -1,47 +1,11 @@\n-import mongoose from \"mongoose\";\n-import connectDB from \"../config/mongoose.js\";\n-import Issuer from \"../objects/Issuer.js\";\n-import Stakeholder from \"../objects/Stakeholder.js\";\n-import StockClass from \"../objects/StockClass.js\";\n-import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n-import StockPlan from \"../objects/StockPlan.js\";\n-import Valuation from \"../objects/Valuation.js\";\n-import VestingTerms from \"../objects/VestingTerms.js\";\n-import { typeToModelType } from \"../operations/transactions.js\"; // Import the typeToModelType object to delete all transactions\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n+import { deseedDatabase } from \"../../tests/integration/utils.ts\";\n \n-const deleteAllTransactions = async () => {\n-    for (const ModelType of Object.values(typeToModelType)) {\n-        await ModelType.deleteMany({});\n-    }\n-};\n-\n-const deleteAll = async () => {\n-    // Delete all documents from the collections\n-    await Issuer.deleteMany({});\n-    await Stakeholder.deleteMany({});\n-    await StockClass.deleteMany({});\n-    await StockLegendTemplate.deleteMany({});\n-    await StockPlan.deleteMany({});\n-    await Valuation.deleteMany({});\n-    await VestingTerms.deleteMany({});\n-    await HistoricalTransaction.deleteMany({});\n-    await deleteAllTransactions(); // Delete all transactions\n-};\n-\n-const deseedDatabase = async () => {\n+const runDeseed = async () => {\n     try {\n-        connectDB();\n-\n-        await deleteAll();\n-\n-        console.log(\"\u2705 Database deseeded successfully\");\n-\n-        // Close the database connection\n-        await mongoose.connection.close();\n+        await deseedDatabase();\n     } catch (err) {\n         console.log(\"\u274c Error deseeding database:\", err);\n     }\n };\n \n-deseedDatabase();\n+runDeseed();"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 89,
                "deletions": 0,
                "patch": "@@ -0,0 +1,89 @@\n+import { Router } from \"express\";\n+import Issuer from \"../db/objects/Issuer\";\n+import Stakeholder from \"../db/objects/Stakeholder\";\n+import StockClass from \"../db/objects/StockClass\";\n+import { StockIssuance } from \"../db/objects/transactions/issuance\";\n+import { getIssuerContract } from \"../utils/caches\";\n+import { decimalScaleValue } from \"../utils/convertToFixedPointDecimals\";\n+import { convertUUIDToBytes16 } from \"../utils/convertUUID\";\n+\n+export const capTable = Router();\n+\n+capTable.get(\"/\", async (req, res) => {\n+    res.send(\"Hello Cap Table!\");\n+});\n+\n+capTable.get(\"/latest\", async (req, res) => {\n+    /* \n+    TODO: handle this in the polling process? or maybe just cache it once in a while?\n+     It will get slow once we have 50+ stakeholders\n+    */\n+    const issuerId = req.query.issuerId;\n+    try {\n+        const stakeholders = await Stakeholder.find({issuer: issuerId});\n+        const stockClasses = await StockClass.find({issuer: issuerId});\n+        const issuer = await Issuer.findById(issuerId);\n+        // Grouping by stakeholder_id and stock_class_id, grab the records with the largest createdAt time\n+        const issuances = await StockIssuance.aggregate([\n+            {\n+                $group: {\n+                    _id: {\n+                        stakeholder_id: \"$stakeholder_id\",\n+                        stock_class_id: \"$stock_class_id\"\n+                    },\n+                    maxDate: { $max: \"$createdAt\" }\n+                }\n+            },\n+            {\n+                $lookup: {\n+                    from: \"stockissuances\",\n+                    let: { stakeholder_id: \"$_id.stakeholder_id\", stock_class_id: \"$_id.stock_class_id\", maxDate: \"$maxDate\" },\n+                    pipeline: [\n+                        {\n+                            $match: {\n+                                $expr: {\n+                                    $and: [\n+                                        { $eq: [\"$stakeholder_id\", \"$$stakeholder_id\"] },\n+                                        { $eq: [\"$stock_class_id\", \"$$stock_class_id\"] },\n+                                        { $eq: [\"$createdAt\", \"$$maxDate\"] }\n+                                    ]\n+                                }\n+                            }\n+                        }\n+                    ],\n+                    as: \"issuanceData\"\n+                }\n+            },\n+            { $unwind: \"$issuanceData\" },\n+            { $replaceRoot: { newRoot: \"$issuanceData\" } }\n+        ]);\n+      \n+        // We need to hit web3 to see which are actually valid\n+        const { contract } = await getIssuerContract(issuer);\n+        let holdings = [];\n+        const stakeholderMap = Object.fromEntries(stakeholders.map((x) => { return [x._id, x]; }));\n+        const stockClassMap = Object.fromEntries(stockClasses.map((x) => { return [x._id, x]; }));\n+        for (const issuance of issuances) {\n+            const { stakeholder_id, security_id, stock_class_id } = issuance;\n+            const [_, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+                convertUUIDToBytes16(stakeholder_id),\n+                convertUUIDToBytes16(security_id),\n+            );\n+            if (quantity == 0) {\n+                continue;\n+            }\n+            holdings.push({\n+                issuance,\n+                stockClass: stockClassMap[stock_class_id],\n+                stakeholder: stakeholderMap[stakeholder_id],\n+                quantity: Number(quantity) / decimalScaleValue,\n+                sharePrice: Number(sharePrice) / decimalScaleValue,\n+                timestamp: Number(timestamp) * 1000,\n+            });\n+        }\n+        res.send({ holdings, stockClasses, issuer });\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+})"
            },
            {
                "filename": "src/routes/index.js",
                "additions": 3,
                "deletions": 11,
                "patch": "@@ -1,19 +1,16 @@\n import { Router } from \"express\";\n import deployCapTable from \"../chain-operations/deployCapTable.js\";\n+import { updateIssuerById } from \"../db/operations/update.js\";\n import seedDB from \"../db/scripts/seed.js\";\n-import { contractCache } from \"../utils/caches.js\";\n import { convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import processManifest from \"../utils/processManifest.js\";\n-import { updateIssuerById } from \"../db/operations/update.js\";\n-import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n \n const router = Router();\n \n router.get(\"/\", async (req, res) => {\n-    console.log(\"Welcome to TAP\")\n+    console.log(\"Welcome to TAP\");\n     res.status(200).send(`Welcome to the future of Transfer Agents \ud83d\udcb8`);\n-})\n-\n+});\n \n router.post(\"/mint-cap-table\", async (req, res) => {\n     try {\n@@ -25,11 +22,6 @@ router.post(\"/mint-cap-table\", async (req, res) => {\n         const { contract, address, provider, libraries } = await deployCapTable(issuerIdBytes16, issuer.legal_name, issuer.initial_shares_authorized);\n \n         const savedIssuerWithDeployedTo = await updateIssuerById(issuer._id, { deployed_to: address });\n-\n-        // add contract to the cache and start listener\n-        contractCache[issuer._id] = { contract, provider, libraries };\n-        await startOnchainListeners(contract, provider, issuer._id, libraries);\n-\n         res.status(200).send({ issuer: savedIssuerWithDeployedTo });\n     } catch (error) {\n         console.error(`error: ${error}`);"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 2,
                "deletions": 8,
                "patch": "@@ -8,9 +8,6 @@ import { countIssuers, readIssuerById } from \"../db/operations/read.js\";\n import { convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n-import { contractCache } from \"../utils/caches.js\";\n-import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n-\n const issuer = Router();\n \n issuer.get(\"/\", async (req, res) => {\n@@ -56,19 +53,16 @@ issuer.post(\"/create\", async (req, res) => {\n \n         const issuerIdBytes16 = convertUUIDToBytes16(incomingIssuerToValidate.id);\n         console.log(\"\ud83d\udcbe | Issuer id in bytes16 \", issuerIdBytes16);\n-        const { contract, provider, address, libraries } = await deployCapTable(\n+        const { address, deployHash } = await deployCapTable(\n             issuerIdBytes16,\n             incomingIssuerToValidate.legal_name,\n             incomingIssuerToValidate.initial_shares_authorized\n         );\n \n-        // add contract to the cache and start listener\n-        contractCache[incomingIssuerToValidate.id] = { contract, provider, libraries };\n-        startOnchainListeners(contract, provider, incomingIssuerToValidate.id, libraries);\n-\n         const incomingIssuerForDB = {\n             ...incomingIssuerToValidate,\n             deployed_to: address,\n+            tx_hash: deployHash,\n         };\n \n         const issuer = await createIssuer(incomingIssuerForDB);"
            },
            {
                "filename": "src/routes/stakeholder.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -10,8 +10,8 @@ import {\n \n import stakeholderSchema from \"../../ocf/schema/objects/Stakeholder.schema.json\" assert { type: \"json\" };\n import { createStakeholder } from \"../db/operations/create.js\";\n-import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n import { readIssuerById } from \"../db/operations/read.js\";\n+import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n const stakeholder = Router();\n "
            },
            {
                "filename": "src/scripts/sampleData.js",
                "additions": 19,
                "deletions": 1,
                "patch": "@@ -1,5 +1,5 @@\n export const issuer = {\n-    legal_name: \"Poet Network Inc.\",\n+    legal_name: \"Banana Peppers Inc.\",\n     formation_date: \"2022-08-23\",\n     country_of_formation: \"US\",\n     country_subdivision_of_formation: \"DE\",\n@@ -174,6 +174,24 @@ export const stakeholder2 = (issuerId) => {\n         },\n     };\n };\n+\n+export const stakeholder3 = (issuerId) => {\n+    return {\n+        issuerId,\n+        data: {\n+            name: {\n+                legal_name: \"Kent Kolze\",\n+                first_name: \"Kent\",\n+                last_name: \"Kolze\",\n+            },\n+            issuer_assigned_id: \"\",\n+            stakeholder_type: \"INDIVIDUAL\",\n+            current_relationship: \"EMPLOYEE\",\n+            comments: [],\n+        },\n+    };\n+};\n+\n export const stockClass = (issuerId) => {\n     return {\n         issuerId,"
            },
            {
                "filename": "src/scripts/testAcceptance.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockAccept } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testCancellation.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -1,7 +1,7 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { issuer, stakeholder1, stakeholder2, stockCancel, stockClass, stockIssuance, stockTransfer } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { stockCancel } from \"./sampleData.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testIssuance.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,9 +1,9 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import Issuer from \"../db/objects/Issuer.js\";\n import Stakeholder from \"../db/objects/Stakeholder.js\";\n import StockClass from \"../db/objects/StockClass.js\";\n-import axios from \"axios\";\n import { stockIssuance } from \"./sampleData.js\";\n-import connectDB from \"../db/config/mongoose.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testIssuerAdjustment.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -1,7 +1,7 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { stockClass, stockClassAuthorizedSharesAdjust, issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testReissuance.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockReissue } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testRepurchase.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,7 +1,7 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockRepurchase } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testRetraction.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockRetract } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n // Connect to MongoDB\n connectDB();\n "
            },
            {
                "filename": "src/scripts/testStockClassAdjustment.js",
                "additions": 4,
                "deletions": 3,
                "patch": "@@ -1,7 +1,8 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { stockClass, stockClassAuthorizedSharesAdjust, issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { stockClassAuthorizedSharesAdjust } from \"./sampleData.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/server.js",
                "additions": 7,
                "deletions": 86,
                "patch": "@@ -1,90 +1,11 @@\n-import { config } from \"dotenv\";\n-import express, { json, urlencoded } from \"express\";\n-config();\n+import { startServer } from \"./app.js\";\n \n-import connectDB from \"./db/config/mongoose.js\";\n+// Function to check if the flag is present\n+const isFlagPresent = (flag) => process.argv.includes(flag);\n \n-import getContractInstance from \"./chain-operations/getContractInstances.js\";\n-import startOnchainListeners from \"./chain-operations/transactionListener.js\";\n+// Setting the default value of the flag to true\n+const finalizedOnly = isFlagPresent(\"--finalized-only\");\n \n-// Routes\n-import historicalTransactions from \"./routes/historicalTransactions.js\";\n-import mainRoutes from \"./routes/index.js\";\n-import issuerRoutes from \"./routes/issuer.js\";\n-import stakeholderRoutes from \"./routes/stakeholder.js\";\n-import stockClassRoutes from \"./routes/stockClass.js\";\n-import stockLegendRoutes from \"./routes/stockLegend.js\";\n-import stockPlanRoutes from \"./routes/stockPlan.js\";\n-import transactionRoutes from \"./routes/transactions.js\";\n-import valuationRoutes from \"./routes/valuation.js\";\n-import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n+console.log(\"Finalized Only:\", finalizedOnly);\n \n-import { readIssuerById, readAllIssuers } from \"./db/operations/read.js\";\n-import { contractCache } from \"./utils/caches.js\";\n-\n-const app = express();\n-\n-// Connect to MongoDB\n-connectDB();\n-\n-const PORT = process.env.PORT;\n-\n-// Middleware to get or create contract instance\n-// the listener is first started on deployment, then here as a backup\n-const contractMiddleware = async (req, res, next) => {\n-    if (!req.body.issuerId) {\n-        console.log(\"\u274c | No issuer ID\");\n-        res.status(400).send(\"issuerId is required\");\n-    }\n-\n-    // fetch issuer to ensure it exists\n-    const issuer = await readIssuerById(req.body.issuerId);\n-    if (!issuer) res.status(400).send(\"issuer not found \");\n-\n-    // Check if contract instance already exists in cache\n-    if (!contractCache[req.body.issuerId]) {\n-        const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to);\n-        contractCache[req.body.issuerId] = { contract, provider, libraries };\n-\n-        // Initialize listener for this contract\n-        startOnchainListeners(contract, provider, req.body.issuerId, libraries);\n-    }\n-\n-    req.contract = contractCache[req.body.issuerId].contract;\n-    req.provider = contractCache[req.body.issuerId].provider;\n-    next();\n-};\n-app.use(urlencoded({ limit: \"50mb\", extended: true }));\n-app.use(json({ limit: \"50mb\" }));\n-app.enable(\"trust proxy\");\n-\n-app.use(\"/\", mainRoutes);\n-app.use(\"/issuer\", issuerRoutes);\n-app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n-app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n-// No middleware required since these are only created offchain\n-app.use(\"/stock-legend\", stockLegendRoutes);\n-app.use(\"/stock-plan\", stockPlanRoutes);\n-app.use(\"/valuation\", valuationRoutes);\n-app.use(\"/vesting-terms\", vestingTermsRoutes);\n-app.use(\"/historical-transactions\", historicalTransactions);\n-\n-// transactions\n-app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n-\n-app.listen(PORT, async () => {\n-    console.log(`\ud83d\ude80  Server successfully launched on port ${PORT}`);\n-     // Fetch all issuers\n-     const issuers = await readAllIssuers();\n-     if (issuers && issuers.length > 0) {\n-         for (const issuer of issuers) {\n-             if (issuer.deployed_to) {\n-                 // Create a new contract instance for each issuer\n-                 const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to);\n- \n-                 // Initialize listener for this contract\n-                 startOnchainListeners(contract, provider, issuer._id, libraries);\n-             }\n-         }\n-     }\n-});\n+startServer(finalizedOnly);"
            },
            {
                "filename": "src/state-machines/process.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,6 +1,6 @@\n import { interpret } from \"xstate\";\n+import { preProcessorCache } from \"../utils/caches.ts\";\n import { parentMachine } from \"./parent.js\";\n-import { preProcessorCache } from \"../utils/caches.js\";\n \n /*\n     @dev: Parent-Child machines are created to calculate current context then deleted."
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 122,
                "deletions": 0,
                "patch": "@@ -0,0 +1,122 @@\n+import axios from \"axios\";\n+import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n+import Factory from \"../../db/objects/Factory\";\n+import { web3WaitTime } from \"../../db/operations/update\";\n+import { issuer as exampleIssuer, stakeholder1, stakeholder2, stakeholder3, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n+import sleep from \"../../utils/sleep\";\n+import { SERVER_BASE, runLocalServer, shutdownLocalServer } from \"./utils\";\n+\n+\n+// Pro-tip: set this to iterate faster in dev after `seedExampleData` finishes\n+const HARDCODED_ISSUER_ID = null;\n+\n+beforeAll(async () => {\n+    await runLocalServer(!HARDCODED_ISSUER_ID);\n+}, 10000);\n+\n+afterAll(shutdownLocalServer, 10000);\n+\n+const WAIT_TIME = 1000;\n+\n+const allowPropagate = async () => {\n+    // Ensure ethers has enough time to catch up\n+    await sleep(WAIT_TIME);\n+}\n+\n+const seedExampleData = async () => {\n+    const rec = await Factory.findOne();\n+    if (!rec) {\n+        throw new Error(\n+            `Manually create the {\"implementation_adress\": ..., \"factory_address\": ...} record \n+            in \"factories\" collection. Run the \"forge script ...\" command from the comment \n+            in \"chain/script/CapTableFactory.s.sol\"`\n+        );\n+    }\n+\n+    const issuerResponse = await axios.post(`${SERVER_BASE}/issuer/create`, exampleIssuer);\n+    const issuerId = issuerResponse.data.issuer._id;\n+    console.log(\"\u2705 | Issuer response \", issuerId, issuerResponse.data);\n+    await allowPropagate();\n+       \n+    const stakeholder1Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder1(issuerId));\n+    const s1Id = stakeholder1Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder1Response\", s1Id, stakeholder1Response.data);\n+    await allowPropagate();\n+    \n+    const stakeholder2Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder2(issuerId));\n+    const s2Id = stakeholder2Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder2Response\", s2Id, stakeholder2Response.data);\n+    await allowPropagate();\n+    \n+    const stakeholder3Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder3(issuerId));\n+    const s3Id = stakeholder3Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder3Response\", s3Id, stakeholder3Response.data);\n+    await allowPropagate();\n+    \n+    const stockClassResponse = await axios.post(`${SERVER_BASE}/stock-class/create`, stockClass(issuerId));\n+    const stockClassId = stockClassResponse.data.stockClass._id;\n+    console.log(\"\u2705 | stockClassResponse\", stockClassId, stockClassResponse.data);\n+    await allowPropagate();\n+    \n+    const stockIssuanceResponse = await axios.post(\n+        `${SERVER_BASE}/transactions/issuance/stock`,\n+        stockIssuance(issuerId, s1Id, stockClassId, \"500\", \"1.2\")\n+    );\n+    const issuance = stockIssuanceResponse.data.stockIssuance;\n+    console.log(\"\u2705 | stockIssuanceResponse\", issuance);\n+    await allowPropagate();\n+    \n+    // TODO: Victor acceptance of issuance?\n+    // const { security_id } = issuance;\n+    // const stockIssuanceAcceptanceResp = await axios.post(\n+    //     `${SERVER_BASE}/transactions/accept/stock`,\n+    //     stockAccept(issuerId, s1Id, stockClassId, security_id, [\"Accepted\"])\n+    // );\n+    // console.log(\"\u2705 | Stock issuance acceptance response\", stockIssuanceAcceptanceResp.data);\n+    // await allowPropagate();\n+    \n+    const stockTransfer1Response = await axios.post(\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n+        stockTransfer(issuerId, \"200\", s1Id, s2Id, stockClassId, \"4.20\")\n+    );\n+    console.log(\"\u2705 | stockTransfer1Response\", stockTransfer1Response.data);\n+    await allowPropagate();\n+    \n+    // TODO: Victor acceptance of transfer1?\n+    // const stockTransferAcceptanceResp = await axios.post(\n+    //     `${SERVER_BASE}/transactions/accept/stock`,\n+    //     stockAccept(issuerId, s2Id, stockClassId, security_id, [\"Accepted\"])\n+    // );\n+    // console.log(\"\u2705 | Stock transfer acceptance response\", stockTransferAcceptanceResp.data);\n+    // await allowPropagate();\n+    \n+    const stockTransfer2Response = await axios.post(\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n+        stockTransfer(issuerId, \"300\", s1Id, s3Id, stockClassId, \"10.66\")\n+    );\n+    console.log(\"\u2705 | stockTransfer2Response\", stockTransfer2Response.data);\n+    await allowPropagate();\n+\n+    // TODO: acceptance of transfer2?\n+\n+    // Allow time for poller process to catch up\n+    await sleep(pollingSleepTime + web3WaitTime + 2000);\n+\n+    return issuerId;\n+}\n+\n+const checkRecs = async (issuerId) => {\n+    const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n+    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice, name: stakeholder.name.legal_name}; });\n+    portions.sort((a, b) => b.quantity - a.quantity);\n+    expect(portions).toStrictEqual([\n+        {quantity: 300, sharePrice: 10.66, name: \"Kent Kolze\"},\n+        {quantity: 200, sharePrice: 4.2, name: \"Victor Mimo\"},\n+    ]);\n+}\n+\n+test('end to end with event processing', async () => {\n+    const issuerId = HARDCODED_ISSUER_ID || await seedExampleData();\n+    await checkRecs(issuerId);\n+    \n+}, WAIT_TIME * 100);"
            },
            {
                "filename": "src/tests/integration/utils.ts",
                "additions": 58,
                "deletions": 0,
                "patch": "@@ -0,0 +1,58 @@\n+import { shutdownServer, startServer } from \"../../app\";\n+import { connectDB } from \"../../db/config/mongoose\";\n+import HistoricalTransaction from \"../../db/objects/HistoricalTransaction\";\n+import Issuer from \"../../db/objects/Issuer\";\n+import Stakeholder from \"../../db/objects/Stakeholder\";\n+import StockClass from \"../../db/objects/StockClass\";\n+import StockLegendTemplate from \"../../db/objects/StockLegendTemplate\";\n+import StockPlan from \"../../db/objects/StockPlan\";\n+import Valuation from \"../../db/objects/Valuation\";\n+import VestingTerms from \"../../db/objects/VestingTerms\";\n+import { typeToModelType } from \"../../db/operations/transactions\"; // Import the typeToModelType object to delete all transactions\n+\n+export const SERVER_BASE = `http://localhost:${process.env.PORT}`;\n+\n+let _server = null;\n+\n+export const runLocalServer = async (deseed) => {\n+    if (deseed) {\n+        await deseedDatabase();\n+    }\n+    console.log(\"starting server\");\n+    _server = await startServer(false);\n+}\n+\n+\n+export const shutdownLocalServer = async () => {\n+    console.log(\"shutting down server\");\n+    await shutdownServer(_server);\n+}\n+\n+\n+const deleteAllTransactions = async () => {\n+    for (const ModelType of Object.values(typeToModelType)) {\n+        // @ts-ignore\n+        await ModelType.deleteMany({});\n+    }\n+};\n+\n+const deleteAll = async () => {\n+    // Delete all documents from the collections\n+    await Issuer.deleteMany({});\n+    await Stakeholder.deleteMany({});\n+    await StockClass.deleteMany({});\n+    await StockLegendTemplate.deleteMany({});\n+    await StockPlan.deleteMany({});\n+    await Valuation.deleteMany({});\n+    await VestingTerms.deleteMany({});\n+    await HistoricalTransaction.deleteMany({});\n+    await deleteAllTransactions(); // Delete all transactions\n+};\n+\n+export const deseedDatabase = async () => {\n+    const connection = await connectDB();\n+    console.log(\"Deseeding from database: \", connection.name);\n+    await deleteAll();\n+    console.log(\"\u2705 Database deseeded successfully\");\n+    await connection.close();\n+};"
            },
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 31,
                "deletions": 0,
                "patch": "@@ -0,0 +1,31 @@\n+import { trimEvents, txFuncs, txTypes } from \"../../chain-operations/transactionPoller\";\n+\n+// TODO: if starts failing again run: yarn add --dev jest-esm-transformer\n+// https://jestjs.io/docs/using-matchers for more docs on `expect`\n+\n+const myEvents = [5, 6, 6, 6, 7, 7, 7].map((x, i) => { return {i, o: {blockNumber: x}}; });\n+\n+test('trimEvents partial', () => {\n+    // @ts-ignore\n+    const [events, block] = trimEvents(myEvents, 2, 10);\n+    expect(events.length).toBe(4);\n+    expect(events).toStrictEqual(myEvents.slice(0, 4));\n+    expect(block).toBe(6);\n+});\n+\n+test('trimEvents full', () => {\n+    // We allow more than maxEvents in order to include all events of the last block\n+    for (const maxEvents of [5, 6, 7, 15]) {\n+        // @ts-ignore\n+        const [events, block] = trimEvents(myEvents, maxEvents, 10);\n+        expect(events.length).toBe(myEvents.length);\n+        expect(events).toStrictEqual(myEvents);\n+        expect(block).toBe(10);\n+    }\n+});\n+\n+test('txMapper to maps', () => {\n+    // @ts-ignore\n+    expect(txTypes[3n]).toBe(\"StockAcceptance\");\n+    expect(txFuncs[\"StockAcceptance\"].name).toBe(\"handleStockAcceptance\");\n+});"
            },
            {
                "filename": "src/utils/caches.js",
                "additions": 0,
                "deletions": 10,
                "patch": "@@ -1,10 +0,0 @@\n-// Centralized contract manager/cache\n-export const contractCache = {};\n-\n-/*\n-issuerId = {\n-        activePositions: {...},\n-        activeSecurityIdsByStockClass: {...},\n-    };\n-*/\n-export const preProcessorCache = {};"
            },
            {
                "filename": "src/utils/caches.ts",
                "additions": 26,
                "deletions": 0,
                "patch": "@@ -0,0 +1,26 @@\n+import { getContractInstance } from \"../chain-operations/getContractInstances.js\";\n+\n+interface CachePayload {\n+    contract: any;\n+    provider: any;\n+    libraries: any;\n+}\n+\n+// Centralized contract manager/cache\n+const contractCache: {[key: string]: CachePayload} = {};\n+\n+export const getIssuerContract = async (issuer): Promise<CachePayload> => {\n+    if (!contractCache[issuer._id]) {\n+        const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to); \n+        contractCache[issuer._id] = { contract, provider, libraries };\n+    }\n+    return contractCache[issuer._id];\n+}\n+\n+/*\n+issuerId = {\n+        activePositions: {...},\n+        activeSecurityIdsByStockClass: {...},\n+    };\n+*/\n+export const preProcessorCache = {};"
            },
            {
                "filename": "src/utils/convertToFixedPointDecimals.js",
                "additions": 4,
                "deletions": 2,
                "patch": "@@ -1,16 +1,18 @@\n import { toBigInt } from \"ethers\";\n \n+export const decimalScaleValue = 1e10;\n+\n // Convert a price to a BigInt\n function toScaledBigNumber(price) {\n-    return toBigInt(Math.round(price * 1e10).toString());\n+    return toBigInt(Math.round(price * decimalScaleValue).toString());\n }\n \n // TODO: might not be refactored correctly from ethers v5 to v6\n // Convert a BigInt back to a decimal price\n function toDecimal(scaledPriceBigInt) {\n     if (typeof scaledPriceBigInt === \"bigint\") {\n         const numberString = scaledPriceBigInt.toString();\n-        return parseFloat(numberString / 1e10).toString();\n+        return parseFloat(numberString / decimalScaleValue).toString();\n     } else {\n         return scaledPriceBigInt;\n     }"
            },
            {
                "filename": "src/utils/sleep.js",
                "additions": 4,
                "deletions": 1,
                "patch": "@@ -1,4 +1,7 @@\n-function sleep(ms) {\n+function sleep(ms, logPrefix = null) {\n+    if (logPrefix) {\n+        console.log(`${logPrefix} ${(ms / 1000).toFixed(1)} seconds`);\n+    }\n     return new Promise((resolve) => setTimeout(resolve, ms));\n }\n "
            },
            {
                "filename": "tsconfig.json",
                "additions": 109,
                "deletions": 0,
                "patch": "@@ -0,0 +1,109 @@\n+{\n+  \"compilerOptions\": {\n+    /* Visit https://aka.ms/tsconfig to read more about this file */\n+\n+    /* Projects */\n+    // \"incremental\": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */\n+    // \"composite\": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */\n+    // \"tsBuildInfoFile\": \"./.tsbuildinfo\",              /* Specify the path to .tsbuildinfo incremental compilation file. */\n+    // \"disableSourceOfProjectReferenceRedirect\": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */\n+    // \"disableSolutionSearching\": true,                 /* Opt a project out of multi-project reference checking when editing. */\n+    // \"disableReferencedProjectLoad\": true,             /* Reduce the number of projects loaded automatically by TypeScript. */\n+\n+    /* Language and Environment */\n+    \"target\": \"ESNext\",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */\n+    // \"lib\": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */\n+    // \"jsx\": \"preserve\",                                /* Specify what JSX code is generated. */\n+    // \"experimentalDecorators\": true,                   /* Enable experimental support for legacy experimental decorators. */\n+    // \"emitDecoratorMetadata\": true,                    /* Emit design-type metadata for decorated declarations in source files. */\n+    // \"jsxFactory\": \"\",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */\n+    // \"jsxFragmentFactory\": \"\",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */\n+    // \"jsxImportSource\": \"\",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */\n+    // \"reactNamespace\": \"\",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */\n+    // \"noLib\": true,                                    /* Disable including any library files, including the default lib.d.ts. */\n+    // \"useDefineForClassFields\": true,                  /* Emit ECMAScript-standard-compliant class fields. */\n+    // \"moduleDetection\": \"auto\",                        /* Control what method is used to detect module-format JS files. */\n+\n+    /* Modules */\n+    \"module\": \"ESNext\",                                /* Specify what module code is generated. */\n+    // \"rootDir\": \"./\",                                  /* Specify the root folder within your source files. */\n+    \"moduleResolution\": \"node\",                     /* Specify how TypeScript looks up a file from a given module specifier. */\n+    // \"baseUrl\": \"./\",                                  /* Specify the base directory to resolve non-relative module names. */\n+    // \"paths\": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */\n+    // \"rootDirs\": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */\n+    // \"typeRoots\": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */\n+    // \"types\": [],                                      /* Specify type package names to be included without being referenced in a source file. */\n+    // \"allowUmdGlobalAccess\": true,                     /* Allow accessing UMD globals from modules. */\n+    // \"moduleSuffixes\": [],                             /* List of file name suffixes to search when resolving a module. */\n+    \"allowImportingTsExtensions\": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */\n+    // \"resolvePackageJsonExports\": true,                /* Use the package.json 'exports' field when resolving package imports. */\n+    // \"resolvePackageJsonImports\": true,                /* Use the package.json 'imports' field when resolving imports. */\n+    // \"customConditions\": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */\n+    // \"resolveJsonModule\": true,                        /* Enable importing .json files. */\n+    // \"allowArbitraryExtensions\": true,                 /* Enable importing files with any extension, provided a declaration file is present. */\n+    // \"noResolve\": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */\n+\n+    /* JavaScript Support */\n+    \"allowJs\": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */\n+    // \"checkJs\": true,                                  /* Enable error reporting in type-checked JavaScript files. */\n+    // \"maxNodeModuleJsDepth\": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */\n+\n+    /* Emit */\n+    // \"declaration\": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */\n+    // \"declarationMap\": true,                           /* Create sourcemaps for d.ts files. */\n+    // \"emitDeclarationOnly\": true,                      /* Only output d.ts files and not JavaScript files. */\n+    \"sourceMap\": true,                                /* Create source map files for emitted JavaScript files. */\n+    // \"inlineSourceMap\": true,                          /* Include sourcemap files inside the emitted JavaScript. */\n+    // \"outFile\": \"./\",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */\n+    // \"outDir\": \"./\",                                   /* Specify an output folder for all emitted files. */\n+    // \"removeComments\": true,                           /* Disable emitting comments. */\n+    // \"noEmit\": true,                                   /* Disable emitting files from a compilation. */\n+    // \"importHelpers\": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */\n+    // \"importsNotUsedAsValues\": \"remove\",               /* Specify emit/checking behavior for imports that are only used for types. */\n+    // \"downlevelIteration\": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */\n+    // \"sourceRoot\": \"\",                                 /* Specify the root path for debuggers to find the reference source code. */\n+    // \"mapRoot\": \"\",                                    /* Specify the location where debugger should locate map files instead of generated locations. */\n+    // \"inlineSources\": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */\n+    // \"emitBOM\": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */\n+    // \"newLine\": \"crlf\",                                /* Set the newline character for emitting files. */\n+    // \"stripInternal\": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */\n+    // \"noEmitHelpers\": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */\n+    // \"noEmitOnError\": true,                            /* Disable emitting files if any type checking errors are reported. */\n+    // \"preserveConstEnums\": true,                       /* Disable erasing 'const enum' declarations in generated code. */\n+    // \"declarationDir\": \"./\",                           /* Specify the output directory for generated declaration files. */\n+    // \"preserveValueImports\": true,                     /* Preserve unused imported values in the JavaScript output that would otherwise be removed. */\n+\n+    /* Interop Constraints */\n+    // \"isolatedModules\": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */\n+    // \"verbatimModuleSyntax\": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */\n+    // \"allowSyntheticDefaultImports\": true,             /* Allow 'import x from y' when a module doesn't have a default export. */\n+    \"esModuleInterop\": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */\n+    // \"preserveSymlinks\": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */\n+    \"forceConsistentCasingInFileNames\": true,            /* Ensure that casing is correct in imports. */\n+\n+    /* Type Checking */\n+    \"strict\": false,                                      /* Enable all strict type-checking options. */\n+    // \"noImplicitAny\": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */\n+    // \"strictNullChecks\": true,                         /* When type checking, take into account 'null' and 'undefined'. */\n+    // \"strictFunctionTypes\": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */\n+    // \"strictBindCallApply\": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */\n+    // \"strictPropertyInitialization\": true,             /* Check for class properties that are declared but not set in the constructor. */\n+    // \"noImplicitThis\": true,                           /* Enable error reporting when 'this' is given the type 'any'. */\n+    // \"useUnknownInCatchVariables\": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */\n+    // \"alwaysStrict\": true,                             /* Ensure 'use strict' is always emitted. */\n+    // \"noUnusedLocals\": true,                           /* Enable error reporting when local variables aren't read. */\n+    // \"noUnusedParameters\": true,                       /* Raise an error when a function parameter isn't read. */\n+    // \"exactOptionalPropertyTypes\": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */\n+    // \"noImplicitReturns\": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */\n+    // \"noFallthroughCasesInSwitch\": true,               /* Enable error reporting for fallthrough cases in switch statements. */\n+    // \"noUncheckedIndexedAccess\": true,                 /* Add 'undefined' to a type when accessed using an index. */\n+    // \"noImplicitOverride\": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */\n+    // \"noPropertyAccessFromIndexSignature\": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */\n+    // \"allowUnusedLabels\": true,                        /* Disable error reporting for unused labels. */\n+    // \"allowUnreachableCode\": true,                     /* Disable error reporting for unreachable code. */\n+\n+    /* Completeness */\n+    // \"skipDefaultLibCheck\": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */\n+    \"skipLibCheck\": true                                 /* Skip type checking all .d.ts files. */\n+  }\n+}"
            },
            {
                "filename": "yarn.lock",
                "additions": 5230,
                "deletions": 892,
                "patch": null
            }
        ]
    },
    {
        "sha": "e45d4f0bb8b8d972c2e9106a7fb43926f2f20e10",
        "author": "victormimo",
        "date": "2024-01-19 19:39:05+00:00",
        "message": "time to move on",
        "files": [
            {
                "filename": "src/scripts/sampleData.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,5 +1,5 @@\n export const issuer = {\n-    legal_name: \"Poet Network Inc.\",\n+    legal_name: \"Banana Peppers Inc.\",\n     formation_date: \"2022-08-23\",\n     country_of_formation: \"US\",\n     country_subdivision_of_formation: \"DE\","
            }
        ]
    },
    {
        "sha": "79ccf81d05a17fdbf56a5a20b8b4fac753bac57b",
        "author": "victormimo",
        "date": "2024-01-19 19:38:53+00:00",
        "message": "passing dbConn instead of declaring it again",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -78,12 +78,12 @@ export const stopEventProcessing = async () => {\n \n export const pollingSleepTime = 1000;\n \n-export const startEventProcessing = async (finalizedOnly: boolean) => {\n+export const startEventProcessing = async (finalizedOnly: boolean, dbConn) => {\n     _keepProcessing = true;\n     _finishedProcessing = false;\n-    const dbConn = await connectDB();\n     while (_keepProcessing) {\n         const issuers = await readAllIssuers();\n+\n         // console.log(`Processing synchronously for ${issuers.length} issuers`);\n         for (const issuer of issuers) {\n             if (issuer.deployed_to) {"
            }
        ]
    },
    {
        "sha": "dff159adf1bb4b2ac98859cbfca3510535cec494",
        "author": "victormimo",
        "date": "2024-01-19 19:38:32+00:00",
        "message": "deleting chain env variable references",
        "files": [
            {
                "filename": "src/app.js",
                "additions": 10,
                "deletions": 18,
                "patch": "@@ -26,13 +26,6 @@ import { getIssuerContract } from \"./utils/caches.ts\";\n const app = express();\n \n const PORT = process.env.PORT;\n-const CHAIN = process.env.CHAIN;\n-\n-// Middlewares\n-const chainMiddleware = (req, res, next) => {\n-    req.chain = CHAIN;\n-    next();\n-};\n \n // Middleware to get or create contract instance\n // the listener is first started on deployment, then here as a backup\n@@ -46,7 +39,7 @@ const contractMiddleware = async (req, res, next) => {\n     const issuer = await readIssuerById(req.body.issuerId);\n     if (!issuer) res.status(400).send(\"issuer not found \");\n \n-    const {contract, provider} = await getIssuerContract(issuer);\n+    const { contract, provider } = await getIssuerContract(issuer);\n     req.contract = contract;\n     req.provider = provider;\n     next();\n@@ -55,9 +48,9 @@ app.use(urlencoded({ limit: \"50mb\", extended: true }));\n app.use(json({ limit: \"50mb\" }));\n app.enable(\"trust proxy\");\n \n-app.use(\"/\", chainMiddleware, mainRoutes);\n-app.use(\"/cap-table\", chainMiddleware, capTableRoutes);\n-app.use(\"/issuer\", chainMiddleware, issuerRoutes);\n+app.use(\"/\", mainRoutes);\n+app.use(\"/cap-table\", capTableRoutes);\n+app.use(\"/issuer\", issuerRoutes);\n app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n // No middleware required since these are only created offchain\n@@ -70,18 +63,18 @@ app.use(\"/historical-transactions\", historicalTransactions);\n // transactions\n app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n \n-export const startServer = async (finalizedOnly = true) => {\n+export const startServer = async (finalizedOnly) => {\n     /*\n     processTo can be \"latest\" or \"finalized\". Latest helps during testing bc we dont have to wait for blocks to finalize\n     */\n \n     // Connect to MongoDB\n-    await connectDB();\n+    const dbConn = await connectDB();\n \n     const server = app.listen(PORT, async () => {\n-        console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n+        console.log(`\ud83d\ude80  Server successfully launched at ${PORT}`);\n         // Asynchronous job to track web3 events in web2\n-        startEventProcessing(finalizedOnly);\n+        startEventProcessing(finalizedOnly, dbConn);\n     });\n \n     return server;\n@@ -92,13 +85,12 @@ export const shutdownServer = async (server) => {\n         console.log(\"Shutting down app server...\");\n         server.close();\n     }\n-    \n+\n     console.log(\"Waiting for event processing to stop...\");\n     await stopEventProcessing();\n \n     if (mongoose.connection?.readyState === mongoose.STATES.connected) {\n         console.log(\"Disconnecting from mongo...\");\n         await mongoose.disconnect();\n     }\n-}\n-\n+};"
            }
        ]
    },
    {
        "sha": "57e16e3e73f2e1a70721546207318c6922623103",
        "author": "victormimo",
        "date": "2024-01-19 19:38:04+00:00",
        "message": "making finalized flag an argument",
        "files": [
            {
                "filename": "src/server.js",
                "additions": 9,
                "deletions": 1,
                "patch": "@@ -1,3 +1,11 @@\n import { startServer } from \"./app.js\";\n \n-startServer();\n+// Function to check if the flag is present\n+const isFlagPresent = (flag) => process.argv.includes(flag);\n+\n+// Setting the default value of the flag to true\n+const finalizedOnly = isFlagPresent(\"--finalized-only\");\n+\n+console.log(\"Finalized Only:\", finalizedOnly);\n+\n+startServer(finalizedOnly);"
            }
        ]
    },
    {
        "sha": "bebc4e988508ced1850d4e084324b686492717bb",
        "author": "victormimo",
        "date": "2024-01-19 19:37:53+00:00",
        "message": "updating .env.example",
        "files": [
            {
                "filename": ".env.example",
                "additions": 0,
                "deletions": 1,
                "patch": "@@ -1,6 +1,5 @@\n # Offchain db connection string for mongodb\n DATABASE_URL=\"mongodb://tap:tap@localhost:27017/mongo?authSource=admin&retryWrites=true&w=majority\"\n-DATABASE_OVERRIDE=\"\"  # use a database other than the default in DATABASE_URL\n DATABASE_REPLSET=\"0\"  # set to \"1\" if using --replSet option in mongo. this allows transactions\n \n # RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)"
            }
        ]
    },
    {
        "sha": "96f7036cb83f65661f93c6bfc63faf92f1bf7edb",
        "author": "victormimo",
        "date": "2024-01-19 19:37:41+00:00",
        "message": "adding finalized flag and removing outdated scripts",
        "files": [
            {
                "filename": "package.json",
                "additions": 1,
                "deletions": 9,
                "patch": "@@ -7,7 +7,7 @@\n     \"description\": \"Transfer Agent Protocol onchain cap table\",\n     \"type\": \"module\",\n     \"scripts\": {\n-        \"start\": \"npx tsx src/server.js\",\n+        \"prod\": \"npx tsx src/server.js --finalized-only\",\n         \"dev\": \"npx tsx watch src/server.js\",\n         \"eslint\": \"eslint --cache --cache-location=node_modules/.cache/.eslintcache --fix\",\n         \"lint\": \"yarn run eslint . --ext .js,.jsx,.ts,.tsx\",\n@@ -22,14 +22,6 @@\n         \"test-js\": \"jest --testPathPattern src/tests/unit\",\n         \"test-js-integration\": \"jest --testPathPattern src/tests/integration\",\n         \"validate-poet-files\": \"cd ocf && yarn validate-poet-files\",\n-        \"test-onchain-cap-table-local\": \"npx tsx src/chain-operations/capTable.cjs local\",\n-        \"test-onchain-cap-table-optimism-goerli\": \"npx tsx src/chain-operations/capTable.cjs optimism-goerli\",\n-        \"test-onchain-cap-table-factory-local\": \"npx tsx src/chain-operations/capTableFactory.cjs local\",\n-        \"test-onchain-cap-table-factory-optimism-goerli\": \"npx tsx src/chain-operations/capTableFactory.cjs optimism-goerli\",\n-        \"forge-deploy-captable-local\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-optimism-goerli\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-factory-local\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-factory-optimism-goerli\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n         \"export-manifest\": \"cd src/db/samples && zip -r notPoet.zip notPoet && mv notPoet.zip $HOME/Downloads\"\n     },\n     \"dependencies\": {"
            }
        ]
    },
    {
        "sha": "911e34f973c617322718b179137cdedc8fd151a6",
        "author": "kentkolze",
        "date": "2024-01-18 19:02:23+00:00",
        "message": "switch from node to npx tsx for better script running",
        "files": [
            {
                "filename": "package.json",
                "additions": 11,
                "deletions": 10,
                "patch": "@@ -7,24 +7,25 @@\n     \"description\": \"Transfer Agent Protocol onchain cap table\",\n     \"type\": \"module\",\n     \"scripts\": {\n-        \"start\": \"nodemon --loader ts-node/esm src/server.js\",\n+        \"start\": \"npx tsx src/server.js\",\n+        \"dev\": \"npx tsx watch src/server.js\",\n         \"eslint\": \"eslint --cache --cache-location=node_modules/.cache/.eslintcache --fix\",\n         \"lint\": \"yarn run eslint . --ext .js,.jsx,.ts,.tsx\",\n         \"format\": \"prettier '**/*' --ignore-unknown --write\",\n         \"typecheck\": \"concurrently --raw yarn:typecheck:*\",\n         \"typecheck:app\": \"tsc --noEmit\",\n         \"prepare\": \"husky install\",\n-        \"build\": \"cd chain && node ../src/scripts/deployAndLinkLibs.js\",\n+        \"build\": \"cd chain && npx tsx ../src/scripts/deployAndLinkLibs.js\",\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n-        \"deseed\": \"node src/db/scripts/deseed.js\",\n+        \"deseed\": \"npx tsx src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\",\n         \"test-js\": \"jest --testPathPattern src/tests/unit\",\n         \"test-js-integration\": \"jest --testPathPattern src/tests/integration\",\n         \"validate-poet-files\": \"cd ocf && yarn validate-poet-files\",\n-        \"test-onchain-cap-table-local\": \"node src/chain-operations/capTable.cjs local\",\n-        \"test-onchain-cap-table-optimism-goerli\": \"node src/chain-operations/capTable.cjs optimism-goerli\",\n-        \"test-onchain-cap-table-factory-local\": \"node src/chain-operations/capTableFactory.cjs local\",\n-        \"test-onchain-cap-table-factory-optimism-goerli\": \"node src/chain-operations/capTableFactory.cjs optimism-goerli\",\n+        \"test-onchain-cap-table-local\": \"npx tsx src/chain-operations/capTable.cjs local\",\n+        \"test-onchain-cap-table-optimism-goerli\": \"npx tsx src/chain-operations/capTable.cjs optimism-goerli\",\n+        \"test-onchain-cap-table-factory-local\": \"npx tsx src/chain-operations/capTableFactory.cjs local\",\n+        \"test-onchain-cap-table-factory-optimism-goerli\": \"npx tsx src/chain-operations/capTableFactory.cjs optimism-goerli\",\n         \"forge-deploy-captable-local\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n         \"forge-deploy-captable-optimism-goerli\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n         \"forge-deploy-captable-factory-local\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n@@ -41,7 +42,9 @@\n         \"ethers\": \"^6.7.1\",\n         \"express\": \"^4.18.2\",\n         \"mongoose\": \"^7.4.2\",\n+        \"npx\": \"^10.2.2\",\n         \"solc\": \"^0.8.20\",\n+        \"tsx\": \"^4.7.0\",\n         \"xstate\": \"^4.38.2\",\n         \"yauzl\": \"^2.10.0\"\n     },\n@@ -52,7 +55,7 @@\n     ],\n     \"devDependencies\": {\n         \"@types/jest\": \"^29.5.11\",\n-        \"@types/node\": \"^20.3.2\",\n+        \"@types/node\": \"^20.11.5\",\n         \"@types/uuid\": \"^9.0.2\",\n         \"eslint\": \"^8.21.0\",\n         \"eslint-config-next\": \"^13.1.6\",\n@@ -61,11 +64,9 @@\n         \"husky\": \"^8.0.1\",\n         \"jest\": \"^29.7.0\",\n         \"lint-staged\": \"^13.0.3\",\n-        \"nodemon\": \"^3.0.1\",\n         \"prettier\": \"^2.7.1\",\n         \"solhint\": \"^3.4.1\",\n         \"ts-jest\": \"^29.1.1\",\n-        \"ts-node\": \"^10.9.2\",\n         \"typescript\": \"^5.3.3\",\n         \"uuid\": \"^9.0.0\"\n     }"
            },
            {
                "filename": "src/db/scripts/deseed.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,4 +1,4 @@\n-import { deseedDatabase } from \"../../tests/integration/utils.js\";\n+import { deseedDatabase } from \"../../tests/integration/utils.ts\";\n \n const runDeseed = async () => {\n     try {"
            },
            {
                "filename": "yarn.lock",
                "additions": 2748,
                "deletions": 194,
                "patch": null
            }
        ]
    },
    {
        "sha": "de60bf69ddb5061088e9f5ac0fab9f1d56625e59",
        "author": "kentkolze",
        "date": "2024-01-18 18:11:36+00:00",
        "message": "fix readme after rebase",
        "files": [
            {
                "filename": "README.md",
                "additions": 2,
                "deletions": 7,
                "patch": "@@ -2,9 +2,9 @@\n \n Developed by:\n \n--   # [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n+# [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n+\n -   [Poet](https://poet.network/)\n-    > > > > > > > ff7779fd6fb670514902163f312e88db4a8aa9be\n -   [Plural Energy](https://www.pluralenergy.co/)\n -   [Fairmint](https://www.fairmint.com/)\n \n@@ -39,12 +39,7 @@ We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo)\n ## Official links\n \n -   [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n--   # [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n--   [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n--   # [Slack](https://transferagentprotocol.slack.com/) - invite only for now. > > > > > > > ec8edb9 (correct the issues with polling for blockchain events and get the integration test working properly)\n--   [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n -   [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n-    > > > > > > > ff7779fd6fb670514902163f312e88db4a8aa9be\n \n ## Getting started\n "
            }
        ]
    },
    {
        "sha": "fd5e40355e832ee3aabd8fd86aecf62239bbe5b8",
        "author": "kentkolze",
        "date": "2024-01-18 17:59:45+00:00",
        "message": "cleanup after rebase",
        "files": [
            {
                "filename": ".env.example",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,5 +1,7 @@\n # Offchain db connection string for mongodb\n DATABASE_URL=\"mongodb://tap:tap@localhost:27017/mongo?authSource=admin&retryWrites=true&w=majority\"\n+DATABASE_OVERRIDE=\"\"  # use a database other than the default in DATABASE_URL\n+DATABASE_REPLSET=\"0\"  # set to \"1\" if using --replSet option in mongo. this allows transactions\n \n # RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n RPC_URL=http://127.0.0.1:8545\n@@ -18,5 +20,3 @@ ETHERSCAN_L1_API_KEY=UPDATE_ME\n # Server port\n PORT=8080\n \n-DATABASE_OVERRIDE=\"\"  # use a database other than the default in DATABASE_URL\n-DATABASE_REPLSET=\"0\"  # set to \"1\" if using --replSet option in mongo. this allows transactions"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 1,
                "deletions": 3,
                "patch": "@@ -6,7 +6,7 @@ import getProvider from \"./getProvider.js\";\n \n config();\n \n-async function getContractInstance(address) {\n+export const getContractInstance = (address) => {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n \n     const provider = getProvider();\n@@ -17,5 +17,3 @@ async function getContractInstance(address) {\n \n     return { contract, provider, libraries };\n }\n-\n-export default getContractInstance;"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 0,
                "deletions": 1,
                "patch": "@@ -54,7 +54,6 @@ issuer.post(\"/create\", async (req, res) => {\n         const issuerIdBytes16 = convertUUIDToBytes16(incomingIssuerToValidate.id);\n         console.log(\"\ud83d\udcbe | Issuer id in bytes16 \", issuerIdBytes16);\n         const { address, deployHash } = await deployCapTable(\n-            chain,\n             issuerIdBytes16,\n             incomingIssuerToValidate.legal_name,\n             incomingIssuerToValidate.initial_shares_authorized"
            },
            {
                "filename": "src/utils/caches.ts",
                "additions": 1,
                "deletions": 3,
                "patch": "@@ -1,7 +1,5 @@\n import { getContractInstance } from \"../chain-operations/getContractInstances.js\";\n \n-const CHAIN = process.env.CHAIN;\n-\n interface CachePayload {\n     contract: any;\n     provider: any;\n@@ -13,7 +11,7 @@ const contractCache: {[key: string]: CachePayload} = {};\n \n export const getIssuerContract = async (issuer): Promise<CachePayload> => {\n     if (!contractCache[issuer._id]) {\n-        const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to); \n+        const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to); \n         contractCache[issuer._id] = { contract, provider, libraries };\n     }\n     return contractCache[issuer._id];"
            }
        ]
    },
    {
        "sha": "31f1c49c61df34df8c35ff97500e028346cb8e26",
        "author": "victormimo",
        "date": "2024-01-18 16:07:07+00:00",
        "message": "Merge branch 'kkolze/resilient-event-processor' of https://github.com/poet-network/tap-cap-table into kkolze/resilient-event-processor",
        "files": [
            {
                "filename": "README.md",
                "additions": 8,
                "deletions": 5,
                "patch": "@@ -2,7 +2,9 @@\n \n Developed by:\n \n--   [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n+-   # [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n+-   [Poet](https://poet.network/)\n+    > > > > > > > ff7779fd6fb670514902163f312e88db4a8aa9be\n -   [Plural Energy](https://www.pluralenergy.co/)\n -   [Fairmint](https://www.fairmint.com/)\n \n@@ -39,8 +41,10 @@ We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo)\n -   [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n -   # [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n -   [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n+-   # [Slack](https://transferagentprotocol.slack.com/) - invite only for now. > > > > > > > ec8edb9 (correct the issues with polling for blockchain events and get the integration test working properly)\n+-   [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n -   [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n-    > > > > > > > ec8edb9 (correct the issues with polling for blockchain events and get the integration test working properly)\n+    > > > > > > > ff7779fd6fb670514902163f312e88db4a8aa9be\n \n ## Getting started\n \n@@ -187,9 +191,8 @@ Integration test setup from no active processes:\n -   Terminal 1: `docker compose up`\n -   Terminal 2: `anvil`\n -   Terminal 3: `cd chain && forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast`\n-\n-    -   To bootstrap `jest-integration` MongoDB collections: `cd .. && yarn test-js-integration` NOTE: we use `jest-integration`\n-    -   Then using MongoDB compass, create/update the record in `jest-integration.factories` with the implementation_address and factory_address\n+    -   In MongoDB compass, create/update `implementation_address` and `factory_address` in `jest-integration.factories`\n+        -   If the `jest-integration` MongoDB databases dont exist: `cd .. && yarn test-js-integration`\n     -   Run `yarn test-js-integration`!\n \n ## Contributing"
            }
        ]
    },
    {
        "sha": "3175082022c580eea4fbffd4c1b46b8219949a62",
        "author": "kentkolze",
        "date": "2024-01-17 19:46:27+00:00",
        "message": "final touches to get the latest snapshot of the cap table!",
        "files": [
            {
                "filename": "src/chain-operations/transactionListener.js",
                "additions": 0,
                "deletions": 129,
                "patch": "@@ -1,129 +0,0 @@\n-/*\n-DEPRECATED! DO NOT USE\n-TODO: delete \n-*/\n-\n-\n-import { AbiCoder } from \"ethers\";\n-import { verifyIssuerAndSeed } from \"./seed.js\";\n-import {\n-    IssuerAuthorizedSharesAdjustment,\n-    StockAcceptance,\n-    StockCancellation,\n-    StockClassAuthorizedSharesAdjustment,\n-    StockIssuance,\n-    StockReissuance,\n-    StockRepurchase,\n-    StockRetraction,\n-    StockTransfer,\n-} from \"./structs.js\";\n-import {\n-    handleIssuerAuthorizedSharesAdjusted,\n-    handleStakeholder,\n-    handleStockAcceptance,\n-    handleStockCancellation,\n-    handleStockClass,\n-    handleStockClassAuthorizedSharesAdjusted,\n-    handleStockIssuance,\n-    handleStockReissuance,\n-    handleStockRepurchase,\n-    handleStockRetraction,\n-    handleStockTransfer,\n-} from \"./transactionHandlers.js\";\n-\n-const abiCoder = new AbiCoder();\n-const eventQueue = [];\n-let issuerEventFired = false;\n-\n-const txMapper = {\n-    0: [\"INVALID\"],\n-    1: [\"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\", IssuerAuthorizedSharesAdjustment],\n-    2: [\"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\", StockClassAuthorizedSharesAdjustment],\n-    3: [\"STOCK_ACCEPTANCE\", StockAcceptance],\n-    4: [\"STOCK_CANCELLATION\", StockCancellation],\n-    5: [\"STOCK_ISSUANCE\", StockIssuance],\n-    6: [\"STOCK_REISSUANCE\", StockReissuance],\n-    7: [\"STOCK_REPURCHASE\", StockRepurchase],\n-    8: [\"STOCK_RETRACTION\", StockRetraction],\n-    9: [\"STOCK_TRANSFER\", StockTransfer],\n-};\n-\n-async function startOnchainListeners(contract, provider, issuerId, libraries) {\n-    console.log(\"\ud83c\udf10 | Initiating on-chain event listeners for issuer\", issuerId, \"at address\", contract.target);\n-\n-    libraries.txHelper.on(\"TxCreated\", async (_, txTypeIdx, txData, event) => {\n-        const [type, structType] = txMapper[txTypeIdx];\n-        const decodedData = abiCoder.decode([structType], txData);\n-        const { timestamp } = await provider.getBlock(event.blockNumber);\n-        eventQueue.push({ type, data: decodedData[0], issuerId, timestamp });\n-    });\n-\n-    contract.on(\"StakeholderCreated\", async (id, _) => {\n-        eventQueue.push({ type: \"STAKEHOLDER_CREATED\", data: id });\n-    });\n-\n-    contract.on(\"StockClassCreated\", async (id, _) => {\n-        eventQueue.push({ type: \"STOCK_CLASS_CREATED\", data: id });\n-    });\n-\n-    const issuerCreatedFilter = contract.filters.IssuerCreated;\n-    const issuerEvents = await contract.queryFilter(issuerCreatedFilter);\n-\n-    if (issuerEvents.length > 0 && !issuerEventFired) {\n-        const id = issuerEvents[0].args[0];\n-        console.log(\"IssuerCreated Event Emitted!\", id);\n-\n-        await verifyIssuerAndSeed(contract, id);\n-        issuerEventFired = true;\n-    }\n-\n-    setInterval(processEventQueue, 5000); // Process every 5 seconds\n-}\n-\n-async function processEventQueue() {\n-    const sortedEventQueue = eventQueue.sort((a, b) => a.timestamp - b.timestamp);\n-    while (sortedEventQueue.length > 0) {\n-        const event = eventQueue[0];\n-        switch (event.type) {\n-            case \"STAKEHOLDER_CREATED\":\n-                await handleStakeholder(event.data);\n-                break;\n-            case \"STOCK_CLASS_CREATED\":\n-                await handleStockClass(event.data);\n-                break;\n-            case \"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\":\n-                await handleIssuerAuthorizedSharesAdjusted(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\":\n-                await handleStockClassAuthorizedSharesAdjusted(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_ACCEPTANCE\":\n-                await handleStockAcceptance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_CANCELLATION\":\n-                await handleStockCancellation(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_ISSUANCE\":\n-                await handleStockIssuance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_REISSUANCE\":\n-                await handleStockReissuance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_REPURCHASE\":\n-                await handleStockRepurchase(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_RETRACTION\":\n-                await handleStockRetraction(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_TRANSFER\":\n-                await handleStockTransfer(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"INVALID\":\n-                throw new Error(\"Invalid transaction type\");\n-                break;\n-        }\n-        sortedEventQueue.shift();\n-    }\n-}\n-\n-// export default startOnchainListeners;"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 62,
                "deletions": 27,
                "patch": "@@ -4,7 +4,8 @@ import Stakeholder from \"../db/objects/Stakeholder\";\n import StockClass from \"../db/objects/StockClass\";\n import { StockIssuance } from \"../db/objects/transactions/issuance\";\n import { getIssuerContract } from \"../utils/caches\";\n-import { convertBytes16ToUUID, convertUUIDToBytes16 } from \"../utils/convertUUID\";\n+import { decimalScaleValue } from \"../utils/convertToFixedPointDecimals\";\n+import { convertUUIDToBytes16 } from \"../utils/convertUUID\";\n \n export const capTable = Router();\n \n@@ -13,40 +14,74 @@ capTable.get(\"/\", async (req, res) => {\n });\n \n capTable.get(\"/latest\", async (req, res) => {\n+    /* \n+    TODO: handle this in the polling process? or maybe just cache it once in a while?\n+     It will get slow once we have 50+ stakeholders\n+    */\n     const issuerId = req.query.issuerId;\n     try {\n         const stakeholders = await Stakeholder.find({issuer: issuerId});\n-        const issuances = await StockIssuance.find({issuer: issuerId});\n         const stockClasses = await StockClass.find({issuer: issuerId});\n         const issuer = await Issuer.findById(issuerId);\n-        const {contract} = await getIssuerContract(issuer);\n-        \n-        // TODO: switch to array of promises for speed\n+        // Grouping by stakeholder_id and stock_class_id, grab the records with the largest createdAt time\n+        const issuances = await StockIssuance.aggregate([\n+            {\n+                $group: {\n+                    _id: {\n+                        stakeholder_id: \"$stakeholder_id\",\n+                        stock_class_id: \"$stock_class_id\"\n+                    },\n+                    maxDate: { $max: \"$createdAt\" }\n+                }\n+            },\n+            {\n+                $lookup: {\n+                    from: \"stockissuances\",\n+                    let: { stakeholder_id: \"$_id.stakeholder_id\", stock_class_id: \"$_id.stock_class_id\", maxDate: \"$maxDate\" },\n+                    pipeline: [\n+                        {\n+                            $match: {\n+                                $expr: {\n+                                    $and: [\n+                                        { $eq: [\"$stakeholder_id\", \"$$stakeholder_id\"] },\n+                                        { $eq: [\"$stock_class_id\", \"$$stock_class_id\"] },\n+                                        { $eq: [\"$createdAt\", \"$$maxDate\"] }\n+                                    ]\n+                                }\n+                            }\n+                        }\n+                    ],\n+                    as: \"issuanceData\"\n+                }\n+            },\n+            { $unwind: \"$issuanceData\" },\n+            { $replaceRoot: { newRoot: \"$issuanceData\" } }\n+        ]);\n+      \n+        // We need to hit web3 to see which are actually valid\n+        const { contract } = await getIssuerContract(issuer);\n         let holdings = [];\n-        for (const stakeholder of stakeholders) {\n-            for (const issuance of issuances) {\n-                const issuanceId = issuance._id;\n-                const stakeholderId = stakeholder._id;\n-                const stakeHolderIdBytes16 = convertUUIDToBytes16(stakeholderId);\n-                const secIdBytes16 = convertUUIDToBytes16(issuance.security_id);\n-                const [stockClassIdBytes16, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n-                    stakeHolderIdBytes16,\n-                    secIdBytes16,\n-                );\n-                const stockClassId = convertBytes16ToUUID(stockClassIdBytes16);\n-                holdings.push({\n-                    stockClassId,\n-                    quantity: Number(quantity),\n-                    sharePrice: Number(sharePrice),\n-                    timestamp: Number(timestamp),\n-                    stakeholderId,\n-                    issuanceId,\n-                });\n+        const stakeholderMap = Object.fromEntries(stakeholders.map((x) => { return [x._id, x]; }));\n+        const stockClassMap = Object.fromEntries(stockClasses.map((x) => { return [x._id, x]; }));\n+        for (const issuance of issuances) {\n+            const { stakeholder_id, security_id, stock_class_id } = issuance;\n+            const [_, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+                convertUUIDToBytes16(stakeholder_id),\n+                convertUUIDToBytes16(security_id),\n+            );\n+            if (quantity == 0) {\n+                continue;\n             }\n+            holdings.push({\n+                issuance,\n+                stockClass: stockClassMap[stock_class_id],\n+                stakeholder: stakeholderMap[stakeholder_id],\n+                quantity: Number(quantity) / decimalScaleValue,\n+                sharePrice: Number(sharePrice) / decimalScaleValue,\n+                timestamp: Number(timestamp) * 1000,\n+            });\n         }\n-\n-        // TODO: munging on server side\n-        res.send({ stakeholders, holdings, stockClasses, issuances });\n+        res.send({ holdings, stockClasses, issuer });\n     } catch (error) {\n         console.error(`error: ${error}`);\n         res.status(500).send(`${error}`);"
            },
            {
                "filename": "src/scripts/sampleData.js",
                "additions": 18,
                "deletions": 0,
                "patch": "@@ -174,6 +174,24 @@ export const stakeholder2 = (issuerId) => {\n         },\n     };\n };\n+\n+export const stakeholder3 = (issuerId) => {\n+    return {\n+        issuerId,\n+        data: {\n+            name: {\n+                legal_name: \"Kent Kolze\",\n+                first_name: \"Kent\",\n+                last_name: \"Kolze\",\n+            },\n+            issuer_assigned_id: \"\",\n+            stakeholder_type: \"INDIVIDUAL\",\n+            current_relationship: \"EMPLOYEE\",\n+            comments: [],\n+        },\n+    };\n+};\n+\n export const stockClass = (issuerId) => {\n     return {\n         issuerId,"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 27,
                "deletions": 10,
                "patch": "@@ -2,13 +2,13 @@ import axios from \"axios\";\n import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n import Factory from \"../../db/objects/Factory\";\n import { web3WaitTime } from \"../../db/operations/update\";\n-import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n+import { issuer as exampleIssuer, stakeholder1, stakeholder2, stakeholder3, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n import sleep from \"../../utils/sleep\";\n import { SERVER_BASE, runLocalServer, shutdownLocalServer } from \"./utils\";\n \n \n // Pro-tip: set this to iterate faster in dev after `seedExampleData` finishes\n-const HARDCODED_ISSUER_ID = \"4e25e7b9-9718-4c95-9f74-57a729b9cfb2\";\n+const HARDCODED_ISSUER_ID = null;\n \n beforeAll(async () => {\n     await runLocalServer(!HARDCODED_ISSUER_ID);\n@@ -48,6 +48,11 @@ const seedExampleData = async () => {\n     console.log(\"\u2705 | stakeholder2Response\", s2Id, stakeholder2Response.data);\n     await allowPropagate();\n     \n+    const stakeholder3Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder3(issuerId));\n+    const s3Id = stakeholder3Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder3Response\", s3Id, stakeholder3Response.data);\n+    await allowPropagate();\n+    \n     const stockClassResponse = await axios.post(`${SERVER_BASE}/stock-class/create`, stockClass(issuerId));\n     const stockClassId = stockClassResponse.data.stockClass._id;\n     console.log(\"\u2705 | stockClassResponse\", stockClassId, stockClassResponse.data);\n@@ -61,7 +66,7 @@ const seedExampleData = async () => {\n     console.log(\"\u2705 | stockIssuanceResponse\", issuance);\n     await allowPropagate();\n     \n-    // TODO: Victor going to finalize these?\n+    // TODO: Victor acceptance of issuance?\n     // const { security_id } = issuance;\n     // const stockIssuanceAcceptanceResp = await axios.post(\n     //     `${SERVER_BASE}/transactions/accept/stock`,\n@@ -70,20 +75,29 @@ const seedExampleData = async () => {\n     // console.log(\"\u2705 | Stock issuance acceptance response\", stockIssuanceAcceptanceResp.data);\n     // await allowPropagate();\n     \n-    const stockTransferResponse = await axios.post(\n+    const stockTransfer1Response = await axios.post(\n         `${SERVER_BASE}/transactions/transfer/stock`,\n         stockTransfer(issuerId, \"200\", s1Id, s2Id, stockClassId, \"4.20\")\n     );\n-    console.log(\"\u2705 | stockTransferResponse\", stockTransferResponse.data);\n+    console.log(\"\u2705 | stockTransfer1Response\", stockTransfer1Response.data);\n     await allowPropagate();\n     \n-    // TODO: Victor going to finalize these?\n+    // TODO: Victor acceptance of transfer1?\n     // const stockTransferAcceptanceResp = await axios.post(\n     //     `${SERVER_BASE}/transactions/accept/stock`,\n     //     stockAccept(issuerId, s2Id, stockClassId, security_id, [\"Accepted\"])\n     // );\n     // console.log(\"\u2705 | Stock transfer acceptance response\", stockTransferAcceptanceResp.data);\n     // await allowPropagate();\n+    \n+    const stockTransfer2Response = await axios.post(\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n+        stockTransfer(issuerId, \"300\", s1Id, s3Id, stockClassId, \"10.66\")\n+    );\n+    console.log(\"\u2705 | stockTransfer2Response\", stockTransfer2Response.data);\n+    await allowPropagate();\n+\n+    // TODO: acceptance of transfer2?\n \n     // Allow time for poller process to catch up\n     await sleep(pollingSleepTime + web3WaitTime + 2000);\n@@ -92,10 +106,13 @@ const seedExampleData = async () => {\n }\n \n const checkRecs = async (issuerId) => {\n-    // TODO: aggregate docs across activePositions to \n-    const { data: capTable } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n-    console.log(\"cap Table Latest: \", capTable);\n-\n+    const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n+    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice, name: stakeholder.name.legal_name}; });\n+    portions.sort((a, b) => b.quantity - a.quantity);\n+    expect(portions).toStrictEqual([\n+        {quantity: 300, sharePrice: 10.66, name: \"Kent Kolze\"},\n+        {quantity: 200, sharePrice: 4.2, name: \"Victor Mimo\"},\n+    ]);\n }\n \n test('end to end with event processing', async () => {"
            },
            {
                "filename": "src/utils/convertToFixedPointDecimals.js",
                "additions": 4,
                "deletions": 2,
                "patch": "@@ -1,16 +1,18 @@\n import { toBigInt } from \"ethers\";\n \n+export const decimalScaleValue = 1e10;\n+\n // Convert a price to a BigInt\n function toScaledBigNumber(price) {\n-    return toBigInt(Math.round(price * 1e10).toString());\n+    return toBigInt(Math.round(price * decimalScaleValue).toString());\n }\n \n // TODO: might not be refactored correctly from ethers v5 to v6\n // Convert a BigInt back to a decimal price\n function toDecimal(scaledPriceBigInt) {\n     if (typeof scaledPriceBigInt === \"bigint\") {\n         const numberString = scaledPriceBigInt.toString();\n-        return parseFloat(numberString / 1e10).toString();\n+        return parseFloat(numberString / decimalScaleValue).toString();\n     } else {\n         return scaledPriceBigInt;\n     }"
            }
        ]
    },
    {
        "sha": "1ca9dfbedf3c17ae75408288910bf057043a1d35",
        "author": "kentkolze",
        "date": "2024-01-17 16:17:01+00:00",
        "message": "refactor integration server utils to separate file so it can be used in other tests\nadd web3 position getters",
        "files": [
            {
                "filename": "chain/src/CapTable.sol",
                "additions": 6,
                "deletions": 0,
                "patch": "@@ -423,6 +423,12 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n         return stockClasses.length;\n     }\n \n+    /// @inheritdoc ICapTable\n+    function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40) {\n+        ActivePosition storage position = positions.activePositions[stakeholderId][securityId];\n+        return (position.stock_class_id, position.quantity, position.share_price, position.timestamp);\n+    }\n+\n     /* Role Based Access Control */\n     modifier onlyOperator() {\n         /// @notice Admins are also considered Operators"
            },
            {
                "filename": "chain/src/interfaces/ICapTable.sol",
                "additions": 3,
                "deletions": 0,
                "patch": "@@ -74,6 +74,9 @@ interface ICapTable {\n \n     function getTotalActiveSecuritiesCount() external view returns (uint256);\n \n+    // Function to get the timestamp of an active position\n+    function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40);\n+\n     function issueStock(StockIssuanceParams calldata params) external;\n \n     function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external;"
            },
            {
                "filename": "src/db/scripts/deseed.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,4 +1,4 @@\n-import { deseedDatabase } from \"../../tests/deseed\";\n+import { deseedDatabase } from \"../../tests/integration/utils.js\";\n \n const runDeseed = async () => {\n     try {"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 36,
                "deletions": 23,
                "patch": "@@ -4,7 +4,7 @@ import Stakeholder from \"../db/objects/Stakeholder\";\n import StockClass from \"../db/objects/StockClass\";\n import { StockIssuance } from \"../db/objects/transactions/issuance\";\n import { getIssuerContract } from \"../utils/caches\";\n-import { convertUUIDToBytes16 } from \"../utils/convertUUID\";\n+import { convertBytes16ToUUID, convertUUIDToBytes16 } from \"../utils/convertUUID\";\n \n export const capTable = Router();\n \n@@ -14,28 +14,41 @@ capTable.get(\"/\", async (req, res) => {\n \n capTable.get(\"/latest\", async (req, res) => {\n     const issuerId = req.query.issuerId;\n-    const stakeholders = await Stakeholder.find({issuer: issuerId});\n-    const issuances = await StockIssuance.find({issuer: issuerId});\n-    const stockClasses = await StockClass.find({issuer: issuerId});\n-    const issuer = await Issuer.findById(issuerId);\n-    const {contract} = await getIssuerContract(issuer);\n-    \n-    // TODO: switch to many promises at once for speed?\n-    let holdings = [];\n-    for (const stakeholder of stakeholders) {\n-        for (const issuance of issuances) {\n-            const issuanceId = issuance._id;\n-            const stakeholderId = stakeholder._id;\n-            const stakeHolderIdBytes16 = convertUUIDToBytes16(stakeholderId);\n-            const secIdBytes16 = convertUUIDToBytes16(issuance.security_id);\n-            const [stockClassIdBytes16, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n-                stakeHolderIdBytes16,\n-                secIdBytes16,\n-            );\n-            holdings.push({stockClassIdBytes16, quantity, sharePrice, timestamp, stakeholderId, issuanceId})\n+    try {\n+        const stakeholders = await Stakeholder.find({issuer: issuerId});\n+        const issuances = await StockIssuance.find({issuer: issuerId});\n+        const stockClasses = await StockClass.find({issuer: issuerId});\n+        const issuer = await Issuer.findById(issuerId);\n+        const {contract} = await getIssuerContract(issuer);\n+        \n+        // TODO: switch to array of promises for speed\n+        let holdings = [];\n+        for (const stakeholder of stakeholders) {\n+            for (const issuance of issuances) {\n+                const issuanceId = issuance._id;\n+                const stakeholderId = stakeholder._id;\n+                const stakeHolderIdBytes16 = convertUUIDToBytes16(stakeholderId);\n+                const secIdBytes16 = convertUUIDToBytes16(issuance.security_id);\n+                const [stockClassIdBytes16, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+                    stakeHolderIdBytes16,\n+                    secIdBytes16,\n+                );\n+                const stockClassId = convertBytes16ToUUID(stockClassIdBytes16);\n+                holdings.push({\n+                    stockClassId,\n+                    quantity: Number(quantity),\n+                    sharePrice: Number(sharePrice),\n+                    timestamp: Number(timestamp),\n+                    stakeholderId,\n+                    issuanceId,\n+                });\n+            }\n         }\n-    }\n \n-    // TODO: munging on server side?\n-    res.send({ stakeholders, holdings, stockClasses, issuances });\n+        // TODO: munging on server side\n+        res.send({ stakeholders, holdings, stockClasses, issuances });\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n })"
            },
            {
                "filename": "src/tests/deseed.js",
                "additions": 0,
                "deletions": 37,
                "patch": "@@ -1,37 +0,0 @@\n-import { connectDB } from \"../db/config/mongoose.ts\";\n-import HistoricalTransaction from \"../db/objects/HistoricalTransaction.js\";\n-import Issuer from \"../db/objects/Issuer.js\";\n-import Stakeholder from \"../db/objects/Stakeholder.js\";\n-import StockClass from \"../db/objects/StockClass.js\";\n-import StockLegendTemplate from \"../db/objects/StockLegendTemplate.js\";\n-import StockPlan from \"../db/objects/StockPlan.js\";\n-import Valuation from \"../db/objects/Valuation.js\";\n-import VestingTerms from \"../db/objects/VestingTerms.js\";\n-import { typeToModelType } from \"../db/operations/transactions.js\"; // Import the typeToModelType object to delete all transactions\n-\n-const deleteAllTransactions = async () => {\n-    for (const ModelType of Object.values(typeToModelType)) {\n-        await ModelType.deleteMany({});\n-    }\n-};\n-\n-const deleteAll = async () => {\n-    // Delete all documents from the collections\n-    await Issuer.deleteMany({});\n-    await Stakeholder.deleteMany({});\n-    await StockClass.deleteMany({});\n-    await StockLegendTemplate.deleteMany({});\n-    await StockPlan.deleteMany({});\n-    await Valuation.deleteMany({});\n-    await VestingTerms.deleteMany({});\n-    await HistoricalTransaction.deleteMany({});\n-    await deleteAllTransactions(); // Delete all transactions\n-};\n-\n-export const deseedDatabase = async () => {\n-    const connection = await connectDB();\n-    console.log(\"Deseeding from database: \", connection.name);\n-    await deleteAll();\n-    console.log(\"\u2705 Database deseeded successfully\");\n-    await connection.close();\n-};"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 5,
                "deletions": 18,
                "patch": "@@ -1,31 +1,20 @@\n import axios from \"axios\";\n-import { shutdownServer, startServer } from \"../../app\";\n import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n import Factory from \"../../db/objects/Factory\";\n import { web3WaitTime } from \"../../db/operations/update\";\n import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n import sleep from \"../../utils/sleep\";\n-import { deseedDatabase } from \"../deseed\";\n+import { SERVER_BASE, runLocalServer, shutdownLocalServer } from \"./utils\";\n \n \n-const SERVER_BASE = `http://localhost:${process.env.PORT}`;\n-// Pro-tip: use this for faster iteration in dev after seedExampleData is done\n-const HARDCODED_ISSUER_ID = null;\n-let _server = null\n-\n+// Pro-tip: set this to iterate faster in dev after `seedExampleData` finishes\n+const HARDCODED_ISSUER_ID = \"4e25e7b9-9718-4c95-9f74-57a729b9cfb2\";\n \n beforeAll(async () => {\n-    if (!HARDCODED_ISSUER_ID) {\n-        await deseedDatabase();\n-    }\n-    console.log(\"starting server\");\n-    _server = await startServer(false);\n+    await runLocalServer(!HARDCODED_ISSUER_ID);\n }, 10000);\n \n-afterAll(async () => {\n-    console.log(\"shutting down server\");\n-    await shutdownServer(_server);\n-}, 10000);\n+afterAll(shutdownLocalServer, 10000);\n \n const WAIT_TIME = 1000;\n \n@@ -103,8 +92,6 @@ const seedExampleData = async () => {\n }\n \n const checkRecs = async (issuerId) => {\n-    // TODO: there is a timing issue when running with `latest` instead of `finalized` \n-\n     // TODO: aggregate docs across activePositions to \n     const { data: capTable } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n     console.log(\"cap Table Latest: \", capTable);"
            },
            {
                "filename": "src/tests/integration/utils.ts",
                "additions": 58,
                "deletions": 0,
                "patch": "@@ -0,0 +1,58 @@\n+import { shutdownServer, startServer } from \"../../app\";\n+import { connectDB } from \"../../db/config/mongoose\";\n+import HistoricalTransaction from \"../../db/objects/HistoricalTransaction\";\n+import Issuer from \"../../db/objects/Issuer\";\n+import Stakeholder from \"../../db/objects/Stakeholder\";\n+import StockClass from \"../../db/objects/StockClass\";\n+import StockLegendTemplate from \"../../db/objects/StockLegendTemplate\";\n+import StockPlan from \"../../db/objects/StockPlan\";\n+import Valuation from \"../../db/objects/Valuation\";\n+import VestingTerms from \"../../db/objects/VestingTerms\";\n+import { typeToModelType } from \"../../db/operations/transactions\"; // Import the typeToModelType object to delete all transactions\n+\n+export const SERVER_BASE = `http://localhost:${process.env.PORT}`;\n+\n+let _server = null;\n+\n+export const runLocalServer = async (deseed) => {\n+    if (deseed) {\n+        await deseedDatabase();\n+    }\n+    console.log(\"starting server\");\n+    _server = await startServer(false);\n+}\n+\n+\n+export const shutdownLocalServer = async () => {\n+    console.log(\"shutting down server\");\n+    await shutdownServer(_server);\n+}\n+\n+\n+const deleteAllTransactions = async () => {\n+    for (const ModelType of Object.values(typeToModelType)) {\n+        // @ts-ignore\n+        await ModelType.deleteMany({});\n+    }\n+};\n+\n+const deleteAll = async () => {\n+    // Delete all documents from the collections\n+    await Issuer.deleteMany({});\n+    await Stakeholder.deleteMany({});\n+    await StockClass.deleteMany({});\n+    await StockLegendTemplate.deleteMany({});\n+    await StockPlan.deleteMany({});\n+    await Valuation.deleteMany({});\n+    await VestingTerms.deleteMany({});\n+    await HistoricalTransaction.deleteMany({});\n+    await deleteAllTransactions(); // Delete all transactions\n+};\n+\n+export const deseedDatabase = async () => {\n+    const connection = await connectDB();\n+    console.log(\"Deseeding from database: \", connection.name);\n+    await deleteAll();\n+    console.log(\"\u2705 Database deseeded successfully\");\n+    await connection.close();\n+};"
            }
        ]
    },
    {
        "sha": "587c12a502de89803c6aee90835585a958cd5ecf",
        "author": "kentkolze",
        "date": "2024-01-17 15:34:41+00:00",
        "message": "switch to retries instead of arbitrary waiting on missed updates",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 0,
                "deletions": 8,
                "patch": "@@ -77,7 +77,6 @@ export const stopEventProcessing = async () => {\n }\n \n export const pollingSleepTime = 1000;\n-export const web3WaitTime = 5000;\n \n export const startEventProcessing = async (finalizedOnly: boolean) => {\n     _keepProcessing = true;\n@@ -155,13 +154,6 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, final\n     // Process only up to a certain amount\n     [events, endBlock] = trimEvents(events, maxEvents, endBlock);\n \n-    if (!finalizedOnly) {\n-        // kkolze: when running against `latest`, our server routes need to wait \n-        //  for the web3 operations to complete before they write to mongo. therefore \n-        //  we need to wait here to ensure the server routes have written to mongo \n-        await sleep(web3WaitTime);\n-    }\n-\n     await withGlobalTransaction(async () => {\n         await persistEvents(issuerId, events);\n         await updateLastProcessed(issuerId, endBlock);"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 32,
                "deletions": 11,
                "patch": "@@ -1,3 +1,4 @@\n+import sleep from \"../../utils/sleep.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n@@ -17,16 +18,36 @@ import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n import { findByIdAndUpdate } from \"./atomic.ts\";\n \n \n+export const web3WaitTime = 5000;\n+\n+\n+const retryOnMiss = async (updateFunc, numRetries = 5, waitBase = null) => {\n+    /* kkolze: When polling `latest` instead of `finalized` web3 blocks, web3 can get ahead of mongo \n+      For example, see the `issuer.post(\"/create\"` code: the issuer is created in mongo after deployCapTable is called  \n+      We add retries to ensure the server routes have written to mongo  */\n+    let tried = 0;\n+    const waitMultiplier = waitBase || web3WaitTime;\n+    while (tried <= numRetries) {\n+        const res = await updateFunc();\n+        if (res !== null) {\n+            return res;\n+        }\n+        tried++;\n+        await sleep(tried * waitMultiplier, \"Returned null, retrying in \");\n+    }\n+}\n+\n+\n export const updateIssuerById = async (id, updatedData) => {\n     return await findByIdAndUpdate(Issuer, id, updatedData, { new: true });\n };\n \n export const updateStakeholderById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(Stakeholder, id, updatedData, { new: true });\n+    return await retryOnMiss(async () => findByIdAndUpdate(Stakeholder, id, updatedData, { new: true }));\n };\n \n export const updateStockClassById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockClass, id, updatedData, { new: true });\n+    return await retryOnMiss(async () => findByIdAndUpdate(StockClass, id, updatedData, { new: true }));\n };\n \n export const updateStockLegendTemplateById = async (id, updatedData) => {\n@@ -46,37 +67,37 @@ export const updateVestingTermsById = async (id, updatedData) => {\n };\n \n export const upsertStockIssuanceById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockIssuance, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockIssuance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockTransferById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockTransfer, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockTransfer, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockCancellationById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockCancellation, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockCancellation, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockRetractionById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockRetraction, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockRetraction, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockReissuanceById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockReissuance, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockReissuance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockRepurchaseById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockRepurchase, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockRepurchase, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockAcceptanceById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockAcceptance, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockAcceptance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockClassAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockClassAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockClassAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n };"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -1,7 +1,8 @@\n import axios from \"axios\";\n import { shutdownServer, startServer } from \"../../app\";\n-import { pollingSleepTime, web3WaitTime } from \"../../chain-operations/transactionPoller\";\n+import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n import Factory from \"../../db/objects/Factory\";\n+import { web3WaitTime } from \"../../db/operations/update\";\n import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n import sleep from \"../../utils/sleep\";\n import { deseedDatabase } from \"../deseed\";"
            },
            {
                "filename": "src/utils/sleep.js",
                "additions": 4,
                "deletions": 1,
                "patch": "@@ -1,4 +1,7 @@\n-function sleep(ms) {\n+function sleep(ms, logPrefix = null) {\n+    if (logPrefix) {\n+        console.log(`${logPrefix} ${(ms / 1000).toFixed(1)} seconds`);\n+    }\n     return new Promise((resolve) => setTimeout(resolve, ms));\n }\n "
            }
        ]
    },
    {
        "sha": "787dcae586d03e56ceb867b048ca4cc9e85efc3e",
        "author": "kentkolze",
        "date": "2024-01-17 14:40:47+00:00",
        "message": "bug fixes around allowing web3 to catch up when trailing latest block in polling process\nfirst cut of publishing the cap table",
        "files": [
            {
                "filename": ".env.example",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -18,4 +18,5 @@ ETHERSCAN_L1_API_KEY=UPDATE_ME\n # Server port\n PORT=8080\n \n-DATABASE_OVERRIDE=\"\"  # use a database other than the default in DATABASE_URL\n\\ No newline at end of file\n+DATABASE_OVERRIDE=\"\"  # use a database other than the default in DATABASE_URL\n+DATABASE_REPLSET=\"0\"  # set to \"1\" if using --replSet option in mongo. this allows transactions"
            },
            {
                "filename": "README.md",
                "additions": 2,
                "deletions": 3,
                "patch": "@@ -36,8 +36,6 @@ We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo)\n \n ## Official links\n \n-<<<<<<< HEAD\n-\n -   [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n -   # [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n -   [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n@@ -184,11 +182,12 @@ Deploy a cap table to local anvil server through a local web2 server. The chain\n \n `yarn test-js-integration`\n \n-Condensed steps from no active processes running:\n+Integration test setup from no active processes:\n \n -   Terminal 1: `docker compose up`\n -   Terminal 2: `anvil`\n -   Terminal 3: `cd chain && forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast`\n+\n     -   To bootstrap `jest-integration` MongoDB collections: `cd .. && yarn test-js-integration` NOTE: we use `jest-integration`\n     -   Then using MongoDB compass, create/update the record in `jest-integration.factories` with the implementation_address and factory_address\n     -   Run `yarn test-js-integration`!"
            },
            {
                "filename": "src/app.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -7,7 +7,7 @@ import { connectDB } from \"./db/config/mongoose.ts\";\n import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n \n // Routes\n-import { capTable as capTableRoutes } from \"./routes/capTable.js\";\n+import { capTable as capTableRoutes } from \"./routes/capTable.ts\";\n import historicalTransactions from \"./routes/historicalTransactions.js\";\n import mainRoutes from \"./routes/index.js\";\n import issuerRoutes from \"./routes/issuer.js\";\n@@ -70,7 +70,7 @@ app.use(\"/historical-transactions\", historicalTransactions);\n // transactions\n app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n \n-export const startServer = async (processTo = \"finalized\") => {\n+export const startServer = async (finalizedOnly = true) => {\n     /*\n     processTo can be \"latest\" or \"finalized\". Latest helps during testing bc we dont have to wait for blocks to finalize\n     */\n@@ -81,7 +81,7 @@ export const startServer = async (processTo = \"finalized\") => {\n     const server = app.listen(PORT, async () => {\n         console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n         // Asynchronous job to track web3 events in web2\n-        startEventProcessing(processTo);\n+        startEventProcessing(finalizedOnly);\n     });\n \n     return server;"
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 13,
                "deletions": 4,
                "patch": "@@ -77,8 +77,9 @@ export const stopEventProcessing = async () => {\n }\n \n export const pollingSleepTime = 1000;\n+export const web3WaitTime = 5000;\n \n-export const startEventProcessing = async (processTo: \"latest\" | \"finalized\") => {\n+export const startEventProcessing = async (finalizedOnly: boolean) => {\n     _keepProcessing = true;\n     _finishedProcessing = false;\n     const dbConn = await connectDB();\n@@ -88,20 +89,20 @@ export const startEventProcessing = async (processTo: \"latest\" | \"finalized\") =>\n         for (const issuer of issuers) {\n             if (issuer.deployed_to) {\n                 const { contract, provider, libraries } = await getIssuerContract(issuer);\n-                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper, processTo);\n+                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper, finalizedOnly);\n             }\n         }\n         await sleep(pollingSleepTime);\n     }\n     _finishedProcessing = true;\n };\n \n-const processEvents = async (dbConn, contract, provider, issuer, txHelper, processTo, maxBlocks = 1500, maxEvents = 250) => {\n+const processEvents = async (dbConn, contract, provider, issuer, txHelper, finalizedOnly, maxBlocks = 1500, maxEvents = 250) => {\n     /*\n     We process up to `maxEvents` across `maxBlocks` to ensure our transaction sizes dont get too big and bog down our db\n     */\n     let {_id: issuerId, last_processed_block: lastProcessedBlock, tx_hash: deployedTxHash} = issuer;\n-    const {number: latestBlock} = await provider.getBlock(processTo);\n+    const {number: latestBlock} = await provider.getBlock(finalizedOnly ? \"finalized\" : \"latest\");\n     // console.log(\"Processing for issuer\", {issuerId, lastProcessedBlock, deployedTxHash, latestBlock});\n     if (lastProcessedBlock === null) {\n         const receipt = await provider.getTransactionReceipt(deployedTxHash);\n@@ -153,6 +154,14 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, proce\n \n     // Process only up to a certain amount\n     [events, endBlock] = trimEvents(events, maxEvents, endBlock);\n+\n+    if (!finalizedOnly) {\n+        // kkolze: when running against `latest`, our server routes need to wait \n+        //  for the web3 operations to complete before they write to mongo. therefore \n+        //  we need to wait here to ensure the server routes have written to mongo \n+        await sleep(web3WaitTime);\n+    }\n+\n     await withGlobalTransaction(async () => {\n         await persistEvents(issuerId, events);\n         await updateLastProcessed(issuerId, endBlock);"
            },
            {
                "filename": "src/db/operations/atomic.ts",
                "additions": 16,
                "deletions": 3,
                "patch": "@@ -21,10 +21,23 @@ export const clearGlobalSession = () => {\n     _globalSession = null;\n }\n \n+const isReplSet = () => {\n+    if (process.env.DATABASE_REPLSET === \"1\") {\n+        return true;\n+    } \n+    return false;\n+}\n+\n export const withGlobalTransaction = async (func: () => Promise<void>, useConn?: Connection) => {\n+    if (!isReplSet()) {\n+        // Transactions in mongo only work when running with --replSet\n+        //  https://www.mongodb.com/docs/manual/tutorial/convert-standalone-to-replica-set/\n+        return await func();\n+    }\n+\n     // Wrap a user defined `func` in a global transaction\n-    const db = useConn || await connectDB();\n-    await db.transaction(async (session) => {\n+    const dbConn = useConn || await connectDB();\n+    await dbConn.transaction(async (session) => {\n         setGlobalSession(session);\n         try {\n             return await func();\n@@ -37,7 +50,7 @@ export const withGlobalTransaction = async (func: () => Promise<void>, useConn?:\n \n const includeSession = (options?: TQueryOptions) => {\n     let useOptions = options || {};\n-    if (!_globalSession) {\n+    if (_globalSession !== null) {\n         if (useOptions.session) {\n             throw new Error(`options.session is already set!: ${useOptions}`);\n         }"
            },
            {
                "filename": "src/routes/capTable.js",
                "additions": 0,
                "deletions": 7,
                "patch": "@@ -1,7 +0,0 @@\n-import { Router } from \"express\";\n-\n-export const capTable = Router();\n-\n-capTable.get(\"/\", async (req, res) => {\n-    res.send(\"Hello Cap Table!\");\n-});"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 41,
                "deletions": 0,
                "patch": "@@ -0,0 +1,41 @@\n+import { Router } from \"express\";\n+import Issuer from \"../db/objects/Issuer\";\n+import Stakeholder from \"../db/objects/Stakeholder\";\n+import StockClass from \"../db/objects/StockClass\";\n+import { StockIssuance } from \"../db/objects/transactions/issuance\";\n+import { getIssuerContract } from \"../utils/caches\";\n+import { convertUUIDToBytes16 } from \"../utils/convertUUID\";\n+\n+export const capTable = Router();\n+\n+capTable.get(\"/\", async (req, res) => {\n+    res.send(\"Hello Cap Table!\");\n+});\n+\n+capTable.get(\"/latest\", async (req, res) => {\n+    const issuerId = req.query.issuerId;\n+    const stakeholders = await Stakeholder.find({issuer: issuerId});\n+    const issuances = await StockIssuance.find({issuer: issuerId});\n+    const stockClasses = await StockClass.find({issuer: issuerId});\n+    const issuer = await Issuer.findById(issuerId);\n+    const {contract} = await getIssuerContract(issuer);\n+    \n+    // TODO: switch to many promises at once for speed?\n+    let holdings = [];\n+    for (const stakeholder of stakeholders) {\n+        for (const issuance of issuances) {\n+            const issuanceId = issuance._id;\n+            const stakeholderId = stakeholder._id;\n+            const stakeHolderIdBytes16 = convertUUIDToBytes16(stakeholderId);\n+            const secIdBytes16 = convertUUIDToBytes16(issuance.security_id);\n+            const [stockClassIdBytes16, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+                stakeHolderIdBytes16,\n+                secIdBytes16,\n+            );\n+            holdings.push({stockClassIdBytes16, quantity, sharePrice, timestamp, stakeholderId, issuanceId})\n+        }\n+    }\n+\n+    // TODO: munging on server side?\n+    res.send({ stakeholders, holdings, stockClasses, issuances });\n+})"
            },
            {
                "filename": "src/routes/stakeholder.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -10,8 +10,8 @@ import {\n \n import stakeholderSchema from \"../../ocf/schema/objects/Stakeholder.schema.json\" assert { type: \"json\" };\n import { createStakeholder } from \"../db/operations/create.js\";\n-import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n import { readIssuerById } from \"../db/operations/read.js\";\n+import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n const stakeholder = Router();\n "
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 8,
                "deletions": 10,
                "patch": "@@ -1,8 +1,7 @@\n import axios from \"axios\";\n import { shutdownServer, startServer } from \"../../app\";\n-import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n+import { pollingSleepTime, web3WaitTime } from \"../../chain-operations/transactionPoller\";\n import Factory from \"../../db/objects/Factory\";\n-import Issuer from \"../../db/objects/Issuer\";\n import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n import sleep from \"../../utils/sleep\";\n import { deseedDatabase } from \"../deseed\";\n@@ -19,13 +18,13 @@ beforeAll(async () => {\n         await deseedDatabase();\n     }\n     console.log(\"starting server\");\n-    _server = await startServer(\"latest\");\n-});\n+    _server = await startServer(false);\n+}, 10000);\n \n afterAll(async () => {\n     console.log(\"shutting down server\");\n     await shutdownServer(_server);\n-});\n+}, 10000);\n \n const WAIT_TIME = 1000;\n \n@@ -97,18 +96,17 @@ const seedExampleData = async () => {\n     // await allowPropagate();\n \n     // Allow time for poller process to catch up\n-    await sleep(pollingSleepTime + 3000);\n+    await sleep(pollingSleepTime + web3WaitTime + 2000);\n \n     return issuerId;\n }\n \n const checkRecs = async (issuerId) => {\n-    const issuer = Issuer.findById(issuerId);\n+    // TODO: there is a timing issue when running with `latest` instead of `finalized` \n \n     // TODO: aggregate docs across activePositions to \n-    const resp = await axios.get(`${SERVER_BASE}/cap-table/`);\n-    console.log(resp);\n-\n+    const { data: capTable } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n+    console.log(\"cap Table Latest: \", capTable);\n \n }\n "
            },
            {
                "filename": "src/utils/caches.ts",
                "additions": 2,
                "deletions": 6,
                "patch": "@@ -11,14 +11,10 @@ interface CachePayload {\n // Centralized contract manager/cache\n const contractCache: {[key: string]: CachePayload} = {};\n \n-const cacheIssuerContract = async (issuer, payload: CachePayload) => {\n-    contractCache[issuer._id] = payload;\n-}\n-\n-export const getIssuerContract = async (issuer) => {\n+export const getIssuerContract = async (issuer): Promise<CachePayload> => {\n     if (!contractCache[issuer._id]) {\n         const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to); \n-        await cacheIssuerContract(issuer, { contract, provider, libraries });\n+        contractCache[issuer._id] = { contract, provider, libraries };\n     }\n     return contractCache[issuer._id];\n }"
            }
        ]
    },
    {
        "sha": "03cecbaa7af0f379aa3037a24203c215f6406f1f",
        "author": "kentkolze",
        "date": "2024-01-16 22:04:19+00:00",
        "message": "add beginnings of the cap table endpoints",
        "files": [
            {
                "filename": "src/app.js",
                "additions": 10,
                "deletions": 4,
                "patch": "@@ -7,6 +7,7 @@ import { connectDB } from \"./db/config/mongoose.ts\";\n import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n \n // Routes\n+import { capTable as capTableRoutes } from \"./routes/capTable.js\";\n import historicalTransactions from \"./routes/historicalTransactions.js\";\n import mainRoutes from \"./routes/index.js\";\n import issuerRoutes from \"./routes/issuer.js\";\n@@ -55,6 +56,7 @@ app.use(json({ limit: \"50mb\" }));\n app.enable(\"trust proxy\");\n \n app.use(\"/\", chainMiddleware, mainRoutes);\n+app.use(\"/cap-table\", chainMiddleware, capTableRoutes);\n app.use(\"/issuer\", chainMiddleware, issuerRoutes);\n app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n@@ -90,9 +92,13 @@ export const shutdownServer = async (server) => {\n         console.log(\"Shutting down app server...\");\n         server.close();\n     }\n-    console.log(\"Stopping event processing...\");\n-    stopEventProcessing();\n-    console.log(\"Disconnecting from mongo...\");\n-    await mongoose.disconnect();\n+    \n+    console.log(\"Waiting for event processing to stop...\");\n+    await stopEventProcessing();\n+\n+    if (mongoose.connection?.readyState === mongoose.STATES.connected) {\n+        console.log(\"Disconnecting from mongo...\");\n+        await mongoose.disconnect();\n+    }\n }\n "
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 11,
                "deletions": 3,
                "patch": "@@ -67,13 +67,20 @@ export const txFuncs = Object.fromEntries(\n );\n \n let _keepProcessing = true;\n+let _finishedProcessing = false;\n \n-export const stopEventProcessing = () => {\n+export const stopEventProcessing = async () => {\n     _keepProcessing = false;\n+    while (!_finishedProcessing) {\n+        await sleep(50);\n+    }\n }\n \n+export const pollingSleepTime = 1000;\n+\n export const startEventProcessing = async (processTo: \"latest\" | \"finalized\") => {\n-    _keepProcessing = true\n+    _keepProcessing = true;\n+    _finishedProcessing = false;\n     const dbConn = await connectDB();\n     while (_keepProcessing) {\n         const issuers = await readAllIssuers();\n@@ -84,8 +91,9 @@ export const startEventProcessing = async (processTo: \"latest\" | \"finalized\") =>\n                 await processEvents(dbConn, contract, provider, issuer, libraries.txHelper, processTo);\n             }\n         }\n-        await sleep(1 * 1000);\n+        await sleep(pollingSleepTime);\n     }\n+    _finishedProcessing = true;\n };\n \n const processEvents = async (dbConn, contract, provider, issuer, txHelper, processTo, maxBlocks = 1500, maxEvents = 250) => {"
            },
            {
                "filename": "src/routes/capTable.js",
                "additions": 7,
                "deletions": 0,
                "patch": "@@ -0,0 +1,7 @@\n+import { Router } from \"express\";\n+\n+export const capTable = Router();\n+\n+capTable.get(\"/\", async (req, res) => {\n+    res.send(\"Hello Cap Table!\");\n+});"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 25,
                "deletions": 13,
                "patch": "@@ -1,22 +1,29 @@\n import axios from \"axios\";\n import { shutdownServer, startServer } from \"../../app\";\n+import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n import Factory from \"../../db/objects/Factory\";\n import Issuer from \"../../db/objects/Issuer\";\n import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n import sleep from \"../../utils/sleep\";\n import { deseedDatabase } from \"../deseed\";\n \n \n+const SERVER_BASE = `http://localhost:${process.env.PORT}`;\n+// Pro-tip: use this for faster iteration in dev after seedExampleData is done\n+const HARDCODED_ISSUER_ID = null;\n let _server = null\n \n+\n beforeAll(async () => {\n-    await deseedDatabase();\n+    if (!HARDCODED_ISSUER_ID) {\n+        await deseedDatabase();\n+    }\n     console.log(\"starting server\");\n     _server = await startServer(\"latest\");\n });\n \n afterAll(async () => {\n-    console.log(\"shuting down server\");\n+    console.log(\"shutting down server\");\n     await shutdownServer(_server);\n });\n \n@@ -37,28 +44,28 @@ const seedExampleData = async () => {\n         );\n     }\n \n-    const issuerResponse = await axios.post(\"http://localhost:8080/issuer/create\", exampleIssuer);\n+    const issuerResponse = await axios.post(`${SERVER_BASE}/issuer/create`, exampleIssuer);\n     const issuerId = issuerResponse.data.issuer._id;\n     console.log(\"\u2705 | Issuer response \", issuerId, issuerResponse.data);\n     await allowPropagate();\n        \n-    const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerId));\n+    const stakeholder1Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder1(issuerId));\n     const s1Id = stakeholder1Response.data.stakeholder._id;\n     console.log(\"\u2705 | stakeholder1Response\", s1Id, stakeholder1Response.data);\n     await allowPropagate();\n     \n-    const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerId));\n+    const stakeholder2Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder2(issuerId));\n     const s2Id = stakeholder2Response.data.stakeholder._id;\n     console.log(\"\u2705 | stakeholder2Response\", s2Id, stakeholder2Response.data);\n     await allowPropagate();\n     \n-    const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerId));\n+    const stockClassResponse = await axios.post(`${SERVER_BASE}/stock-class/create`, stockClass(issuerId));\n     const stockClassId = stockClassResponse.data.stockClass._id;\n     console.log(\"\u2705 | stockClassResponse\", stockClassId, stockClassResponse.data);\n     await allowPropagate();\n     \n     const stockIssuanceResponse = await axios.post(\n-        \"http://localhost:8080/transactions/issuance/stock\",\n+        `${SERVER_BASE}/transactions/issuance/stock`,\n         stockIssuance(issuerId, s1Id, stockClassId, \"500\", \"1.2\")\n     );\n     const issuance = stockIssuanceResponse.data.stockIssuance;\n@@ -68,40 +75,45 @@ const seedExampleData = async () => {\n     // TODO: Victor going to finalize these?\n     // const { security_id } = issuance;\n     // const stockIssuanceAcceptanceResp = await axios.post(\n-    //     \"http://localhost:8080/transactions/accept/stock\",\n+    //     `${SERVER_BASE}/transactions/accept/stock`,\n     //     stockAccept(issuerId, s1Id, stockClassId, security_id, [\"Accepted\"])\n     // );\n     // console.log(\"\u2705 | Stock issuance acceptance response\", stockIssuanceAcceptanceResp.data);\n     // await allowPropagate();\n     \n     const stockTransferResponse = await axios.post(\n-        \"http://localhost:8080/transactions/transfer/stock\",\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n         stockTransfer(issuerId, \"200\", s1Id, s2Id, stockClassId, \"4.20\")\n     );\n     console.log(\"\u2705 | stockTransferResponse\", stockTransferResponse.data);\n     await allowPropagate();\n     \n     // TODO: Victor going to finalize these?\n     // const stockTransferAcceptanceResp = await axios.post(\n-    //     \"http://localhost:8080/transactions/accept/stock\",\n+    //     `${SERVER_BASE}/transactions/accept/stock`,\n     //     stockAccept(issuerId, s2Id, stockClassId, security_id, [\"Accepted\"])\n     // );\n     // console.log(\"\u2705 | Stock transfer acceptance response\", stockTransferAcceptanceResp.data);\n     // await allowPropagate();\n \n+    // Allow time for poller process to catch up\n+    await sleep(pollingSleepTime + 3000);\n+\n     return issuerId;\n }\n \n const checkRecs = async (issuerId) => {\n     const issuer = Issuer.findById(issuerId);\n \n     // TODO: aggregate docs across activePositions to \n+    const resp = await axios.get(`${SERVER_BASE}/cap-table/`);\n+    console.log(resp);\n+\n+\n }\n \n test('end to end with event processing', async () => {\n-    const issuerId = await seedExampleData();\n-    // Allow time for background process to catch up\n-    await sleep(15000);\n+    const issuerId = HARDCODED_ISSUER_ID || await seedExampleData();\n     await checkRecs(issuerId);\n     \n }, WAIT_TIME * 100);"
            }
        ]
    },
    {
        "sha": "e19d673d6bfeab6df35e4313c65529e2c2de3625",
        "author": "kentkolze",
        "date": "2024-01-16 21:25:05+00:00",
        "message": "correct the issues with polling for blockchain events and get the integration test working properly",
        "files": [
            {
                "filename": "README.md",
                "additions": 46,
                "deletions": 20,
                "patch": "@@ -2,9 +2,9 @@\n \n Developed by:\n \n-- [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n-- [Plural Energy](https://www.pluralenergy.co/)\n-- [Fairmint](https://www.fairmint.com/)\n+-   [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n+-   [Plural Energy](https://www.pluralenergy.co/)\n+-   [Fairmint](https://www.fairmint.com/)\n \n This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety. In development, it's meant to be run in a Docker container with a local MongoDB instance. While in active development, it's meant to be run with [Anvil](https://book.getfoundry.sh/anvil/) and [Forge](https://book.getfoundry.sh/forge/).\n \n@@ -16,28 +16,33 @@ This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap\n \n ## Dependencies\n \n-- [Docker](https://docs.docker.com/get-docker/)\n+-   [Docker](https://docs.docker.com/get-docker/)\n \n-- [Foundry](https://getfoundry.sh/)\n+-   [Foundry](https://getfoundry.sh/)\n \n ```sh\n curl -L https://foundry.paradigm.xyz | bash\n ```\n \n-- [Mongo Compass](https://www.mongodb.com/try/download/compass)\n+-   [Mongo Compass](https://www.mongodb.com/try/download/compass)\n \n-- [Postman App](https://www.postman.com/downloads/)\n+-   [Postman App](https://www.postman.com/downloads/)\n \n-- [Node.js v18.16.0](https://nodejs.org/en/download/)\n+-   [Node.js v18.16.0](https://nodejs.org/en/download/)\n \n-- [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n+-   [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n \n We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo) for the local development database. You can find the [Docker Compose file](./docker-compose.yml) in the root of this repository.\n \n ## Official links\n \n-- [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n-- [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n+<<<<<<< HEAD\n+\n+-   [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n+-   # [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n+-   [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n+-   [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n+    > > > > > > > ec8edb9 (correct the issues with polling for blockchain events and get the integration test working properly)\n \n ## Getting started\n \n@@ -100,9 +105,6 @@ yarn build\n \n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n-To deploy the cap table smart contracts to a testnet, update `RPC_URL` and `CHAIN_ID` in the `.env` file, then run the same `yarn build` command.\n-\n-\n ## Running the cap table server\n \n After the deployment script is completed, start the server with nodemon:\n@@ -156,16 +158,40 @@ We're shipping code fast. If you run into an issue, particularly one that result\n \n Inside of `/chain`:\n \n-- Restart anvil\n-- Run `forge clean`\n-- Move back to the root directory, then run `yarn build`\n+-   Restart anvil\n+-   Run `forge clean`\n+-   Move back to the root directory, then run `yarn build`\n \n After, you can seed and deploy the cap table with either of the above options. If the bug persists, please open an issue with an attached screenshot and steps to reproduce.\n \n-## Testing\n+## Testing Web3\n+\n+Run all smart contracts tests\n+\n+`yarn test`\n+\n+## Testing Web2\n+\n+### Unit tests\n+\n+Run all javascript unit tests with jest\n+\n+`yarn test-js`\n+\n+### Integration tests\n+\n+Deploy a cap table to local anvil server through a local web2 server. The chain event listener is also run to ensure the events are properly mirrored into the mongo database. NOTE: running this deletes your local mongo collections first\n+\n+`yarn test-js-integration`\n+\n+Condensed steps from no active processes running:\n \n-To run tests for the smart contracts, run `yarn test`.\n-This will run all the tests defined in the test suite and output the results to the console.\n+-   Terminal 1: `docker compose up`\n+-   Terminal 2: `anvil`\n+-   Terminal 3: `cd chain && forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast`\n+    -   To bootstrap `jest-integration` MongoDB collections: `cd .. && yarn test-js-integration` NOTE: we use `jest-integration`\n+    -   Then using MongoDB compass, create/update the record in `jest-integration.factories` with the implementation_address and factory_address\n+    -   Run `yarn test-js-integration`!\n \n ## Contributing\n "
            },
            {
                "filename": "jest.config.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -4,7 +4,7 @@\n  */\n \n // This allows us to avoid messing with the state of people's standard dev database\n-process.env['DATABASE_OVERRIDE'] = 'jest';\n+process.env['DATABASE_OVERRIDE'] = 'jest-integration';\n \n /** @type {import('jest').Config} */\n const config = {"
            },
            {
                "filename": "src/app.js",
                "additions": 10,
                "deletions": 5,
                "patch": "@@ -68,26 +68,31 @@ app.use(\"/historical-transactions\", historicalTransactions);\n // transactions\n app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n \n-export const startServer = async () => {\n+export const startServer = async (processTo = \"finalized\") => {\n+    /*\n+    processTo can be \"latest\" or \"finalized\". Latest helps during testing bc we dont have to wait for blocks to finalize\n+    */\n+\n     // Connect to MongoDB\n     await connectDB();\n \n     const server = app.listen(PORT, async () => {\n         console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n         // Asynchronous job to track web3 events in web2\n-        startEventProcessing();\n+        startEventProcessing(processTo);\n     });\n \n     return server;\n };\n \n export const shutdownServer = async (server) => {\n-    console.log(\"Shutting down app server...\");\n-    server.close();\n+    if (server) {\n+        console.log(\"Shutting down app server...\");\n+        server.close();\n+    }\n     console.log(\"Stopping event processing...\");\n     stopEventProcessing();\n     console.log(\"Disconnecting from mongo...\");\n     await mongoose.disconnect();\n-    console.log(\" done!\");\n }\n "
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 83,
                "deletions": 77,
                "patch": "@@ -1,4 +1,4 @@\n-import { AbiCoder } from \"ethers\";\n+import { AbiCoder, EventLog } from \"ethers\";\n import { connectDB } from \"../db/config/mongoose.ts\";\n import { withGlobalTransaction } from \"../db/operations/atomic.ts\";\n import { readAllIssuers } from \"../db/operations/read.js\";\n@@ -33,28 +33,37 @@ import {\n \n const abiCoder = new AbiCoder();\n \n+interface QueuedEvent {\n+    type: string;\n+    timestamp: Date;\n+    data: any;\n+    o: EventLog;\n+}\n+\n const contractFuncs = new Map([\n     [\"StakeholderCreated\", handleStakeholder],\n     [\"StockClassCreated\", handleStockClass],\n ]);\n \n const txMapper = {\n-    0: [\"INVALID\"],\n-    1: [\"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\", IssuerAuthorizedSharesAdjustment, handleIssuerAuthorizedSharesAdjusted],\n-    2: [\"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\", StockClassAuthorizedSharesAdjustment, handleStockClassAuthorizedSharesAdjusted],\n-    3: [\"STOCK_ACCEPTANCE\", StockAcceptance, handleStockAcceptance],\n-    4: [\"STOCK_CANCELLATION\", StockCancellation, handleStockCancellation],\n-    5: [\"STOCK_ISSUANCE\", StockIssuance, handleStockIssuance],\n-    6: [\"STOCK_REISSUANCE\", StockReissuance, handleStockReissuance],\n-    7: [\"STOCK_REPURCHASE\", StockRepurchase, handleStockRepurchase],\n-    8: [\"STOCK_RETRACTION\", StockRetraction, handleStockRetraction],\n-    9: [\"STOCK_TRANSFER\", StockTransfer, handleStockTransfer],\n+    1: [IssuerAuthorizedSharesAdjustment, handleIssuerAuthorizedSharesAdjusted],\n+    2: [StockClassAuthorizedSharesAdjustment, handleStockClassAuthorizedSharesAdjusted],\n+    3: [StockAcceptance, handleStockAcceptance],\n+    4: [StockCancellation, handleStockCancellation],\n+    5: [StockIssuance, handleStockIssuance],\n+    6: [StockReissuance, handleStockReissuance],\n+    7: [StockRepurchase, handleStockRepurchase],\n+    8: [StockRetraction, handleStockRetraction],\n+    9: [StockTransfer, handleStockTransfer],\n };\n-\n-// Map(event.type => handler) derived from the above\n-const txFuncs = new Map(\n+// (idx => type name) derived from txMapper\n+export const txTypes = Object.fromEntries(\n     // @ts-ignore\n-    Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_x, [name, _y, handleFunc]]) => [name, handleFunc])\n+    Object.entries(txMapper).map(([i, [_, f]]) => [i, f.name.replace(\"handle\", \"\")])\n+);\n+// (name => handler) derived from txMapper\n+export const txFuncs = Object.fromEntries(\n+    Object.entries(txMapper).map(([i, [_, f]]) => [txTypes[i], f])\n );\n \n let _keepProcessing = true;\n@@ -63,72 +72,69 @@ export const stopEventProcessing = () => {\n     _keepProcessing = false;\n }\n \n-export const startEventProcessing = async () => {\n+export const startEventProcessing = async (processTo: \"latest\" | \"finalized\") => {\n     _keepProcessing = true\n     const dbConn = await connectDB();\n     while (_keepProcessing) {\n         const issuers = await readAllIssuers();\n-        console.log(`Processing synchronously for ${issuers.length} issuers`);\n+        // console.log(`Processing synchronously for ${issuers.length} issuers`);\n         for (const issuer of issuers) {\n             if (issuer.deployed_to) {\n                 const { contract, provider, libraries } = await getIssuerContract(issuer);\n-                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper);\n+                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper, processTo);\n             }\n         }\n-        await sleep(10 * 1000);\n+        await sleep(1 * 1000);\n     }\n };\n \n-const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBlocks = 1500, maxEvents = 250) => {\n+const processEvents = async (dbConn, contract, provider, issuer, txHelper, processTo, maxBlocks = 1500, maxEvents = 250) => {\n     /*\n     We process up to `maxEvents` across `maxBlocks` to ensure our transaction sizes dont get too big and bog down our db\n     */\n-    let {_id: issuerId, last_processed_block: startBlock, tx_hash: deployedTxHash} = issuer;\n-    console.log(\"Processing for issuer\", issuerId, startBlock, deployedTxHash);\n-    if (startBlock === null) {\n+    let {_id: issuerId, last_processed_block: lastProcessedBlock, tx_hash: deployedTxHash} = issuer;\n+    const {number: latestBlock} = await provider.getBlock(processTo);\n+    // console.log(\"Processing for issuer\", {issuerId, lastProcessedBlock, deployedTxHash, latestBlock});\n+    if (lastProcessedBlock === null) {\n         const receipt = await provider.getTransactionReceipt(deployedTxHash);\n-        const tx = await provider.getTransaction(deployedTxHash);\n-        console.log(\"tx\", tx);\n         if (!receipt) {\n-            console.error(\"Transaction receipt not found\");\n+            console.error(\"Deployment receipt not found\");\n             return;\n         }\n-        startBlock = await bootstrapIssuer(issuerId, receipt.blockNumber, contract, dbConn);\n+        if (receipt.blockNumber > latestBlock) {\n+            console.log(\"Deployment tx not finalized\", {receipt, lastFinalizedBlock: latestBlock});\n+            return;\n+        }\n+        lastProcessedBlock = await issuerDeployed(issuerId, receipt, contract, dbConn);\n     }\n-    const {number: latestBlock} = await provider.getBlock('finalized');\n+    const startBlock = lastProcessedBlock + 1;\n     let endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n+    if (startBlock >= endBlock) {\n+        return;\n+    }\n+    \n+    // console.log(\" processing from\", { startBlock, endBlock });\n+    let events: QueuedEvent[] = [];\n \n-    let events: any[] = [];\n-\n-    // TODO: fix filter\n-    const contractEvents = await contract.queryFilter(\"*\", startBlock, endBlock);\n+    const contractEvents: EventLog[] = await contract.queryFilter(\"*\", startBlock, endBlock);\n     for (const event of contractEvents) {\n-        if (contractFuncs.has(event.type)) {\n-            // TODO: how to deserialize event.data?\n-            console.log(\"contract event: \", event);\n-            events.push(event);\n+        const type = event?.fragment?.name;\n+        if (contractFuncs.has(type)) {\n+            const { timestamp } = await provider.getBlock(event.blockNumber);\n+            events.push({type, timestamp, data: event.args[0], o: event });\n         }\n     }\n \n-    // TODO: fix filter\n-    const txEvents = await txHelper.queryFilter(\"*\", startBlock, endBlock);\n+    const txEvents: EventLog[] = await txHelper.queryFilter(txHelper.filters.TxCreated, startBlock, endBlock);\n     for (const event of txEvents) {\n-        // TODO: the same processing as libraries.txHelper.on     \n-        // TODO:  emit TxCreated(transactions.length, txType, txData);\n-        //  how do we parse the event.data string of each event? \n-        //    https://www.npmjs.com/package/@ethersproject/abstract-provider?activeTab=code (line 102: Log.data is string-type)\n-        console.log(\"txHelper event: \", event);\n         if (event.removed) {\n             continue;\n         }\n-        // TODO: does txTypeIdx even come with the event??? need to test this...\n-        let txTypeIdx;\n-        let txData;\n-        const [type, structType] = txMapper[txTypeIdx];\n+        const [_len, typeIdx, txData] = event.args;\n+        const [structType, _] = txMapper[typeIdx];\n         const decodedData = abiCoder.decode([structType], txData);\n         const { timestamp } = await provider.getBlock(event.blockNumber);\n-        // TODO: I think the below needs a lot of work\n-        events.push({ ...event, type, timestamp, data: decodedData[0] });\n+        events.push({ type: txTypes[typeIdx], timestamp, data: decodedData[0], o: event });\n     }\n \n     // Nothing to process\n@@ -137,62 +143,62 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBl\n         return;\n     }\n \n-    // Process in the correct order\n-    events.sort((a, b) => a.blockNumber - b.blockNumber || a.transactionIndex - b.transactionIndex);\n+    // Process only up to a certain amount\n     [events, endBlock] = trimEvents(events, maxEvents, endBlock);\n-\n     await withGlobalTransaction(async () => {\n         await persistEvents(issuerId, events);\n         await updateLastProcessed(issuerId, endBlock);\n     }, dbConn);\n };\n \n-const bootstrapIssuer = async (issuerId, deployedBlockNumber, contract, dbConn) => {\n-    console.log(\"Bootstrapping issuer\");\n-    // TODO: fix the copy-pasted query\n-    const issuerCreatedFilter = contract.filters.IssuerCreated;\n-    const issuerEvents = await contract.queryFilter(issuerCreatedFilter);\n-    if (issuerEvents.length === 0) {\n+const issuerDeployed = async (issuerId, receipt, contract, dbConn) => {\n+    console.log(\"New issuer was deployed\", {issuerId});\n+    const events = await contract.queryFilter(contract.filters.IssuerCreated);\n+    if (events.length === 0) {\n         throw new Error(`No issuer events found!`);\n     }\n-    const issuerCreatedEventId = issuerEvents[0].args[0];\n-    console.log(\"IssuerCreated Event Emitted!\", issuerCreatedEventId);\n-    const tMinusOne = deployedBlockNumber - 1;\n-    \n+    const issuerCreatedEventId = events[0].args[0];\n+    console.log(\"IssuerCreated event captured!\", {issuerCreatedEventId});\n+    const lastProcessedBlock = receipt.blockNumber - 1;\n     await withGlobalTransaction(async () => {\n         await verifyIssuerAndSeed(contract, issuerCreatedEventId);\n-        await updateLastProcessed(issuerId, tMinusOne);\n+        await updateLastProcessed(issuerId, lastProcessedBlock);\n     }, dbConn);\n-\n-    return tMinusOne;\n+    return lastProcessedBlock;\n };\n \n-const persistEvents = async (issuerId, events) => {\n+const persistEvents = async (issuerId, events: QueuedEvent[]) => {\n     // Persist all the necessary changes for each event gathered in process events\n+    console.log(`${events.length} events to process for issuerId ${issuerId}`);\n     for (const event of events) {\n-        const txHandleFunc = txFuncs.get(event.type);\n-        console.log(\"persistEvent: \", event);\n+        const {type, data, timestamp} = event;\n+        const txHandleFunc = txFuncs[type];\n+        // console.log(\"persistEvent: \", {type, data, timestamp});\n         if (txHandleFunc) {\n             // @ts-ignore\n-            await txHandleFunc(event.data, issuerId, event.timestamp);\n+            await txHandleFunc(data, issuerId, timestamp);\n             continue;\n         }\n-        const contractHandleFunc = contractFuncs.get(event.type);\n+        const contractHandleFunc = contractFuncs.get(type);\n         if (contractHandleFunc) {\n-            await contractHandleFunc(event.data);\n+            await contractHandleFunc(data);\n             continue;\n         }\n-        throw new Error(`Invalid transaction type: \"${event.type}\" for ${event}`);\n+        console.error(\"Invalid transaction type: \", type, event);\n+        throw new Error(`Invalid transaction type: \"${type}\"`);\n     }\n };\n \n-export const trimEvents = (events, maxEvents, endBlock) => {\n+export const trimEvents = (origEvents: QueuedEvent[], maxEvents, endBlock) => {\n+    // Sort for correct execution order\n+    let events = [...origEvents];\n+    events.sort((a, b) => a.o.blockNumber - b.o.blockNumber || a.o.transactionIndex - b.o.transactionIndex || a.o.index - b.o.index);\n     let index = 0;    \n     while (index < maxEvents && index < events.length) {\n         // Include the entire next block\n-        const includeBlock = events[index].blockNumber;\n+        const includeBlock = events[index].o.blockNumber;\n         index++;\n-        while (index < events.length && events[index].blockNumber === includeBlock) {\n+        while (index < events.length && events[index].o.blockNumber === includeBlock) {\n             index++;\n         }\n     }\n@@ -203,10 +209,10 @@ export const trimEvents = (events, maxEvents, endBlock) => {\n     // We processed up to the last events' blockNumber\n     // `index` is *exclusive* when trimming\n     const useEvents = [...events.slice(0, index)];\n-    return [useEvents, useEvents[useEvents.length - 1].blockNumber];\n+    return [useEvents, useEvents[useEvents.length - 1].o.blockNumber];\n };\n \n \n const updateLastProcessed = async (issuerId, lastProcessedBlock) => {\n-    return updateIssuerById(issuerId, {lastProcessedBlock});\n+    return updateIssuerById(issuerId, {last_processed_block: lastProcessedBlock});\n };"
            },
            {
                "filename": "src/db/operations/delete.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -6,6 +6,7 @@ import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n+import { findByIdAndDelete } from \"./atomic.ts\";\n \n // TODO: since we're doing a time series db that's meant to be immutable, is this needed?\n "
            },
            {
                "filename": "src/db/operations/transactions.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,8 +1,8 @@\n import * as Acceptance from \"../objects/transactions/acceptance/index.js\";\n import * as Adjustment from \"../objects/transactions/adjustment/index.js\";\n import * as Cancellation from \"../objects/transactions/cancellation/index.js\";\n-import * as Exercise from \"../objects/transactions/exercise/index.js\";\n import * as Conversion from \"../objects/transactions/conversion/index.js\";\n+import * as Exercise from \"../objects/transactions/exercise/index.js\";\n import * as Issuance from \"../objects/transactions/issuance/index.js\";\n import * as Reissuance from \"../objects/transactions/reissuance/index.js\";\n import * as Release from \"../objects/transactions/release/index.js\";\n@@ -12,6 +12,7 @@ import * as ReturnToPool from \"../objects/transactions/return_to_pool/index.js\";\n import * as Split from \"../objects/transactions/split/index.js\";\n import * as Transfer from \"../objects/transactions/transfer/index.js\";\n import * as Vesting from \"../objects/transactions/vesting/index.js\";\n+import { save } from \"./atomic.ts\";\n \n const typeToModelType = {\n     // Acceptance\n@@ -91,7 +92,7 @@ const addTransactions = async (inputTransactions, issuerId) => {\n         inputTransaction = { ...inputTransaction, issuer: issuerId };\n         const ModelType = typeToModelType[inputTransaction.object_type];\n         if (ModelType) {\n-            const transaction = await new ModelType(inputTransaction).save();\n+            const transaction = await save(new ModelType(inputTransaction));\n             console.log(`${inputTransaction.object_type} transaction added. Details:`, JSON.stringify(transaction, null, 2));\n         } else {\n             console.log(`Unknown object type for transaction:`, JSON.stringify(inputTransaction, null, 2));"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -14,6 +14,7 @@ import StockReissuance from \"../objects/transactions/reissuance/StockReissuance.\n import StockRepurchase from \"../objects/transactions/repurchase/StockRepurchase.js\";\n import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n+import { findByIdAndUpdate } from \"./atomic.ts\";\n \n \n export const updateIssuerById = async (id, updatedData) => {"
            },
            {
                "filename": "src/db/scripts/deseed.js",
                "additions": 4,
                "deletions": 40,
                "patch": "@@ -1,47 +1,11 @@\n-import mongoose from \"mongoose\";\n-import { connectDB } from \"../config/mongoose.ts\";\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n-import Issuer from \"../objects/Issuer.js\";\n-import Stakeholder from \"../objects/Stakeholder.js\";\n-import StockClass from \"../objects/StockClass.js\";\n-import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n-import StockPlan from \"../objects/StockPlan.js\";\n-import Valuation from \"../objects/Valuation.js\";\n-import VestingTerms from \"../objects/VestingTerms.js\";\n-import { typeToModelType } from \"../operations/transactions.js\"; // Import the typeToModelType object to delete all transactions\n+import { deseedDatabase } from \"../../tests/deseed\";\n \n-const deleteAllTransactions = async () => {\n-    for (const ModelType of Object.values(typeToModelType)) {\n-        await ModelType.deleteMany({});\n-    }\n-};\n-\n-const deleteAll = async () => {\n-    // Delete all documents from the collections\n-    await Issuer.deleteMany({});\n-    await Stakeholder.deleteMany({});\n-    await StockClass.deleteMany({});\n-    await StockLegendTemplate.deleteMany({});\n-    await StockPlan.deleteMany({});\n-    await Valuation.deleteMany({});\n-    await VestingTerms.deleteMany({});\n-    await HistoricalTransaction.deleteMany({});\n-    await deleteAllTransactions(); // Delete all transactions\n-};\n-\n-const deseedDatabase = async () => {\n+const runDeseed = async () => {\n     try {\n-        connectDB();\n-\n-        await deleteAll();\n-\n-        console.log(\"\u2705 Database deseeded successfully\");\n-\n-        // Close the database connection\n-        await mongoose.connection.close();\n+        await deseedDatabase();\n     } catch (err) {\n         console.log(\"\u274c Error deseeding database:\", err);\n     }\n };\n \n-deseedDatabase();\n+runDeseed();"
            },
            {
                "filename": "src/scripts/testAcceptance.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,5 +1,5 @@\n import axios from \"axios\";\n-import { connectDB } from \"../config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockAccept } from \"./sampleData.js\";\n "
            },
            {
                "filename": "src/tests/deseed.js",
                "additions": 37,
                "deletions": 0,
                "patch": "@@ -0,0 +1,37 @@\n+import { connectDB } from \"../db/config/mongoose.ts\";\n+import HistoricalTransaction from \"../db/objects/HistoricalTransaction.js\";\n+import Issuer from \"../db/objects/Issuer.js\";\n+import Stakeholder from \"../db/objects/Stakeholder.js\";\n+import StockClass from \"../db/objects/StockClass.js\";\n+import StockLegendTemplate from \"../db/objects/StockLegendTemplate.js\";\n+import StockPlan from \"../db/objects/StockPlan.js\";\n+import Valuation from \"../db/objects/Valuation.js\";\n+import VestingTerms from \"../db/objects/VestingTerms.js\";\n+import { typeToModelType } from \"../db/operations/transactions.js\"; // Import the typeToModelType object to delete all transactions\n+\n+const deleteAllTransactions = async () => {\n+    for (const ModelType of Object.values(typeToModelType)) {\n+        await ModelType.deleteMany({});\n+    }\n+};\n+\n+const deleteAll = async () => {\n+    // Delete all documents from the collections\n+    await Issuer.deleteMany({});\n+    await Stakeholder.deleteMany({});\n+    await StockClass.deleteMany({});\n+    await StockLegendTemplate.deleteMany({});\n+    await StockPlan.deleteMany({});\n+    await Valuation.deleteMany({});\n+    await VestingTerms.deleteMany({});\n+    await HistoricalTransaction.deleteMany({});\n+    await deleteAllTransactions(); // Delete all transactions\n+};\n+\n+export const deseedDatabase = async () => {\n+    const connection = await connectDB();\n+    console.log(\"Deseeding from database: \", connection.name);\n+    await deleteAll();\n+    console.log(\"\u2705 Database deseeded successfully\");\n+    await connection.close();\n+};"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 90,
                "deletions": 17,
                "patch": "@@ -1,34 +1,107 @@\n import axios from \"axios\";\n import { shutdownServer, startServer } from \"../../app\";\n-import { connectDB } from \"../../db/config/mongoose\";\n-import { issuer } from \"../../scripts/sampleData\";\n+import Factory from \"../../db/objects/Factory\";\n+import Issuer from \"../../db/objects/Issuer\";\n+import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n+import sleep from \"../../utils/sleep\";\n+import { deseedDatabase } from \"../deseed\";\n \n \n let _server = null\n \n-const deleteAllCollections = async () => {\n-    const dbConn = await connectDB();\n-    console.log(\"Dropping mongo database: \", dbConn.name);\n-    await dbConn.dropDatabase();\n-}\n-\n beforeAll(async () => {\n-    await deleteAllCollections();\n+    await deseedDatabase();\n     console.log(\"starting server\");\n-    _server = await startServer();\n+    _server = await startServer(\"latest\");\n });\n \n afterAll(async () => {\n     console.log(\"shuting down server\");\n     await shutdownServer(_server);\n });\n \n+const WAIT_TIME = 1000;\n+\n+const allowPropagate = async () => {\n+    // Ensure ethers has enough time to catch up\n+    await sleep(WAIT_TIME);\n+}\n+\n+const seedExampleData = async () => {\n+    const rec = await Factory.findOne();\n+    if (!rec) {\n+        throw new Error(\n+            `Manually create the {\"implementation_adress\": ..., \"factory_address\": ...} record \n+            in \"factories\" collection. Run the \"forge script ...\" command from the comment \n+            in \"chain/script/CapTableFactory.s.sol\"`\n+        );\n+    }\n+\n+    const issuerResponse = await axios.post(\"http://localhost:8080/issuer/create\", exampleIssuer);\n+    const issuerId = issuerResponse.data.issuer._id;\n+    console.log(\"\u2705 | Issuer response \", issuerId, issuerResponse.data);\n+    await allowPropagate();\n+       \n+    const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerId));\n+    const s1Id = stakeholder1Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder1Response\", s1Id, stakeholder1Response.data);\n+    await allowPropagate();\n+    \n+    const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerId));\n+    const s2Id = stakeholder2Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder2Response\", s2Id, stakeholder2Response.data);\n+    await allowPropagate();\n+    \n+    const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerId));\n+    const stockClassId = stockClassResponse.data.stockClass._id;\n+    console.log(\"\u2705 | stockClassResponse\", stockClassId, stockClassResponse.data);\n+    await allowPropagate();\n+    \n+    const stockIssuanceResponse = await axios.post(\n+        \"http://localhost:8080/transactions/issuance/stock\",\n+        stockIssuance(issuerId, s1Id, stockClassId, \"500\", \"1.2\")\n+    );\n+    const issuance = stockIssuanceResponse.data.stockIssuance;\n+    console.log(\"\u2705 | stockIssuanceResponse\", issuance);\n+    await allowPropagate();\n+    \n+    // TODO: Victor going to finalize these?\n+    // const { security_id } = issuance;\n+    // const stockIssuanceAcceptanceResp = await axios.post(\n+    //     \"http://localhost:8080/transactions/accept/stock\",\n+    //     stockAccept(issuerId, s1Id, stockClassId, security_id, [\"Accepted\"])\n+    // );\n+    // console.log(\"\u2705 | Stock issuance acceptance response\", stockIssuanceAcceptanceResp.data);\n+    // await allowPropagate();\n+    \n+    const stockTransferResponse = await axios.post(\n+        \"http://localhost:8080/transactions/transfer/stock\",\n+        stockTransfer(issuerId, \"200\", s1Id, s2Id, stockClassId, \"4.20\")\n+    );\n+    console.log(\"\u2705 | stockTransferResponse\", stockTransferResponse.data);\n+    await allowPropagate();\n+    \n+    // TODO: Victor going to finalize these?\n+    // const stockTransferAcceptanceResp = await axios.post(\n+    //     \"http://localhost:8080/transactions/accept/stock\",\n+    //     stockAccept(issuerId, s2Id, stockClassId, security_id, [\"Accepted\"])\n+    // );\n+    // console.log(\"\u2705 | Stock transfer acceptance response\", stockTransferAcceptanceResp.data);\n+    // await allowPropagate();\n+\n+    return issuerId;\n+}\n+\n+const checkRecs = async (issuerId) => {\n+    const issuer = Issuer.findById(issuerId);\n+\n+    // TODO: aggregate docs across activePositions to \n+}\n+\n test('end to end with event processing', async () => {\n-    // TODO: talk to Victor to get a good set of example data going\n-    //  ??\n-    // Deploy a cap table and seed the database\n-    const issuerResponse = await axios.post(\"http://localhost:8080/issuer/create\", issuer);\n+    const issuerId = await seedExampleData();\n+    // Allow time for background process to catch up\n+    await sleep(15000);\n+    await checkRecs(issuerId);\n     \n-    // TODO: check that mongo has the appropriate updates\n-    // \n-});\n+}, WAIT_TIME * 100);"
            },
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 10,
                "deletions": 2,
                "patch": "@@ -1,11 +1,12 @@\n-import { trimEvents } from \"../../chain-operations/transactionPoller\";\n+import { trimEvents, txFuncs, txTypes } from \"../../chain-operations/transactionPoller\";\n \n // TODO: if starts failing again run: yarn add --dev jest-esm-transformer\n // https://jestjs.io/docs/using-matchers for more docs on `expect`\n \n-const myEvents = [5, 6, 6, 6, 7, 7, 7].map((x, i) => { return {blockNumber: x, i}; });\n+const myEvents = [5, 6, 6, 6, 7, 7, 7].map((x, i) => { return {i, o: {blockNumber: x}}; });\n \n test('trimEvents partial', () => {\n+    // @ts-ignore\n     const [events, block] = trimEvents(myEvents, 2, 10);\n     expect(events.length).toBe(4);\n     expect(events).toStrictEqual(myEvents.slice(0, 4));\n@@ -15,9 +16,16 @@ test('trimEvents partial', () => {\n test('trimEvents full', () => {\n     // We allow more than maxEvents in order to include all events of the last block\n     for (const maxEvents of [5, 6, 7, 15]) {\n+        // @ts-ignore\n         const [events, block] = trimEvents(myEvents, maxEvents, 10);\n         expect(events.length).toBe(myEvents.length);\n         expect(events).toStrictEqual(myEvents);\n         expect(block).toBe(10);\n     }\n });\n+\n+test('txMapper to maps', () => {\n+    // @ts-ignore\n+    expect(txTypes[3n]).toBe(\"StockAcceptance\");\n+    expect(txFuncs[\"StockAcceptance\"].name).toBe(\"handleStockAcceptance\");\n+});"
            }
        ]
    },
    {
        "sha": "11fa34003e79ba237421649d4d5372578aaaab59",
        "author": "kentkolze",
        "date": "2024-01-14 02:10:36+00:00",
        "message": "setup jest for running unit and integration tests in javascript\nget the service working with the mix of ts and js",
        "files": [
            {
                "filename": ".env.example",
                "additions": 3,
                "deletions": 1,
                "patch": "@@ -16,4 +16,6 @@ ETHERSCAN_L2_API_KEY=UPDATE_ME\n ETHERSCAN_L1_API_KEY=UPDATE_ME\n \n # Server port\n-PORT=8080\n\\ No newline at end of file\n+PORT=8080\n+\n+DATABASE_OVERRIDE=\"\"  # use a database other than the default in DATABASE_URL\n\\ No newline at end of file"
            },
            {
                "filename": "jest.config.js",
                "additions": 3,
                "deletions": 0,
                "patch": "@@ -3,6 +3,9 @@\n  * https://jestjs.io/docs/configuration\n  */\n \n+// This allows us to avoid messing with the state of people's standard dev database\n+process.env['DATABASE_OVERRIDE'] = 'jest';\n+\n /** @type {import('jest').Config} */\n const config = {\n   // All imported modules in your tests should be mocked automatically"
            },
            {
                "filename": "package.json",
                "additions": 4,
                "deletions": 4,
                "patch": "@@ -7,7 +7,7 @@\n     \"description\": \"Transfer Agent Protocol onchain cap table\",\n     \"type\": \"module\",\n     \"scripts\": {\n-        \"start\": \"nodemon src/server.js\",\n+        \"start\": \"nodemon --loader ts-node/esm src/server.js\",\n         \"eslint\": \"eslint --cache --cache-location=node_modules/.cache/.eslintcache --fix\",\n         \"lint\": \"yarn run eslint . --ext .js,.jsx,.ts,.tsx\",\n         \"format\": \"prettier '**/*' --ignore-unknown --write\",\n@@ -18,8 +18,8 @@\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n         \"deseed\": \"node src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\",\n-        \"test-js\": \"jest --testPathPattern /src/tests/unit\",\n-        \"test-js-integration\": \"jest --testPathPattern /src/tests/integration\",\n+        \"test-js\": \"jest --testPathPattern src/tests/unit\",\n+        \"test-js-integration\": \"jest --testPathPattern src/tests/integration\",\n         \"validate-poet-files\": \"cd ocf && yarn validate-poet-files\",\n         \"test-onchain-cap-table-local\": \"node src/chain-operations/capTable.cjs local\",\n         \"test-onchain-cap-table-optimism-goerli\": \"node src/chain-operations/capTable.cjs optimism-goerli\",\n@@ -42,7 +42,6 @@\n         \"express\": \"^4.18.2\",\n         \"mongoose\": \"^7.4.2\",\n         \"solc\": \"^0.8.20\",\n-        \"typescript\": \"^5.3.3\",\n         \"xstate\": \"^4.38.2\",\n         \"yauzl\": \"^2.10.0\"\n     },\n@@ -67,6 +66,7 @@\n         \"solhint\": \"^3.4.1\",\n         \"ts-jest\": \"^29.1.1\",\n         \"ts-node\": \"^10.9.2\",\n+        \"typescript\": \"^5.3.3\",\n         \"uuid\": \"^9.0.0\"\n     }\n }"
            },
            {
                "filename": "src/app.js",
                "additions": 93,
                "deletions": 0,
                "patch": "@@ -0,0 +1,93 @@\n+import { config } from \"dotenv\";\n+import express, { json, urlencoded } from \"express\";\n+config();\n+\n+import { connectDB } from \"./db/config/mongoose.ts\";\n+\n+import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n+\n+// Routes\n+import historicalTransactions from \"./routes/historicalTransactions.js\";\n+import mainRoutes from \"./routes/index.js\";\n+import issuerRoutes from \"./routes/issuer.js\";\n+import stakeholderRoutes from \"./routes/stakeholder.js\";\n+import stockClassRoutes from \"./routes/stockClass.js\";\n+import stockLegendRoutes from \"./routes/stockLegend.js\";\n+import stockPlanRoutes from \"./routes/stockPlan.js\";\n+import transactionRoutes from \"./routes/transactions.js\";\n+import valuationRoutes from \"./routes/valuation.js\";\n+import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n+\n+import mongoose from \"mongoose\";\n+import { readIssuerById } from \"./db/operations/read.js\";\n+import { getIssuerContract } from \"./utils/caches.ts\";\n+\n+const app = express();\n+\n+const PORT = process.env.PORT;\n+const CHAIN = process.env.CHAIN;\n+\n+// Middlewares\n+const chainMiddleware = (req, res, next) => {\n+    req.chain = CHAIN;\n+    next();\n+};\n+\n+// Middleware to get or create contract instance\n+// the listener is first started on deployment, then here as a backup\n+const contractMiddleware = async (req, res, next) => {\n+    if (!req.body.issuerId) {\n+        console.log(\"\u274c | No issuer ID\");\n+        res.status(400).send(\"issuerId is required\");\n+    }\n+\n+    // fetch issuer to ensure it exists\n+    const issuer = await readIssuerById(req.body.issuerId);\n+    if (!issuer) res.status(400).send(\"issuer not found \");\n+\n+    const {contract, provider} = await getIssuerContract(issuer);\n+    req.contract = contract;\n+    req.provider = provider;\n+    next();\n+};\n+app.use(urlencoded({ limit: \"50mb\", extended: true }));\n+app.use(json({ limit: \"50mb\" }));\n+app.enable(\"trust proxy\");\n+\n+app.use(\"/\", chainMiddleware, mainRoutes);\n+app.use(\"/issuer\", chainMiddleware, issuerRoutes);\n+app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n+app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n+// No middleware required since these are only created offchain\n+app.use(\"/stock-legend\", stockLegendRoutes);\n+app.use(\"/stock-plan\", stockPlanRoutes);\n+app.use(\"/valuation\", valuationRoutes);\n+app.use(\"/vesting-terms\", vestingTermsRoutes);\n+app.use(\"/historical-transactions\", historicalTransactions);\n+\n+// transactions\n+app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n+\n+export const startServer = async () => {\n+    // Connect to MongoDB\n+    await connectDB();\n+\n+    const server = app.listen(PORT, async () => {\n+        console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n+        // Asynchronous job to track web3 events in web2\n+        startEventProcessing();\n+    });\n+\n+    return server;\n+};\n+\n+export const shutdownServer = async (server) => {\n+    console.log(\"Shutting down app server...\");\n+    server.close();\n+    console.log(\"Stopping event processing...\");\n+    stopEventProcessing();\n+    console.log(\"Disconnecting from mongo...\");\n+    await mongoose.disconnect();\n+    console.log(\" done!\");\n+}\n+"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,5 +1,5 @@\n-import { ethers } from \"ethers\";\n import { config } from \"dotenv\";\n+import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n import getProvider from \"./getProvider.js\";"
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 19,
                "deletions": 12,
                "patch": "@@ -4,6 +4,7 @@ import { withGlobalTransaction } from \"../db/operations/atomic.ts\";\n import { readAllIssuers } from \"../db/operations/read.js\";\n import { updateIssuerById } from \"../db/operations/update.js\";\n import { getIssuerContract } from \"../utils/caches.ts\";\n+import sleep from \"../utils/sleep.js\";\n import { verifyIssuerAndSeed } from \"./seed.js\";\n import {\n     IssuerAuthorizedSharesAdjustment,\n@@ -56,38 +57,43 @@ const txFuncs = new Map(\n     Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_x, [name, _y, handleFunc]]) => [name, handleFunc])\n );\n \n+let _keepProcessing = true;\n \n-const sleep = (delay) => new Promise((resolve) => setTimeout(resolve, delay));\n+export const stopEventProcessing = () => {\n+    _keepProcessing = false;\n+}\n \n-export const startSynchronousEventProcessing = async () => {\n-    while (true) {\n-        await sleep(1 * 1000);\n+export const startEventProcessing = async () => {\n+    _keepProcessing = true\n+    const dbConn = await connectDB();\n+    while (_keepProcessing) {\n         const issuers = await readAllIssuers();\n-        const dbConn = await connectDB();\n-        // Process events synchronously for each issuer\n-        console.log(`Processing for ${issuers.length} issuers`);\n+        console.log(`Processing synchronously for ${issuers.length} issuers`);\n         for (const issuer of issuers) {\n             if (issuer.deployed_to) {\n                 const { contract, provider, libraries } = await getIssuerContract(issuer);\n                 await processEvents(dbConn, contract, provider, issuer, libraries.txHelper);\n             }\n         }\n+        await sleep(10 * 1000);\n     }\n };\n \n const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBlocks = 1500, maxEvents = 250) => {\n     /*\n     We process up to `maxEvents` across `maxBlocks` to ensure our transaction sizes dont get too big and bog down our db\n     */\n-    console.log(\" processEvents for issuer\", issuer);\n-    let {_id: issuerId, last_processed_block: startBlock, tx_hash: deployedTx} = issuer;\n+    let {_id: issuerId, last_processed_block: startBlock, tx_hash: deployedTxHash} = issuer;\n+    console.log(\"Processing for issuer\", issuerId, startBlock, deployedTxHash);\n     if (startBlock === null) {\n-        const receipt = await provider.getTransactionReceipt(deployedTx);\n+        const receipt = await provider.getTransactionReceipt(deployedTxHash);\n+        const tx = await provider.getTransaction(deployedTxHash);\n+        console.log(\"tx\", tx);\n         if (!receipt) {\n             console.error(\"Transaction receipt not found\");\n             return;\n         }\n-        startBlock = await bootstrapTable(issuerId, receipt.blockNumber, contract, dbConn);\n+        startBlock = await bootstrapIssuer(issuerId, receipt.blockNumber, contract, dbConn);\n     }\n     const {number: latestBlock} = await provider.getBlock('finalized');\n     let endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n@@ -141,7 +147,8 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBl\n     }, dbConn);\n };\n \n-const bootstrapTable = async (issuerId, deployedBlockNumber, contract, dbConn) => {\n+const bootstrapIssuer = async (issuerId, deployedBlockNumber, contract, dbConn) => {\n+    console.log(\"Bootstrapping issuer\");\n     // TODO: fix the copy-pasted query\n     const issuerCreatedFilter = contract.filters.IssuerCreated;\n     const issuerEvents = await contract.queryFilter(issuerCreatedFilter);"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 3,
                "deletions": 1,
                "patch": "@@ -4,10 +4,12 @@ import mongoose from \"mongoose\";\n dotenv.config();\n \n const DATABASE_URL = process.env.DATABASE_URL;\n+const DATABASE_OVERRIDE = process.env.DATABASE_OVERRIDE;\n \n export const connectDB = async () => {\n+    const connectOptions = DATABASE_OVERRIDE ? {dbName: DATABASE_OVERRIDE} : {};\n     try {\n-        await mongoose.connect(DATABASE_URL);\n+        await mongoose.connect(DATABASE_URL, connectOptions);\n         console.log(\"\u2705 | Mongo connected succesfully\");\n         return mongoose.connection;\n     } catch (error) {"
            },
            {
                "filename": "src/db/scripts/deseed.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,5 +1,6 @@\n import mongoose from \"mongoose\";\n-import connectDB from \"../config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n@@ -8,7 +9,6 @@ import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n import { typeToModelType } from \"../operations/transactions.js\"; // Import the typeToModelType object to delete all transactions\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n \n const deleteAllTransactions = async () => {\n     for (const ModelType of Object.values(typeToModelType)) {"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 1,
                "deletions": 8,
                "patch": "@@ -8,9 +8,6 @@ import { countIssuers, readIssuerById } from \"../db/operations/read.js\";\n import { convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n-import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n-import { contractCache } from \"../utils/caches.ts\";\n-\n const issuer = Router();\n \n issuer.get(\"/\", async (req, res) => {\n@@ -56,17 +53,13 @@ issuer.post(\"/create\", async (req, res) => {\n \n         const issuerIdBytes16 = convertUUIDToBytes16(incomingIssuerToValidate.id);\n         console.log(\"\ud83d\udcbe | Issuer id in bytes16 \", issuerIdBytes16);\n-        const { contract, provider, address, libraries, deployHash } = await deployCapTable(\n+        const { address, deployHash } = await deployCapTable(\n             chain,\n             issuerIdBytes16,\n             incomingIssuerToValidate.legal_name,\n             incomingIssuerToValidate.initial_shares_authorized\n         );\n \n-        // add contract to the cache and start listener\n-        contractCache[incomingIssuerToValidate.id] = { contract, provider, libraries };\n-        startOnchainListeners(contract, provider, incomingIssuerToValidate.id, libraries);\n-\n         const incomingIssuerForDB = {\n             ...incomingIssuerToValidate,\n             deployed_to: address,"
            },
            {
                "filename": "src/scripts/testAcceptance.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.js\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockAccept } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testCancellation.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -1,7 +1,7 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { issuer, stakeholder1, stakeholder2, stockCancel, stockClass, stockIssuance, stockTransfer } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { stockCancel } from \"./sampleData.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testIssuance.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,9 +1,9 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import Issuer from \"../db/objects/Issuer.js\";\n import Stakeholder from \"../db/objects/Stakeholder.js\";\n import StockClass from \"../db/objects/StockClass.js\";\n-import axios from \"axios\";\n import { stockIssuance } from \"./sampleData.js\";\n-import connectDB from \"../db/config/mongoose.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testIssuerAdjustment.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -1,7 +1,7 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { stockClass, stockClassAuthorizedSharesAdjust, issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testReissuance.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockReissue } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testRepurchase.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,7 +1,7 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockRepurchase } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testRetraction.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockRetract } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n // Connect to MongoDB\n connectDB();\n "
            },
            {
                "filename": "src/scripts/testStockClassAdjustment.js",
                "additions": 4,
                "deletions": 3,
                "patch": "@@ -1,7 +1,8 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { stockClass, stockClassAuthorizedSharesAdjust, issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { stockClassAuthorizedSharesAdjust } from \"./sampleData.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/server.js",
                "additions": 2,
                "deletions": 69,
                "patch": "@@ -1,70 +1,3 @@\n-import { config } from \"dotenv\";\n-import express, { json, urlencoded } from \"express\";\n-config();\n+import { startServer } from \"./app.js\";\n \n-import connectDB from \"./db/config/mongoose.js\";\n-\n-import startSynchronousEventProcessing from \"./chain-operations/transactionPoller.js\";\n-\n-// Routes\n-import historicalTransactions from \"./routes/historicalTransactions.js\";\n-import mainRoutes from \"./routes/index.js\";\n-import issuerRoutes from \"./routes/issuer.js\";\n-import stakeholderRoutes from \"./routes/stakeholder.js\";\n-import stockClassRoutes from \"./routes/stockClass.js\";\n-import stockLegendRoutes from \"./routes/stockLegend.js\";\n-import stockPlanRoutes from \"./routes/stockPlan.js\";\n-import transactionRoutes from \"./routes/transactions.js\";\n-import valuationRoutes from \"./routes/valuation.js\";\n-import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n-\n-import { readIssuerById } from \"./db/operations/read.js\";\n-import { getIssuerContract } from \"./utils/caches.ts\";\n-\n-const app = express();\n-\n-// Connect to MongoDB\n-connectDB();\n-\n-const PORT = process.env.PORT;\n-\n-// Middleware to get or create contract instance\n-// the listener is first started on deployment, then here as a backup\n-const contractMiddleware = async (req, res, next) => {\n-    if (!req.body.issuerId) {\n-        console.log(\"\u274c | No issuer ID\");\n-        res.status(400).send(\"issuerId is required\");\n-    }\n-\n-    // fetch issuer to ensure it exists\n-    const issuer = await readIssuerById(req.body.issuerId);\n-    if (!issuer) res.status(400).send(\"issuer not found \");\n-\n-    const { contract, provider } = await getIssuerContract(issuer);\n-    req.contract = contract;\n-    req.provider = provider;\n-    next();\n-};\n-app.use(urlencoded({ limit: \"50mb\", extended: true }));\n-app.use(json({ limit: \"50mb\" }));\n-app.enable(\"trust proxy\");\n-\n-app.use(\"/\", mainRoutes);\n-app.use(\"/issuer\", issuerRoutes);\n-app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n-app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n-// No middleware required since these are only created offchain\n-app.use(\"/stock-legend\", stockLegendRoutes);\n-app.use(\"/stock-plan\", stockPlanRoutes);\n-app.use(\"/valuation\", valuationRoutes);\n-app.use(\"/vesting-terms\", vestingTermsRoutes);\n-app.use(\"/historical-transactions\", historicalTransactions);\n-\n-// transactions\n-app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n-\n-app.listen(PORT, async () => {\n-    console.log(`\ud83d\ude80  Server successfully launched at: ${PORT}`);\n-    // Kick off asynchronous job to process changes\n-    startSynchronousEventProcessing();\n-});\n+startServer();"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 33,
                "deletions": 0,
                "patch": "@@ -1 +1,34 @@\n+import axios from \"axios\";\n+import { shutdownServer, startServer } from \"../../app\";\n+import { connectDB } from \"../../db/config/mongoose\";\n+import { issuer } from \"../../scripts/sampleData\";\n \n+\n+let _server = null\n+\n+const deleteAllCollections = async () => {\n+    const dbConn = await connectDB();\n+    console.log(\"Dropping mongo database: \", dbConn.name);\n+    await dbConn.dropDatabase();\n+}\n+\n+beforeAll(async () => {\n+    await deleteAllCollections();\n+    console.log(\"starting server\");\n+    _server = await startServer();\n+});\n+\n+afterAll(async () => {\n+    console.log(\"shuting down server\");\n+    await shutdownServer(_server);\n+});\n+\n+test('end to end with event processing', async () => {\n+    // TODO: talk to Victor to get a good set of example data going\n+    //  ??\n+    // Deploy a cap table and seed the database\n+    const issuerResponse = await axios.post(\"http://localhost:8080/issuer/create\", issuer);\n+    \n+    // TODO: check that mongo has the appropriate updates\n+    // \n+});"
            },
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -1,6 +1,7 @@\n import { trimEvents } from \"../../chain-operations/transactionPoller\";\n \n-// TODO: if starts failing again: yarn add --dev jest-esm-transformer\n+// TODO: if starts failing again run: yarn add --dev jest-esm-transformer\n+// https://jestjs.io/docs/using-matchers for more docs on `expect`\n \n const myEvents = [5, 6, 6, 6, 7, 7, 7].map((x, i) => { return {blockNumber: x, i}; });\n "
            },
            {
                "filename": "src/utils/caches.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,4 +1,4 @@\n-import getContractInstance from \"../chain-operations/getContractInstances\";\n+import { getContractInstance } from \"../chain-operations/getContractInstances.js\";\n \n const CHAIN = process.env.CHAIN;\n "
            }
        ]
    },
    {
        "sha": "73b6063701e060f69fb3c909261eb9fa4c83747b",
        "author": "kentkolze",
        "date": "2024-01-11 18:45:38+00:00",
        "message": "better docs and test",
        "files": [
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 4,
                "deletions": 3,
                "patch": "@@ -2,7 +2,7 @@ import { trimEvents } from \"../../chain-operations/transactionPoller\";\n \n // TODO: if starts failing again: yarn add --dev jest-esm-transformer\n \n-const myEvents = [5, 6, 6, 6, 7].map((x, i) => { return {blockNumber: x, i}; });\n+const myEvents = [5, 6, 6, 6, 7, 7, 7].map((x, i) => { return {blockNumber: x, i}; });\n \n test('trimEvents partial', () => {\n     const [events, block] = trimEvents(myEvents, 2, 10);\n@@ -12,8 +12,9 @@ test('trimEvents partial', () => {\n });\n \n test('trimEvents full', () => {\n-    for (const max of [5, 6, 15]) {\n-        const [events, block] = trimEvents(myEvents, max, 10);\n+    // We allow more than maxEvents in order to include all events of the last block\n+    for (const maxEvents of [5, 6, 7, 15]) {\n+        const [events, block] = trimEvents(myEvents, maxEvents, 10);\n         expect(events.length).toBe(myEvents.length);\n         expect(events).toStrictEqual(myEvents);\n         expect(block).toBe(10);"
            }
        ]
    },
    {
        "sha": "5d8fd005ee5e8d38893a90ad2648d91ddb761512",
        "author": "kentkolze",
        "date": "2024-01-11 18:43:33+00:00",
        "message": "fix trimEvents and tests",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 4,
                "deletions": 6,
                "patch": "@@ -182,23 +182,21 @@ const persistEvents = async (issuerId, events) => {\n export const trimEvents = (events, maxEvents, endBlock) => {\n     let index = 0;    \n     while (index < maxEvents && index < events.length) {\n-        // Iterate through the entire next block\n+        // Include the entire next block\n         const includeBlock = events[index].blockNumber;\n         index++;\n         while (index < events.length && events[index].blockNumber === includeBlock) {\n             index++;\n         }\n     }\n-\n     // Nothing to trim!\n-    if (index >= (events.length - 1)) {\n+    if (index >= events.length) {\n         return [events, endBlock];\n     }\n-\n-    // Trim up to index (exclusive)\n     // We processed up to the last events' blockNumber\n+    // `index` is *exclusive* when trimming\n     const useEvents = [...events.slice(0, index)];\n-    return [useEvents, trimEvents[useEvents.length - 1].blockNumber];\n+    return [useEvents, useEvents[useEvents.length - 1].blockNumber];\n };\n \n "
            },
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 7,
                "deletions": 5,
                "patch": "@@ -7,13 +7,15 @@ const myEvents = [5, 6, 6, 6, 7].map((x, i) => { return {blockNumber: x, i}; });\n test('trimEvents partial', () => {\n     const [events, block] = trimEvents(myEvents, 2, 10);\n     expect(events.length).toBe(4);\n-    expect(events).toBe(myEvents.slice(0, 4));\n+    expect(events).toStrictEqual(myEvents.slice(0, 4));\n     expect(block).toBe(6);\n });\n \n test('trimEvents full', () => {\n-    const [events, block] = trimEvents(myEvents, 2, 10);\n-    expect(events.length).toBe(myEvents.length);\n-    expect(events).toBe(myEvents);\n-    expect(block).toBe(10);\n+    for (const max of [5, 6, 15]) {\n+        const [events, block] = trimEvents(myEvents, max, 10);\n+        expect(events.length).toBe(myEvents.length);\n+        expect(events).toStrictEqual(myEvents);\n+        expect(block).toBe(10);\n+    }\n });"
            }
        ]
    },
    {
        "sha": "adf72399a97b0a67273bddd8a536bff6ec9e2fd7",
        "author": "kentkolze",
        "date": "2024-01-11 18:31:33+00:00",
        "message": "get jest unittests setup working",
        "files": [
            {
                "filename": "jest.config.js",
                "additions": 200,
                "deletions": 0,
                "patch": "@@ -0,0 +1,200 @@\n+/**\n+ * For a detailed explanation regarding each configuration property, visit:\n+ * https://jestjs.io/docs/configuration\n+ */\n+\n+/** @type {import('jest').Config} */\n+const config = {\n+  // All imported modules in your tests should be mocked automatically\n+  // automock: false,\n+\n+  // Stop running tests after `n` failures\n+  // bail: 0,\n+\n+  // The directory where Jest should store its cached dependency information\n+  // cacheDirectory: \"/tmp/jest_rs\",\n+\n+  // Automatically clear mock calls, instances, contexts and results before every test\n+  // clearMocks: false,\n+\n+  // Indicates whether the coverage information should be collected while executing the test\n+  // collectCoverage: false,\n+\n+  // An array of glob patterns indicating a set of files for which coverage information should be collected\n+  // collectCoverageFrom: undefined,\n+\n+  // The directory where Jest should output its coverage files\n+  // coverageDirectory: \"coverage\",\n+\n+  // An array of regexp pattern strings used to skip coverage collection\n+  // coveragePathIgnorePatterns: [\n+  //   \"/node_modules/\"\n+  // ],\n+\n+  // Indicates which provider should be used to instrument code for coverage\n+  // coverageProvider: \"babel\",\n+\n+  // A list of reporter names that Jest uses when writing coverage reports\n+  // coverageReporters: [\n+  //   \"json\",\n+  //   \"text\",\n+  //   \"lcov\",\n+  //   \"clover\"\n+  // ],\n+\n+  // An object that configures minimum threshold enforcement for coverage results\n+  // coverageThreshold: undefined,\n+\n+  // A path to a custom dependency extractor\n+  // dependencyExtractor: undefined,\n+\n+  // Make calling deprecated APIs throw helpful error messages\n+  // errorOnDeprecated: false,\n+\n+  // The default configuration for fake timers\n+  // fakeTimers: {\n+  //   \"enableGlobally\": false\n+  // },\n+\n+  // Force coverage collection from ignored files using an array of glob patterns\n+  // forceCoverageMatch: [],\n+\n+  // A path to a module which exports an async function that is triggered once before all test suites\n+  // globalSetup: undefined,\n+\n+  // A path to a module which exports an async function that is triggered once after all test suites\n+  // globalTeardown: undefined,\n+\n+  // A set of global variables that need to be available in all test environments\n+  // globals: {},\n+\n+  // The maximum amount of workers used to run your tests. Can be specified as % or a number. E.g. maxWorkers: 10% will use 10% of your CPU amount + 1 as the maximum worker number. maxWorkers: 2 will use a maximum of 2 workers.\n+  // maxWorkers: \"50%\",\n+\n+  // An array of directory names to be searched recursively up from the requiring module's location\n+  // moduleDirectories: [\n+  //   \"node_modules\"\n+  // ],\n+\n+  // An array of file extensions your modules use\n+  // moduleFileExtensions: [\n+  //   \"js\",\n+  //   \"mjs\",\n+  //   \"cjs\",\n+  //   \"jsx\",\n+  //   \"ts\",\n+  //   \"tsx\",\n+  //   \"json\",\n+  //   \"node\"\n+  // ],\n+\n+  // A map from regular expressions to module names or to arrays of module names that allow to stub out resources with a single module\n+  // moduleNameMapper: {},\n+\n+  // An array of regexp pattern strings, matched against all module paths before considered 'visible' to the module loader\n+  // modulePathIgnorePatterns: [],\n+\n+  // Activates notifications for test results\n+  // notify: false,\n+\n+  // An enum that specifies notification mode. Requires { notify: true }\n+  // notifyMode: \"failure-change\",\n+\n+  // A preset that is used as a base for Jest's configuration\n+  preset: \"ts-jest\",\n+\n+  // Run tests from one or more projects\n+  // projects: undefined,\n+\n+  // Use this configuration option to add custom reporters to Jest\n+  // reporters: undefined,\n+\n+  // Automatically reset mock state before every test\n+  // resetMocks: false,\n+\n+  // Reset the module registry before running each individual test\n+  // resetModules: false,\n+\n+  // A path to a custom resolver\n+  // resolver: undefined,\n+\n+  // Automatically restore mock state and implementation before every test\n+  // restoreMocks: false,\n+\n+  // The root directory that Jest should scan for tests and modules within\n+  // rootDir: undefined,\n+\n+  // A list of paths to directories that Jest should use to search for files in\n+  // roots: [\n+  //   \"<rootDir>\"\n+  // ],\n+\n+  // Allows you to use a custom runner instead of Jest's default test runner\n+  // runner: \"jest-runner\",\n+\n+  // The paths to modules that run some code to configure or set up the testing environment before each test\n+  // setupFiles: [],\n+\n+  // A list of paths to modules that run some code to configure or set up the testing framework before each test\n+  // setupFilesAfterEnv: [],\n+\n+  // The number of seconds after which a test is considered as slow and reported as such in the results.\n+  // slowTestThreshold: 5,\n+\n+  // A list of paths to snapshot serializer modules Jest should use for snapshot testing\n+  // snapshotSerializers: [],\n+\n+  // The test environment that will be used for testing\n+  // testEnvironment: \"jest-environment-node\",\n+\n+  // Options that will be passed to the testEnvironment\n+  // testEnvironmentOptions: {},\n+\n+  // Adds a location field to test results\n+  // testLocationInResults: false,\n+\n+  // The glob patterns Jest uses to detect test files\n+  // testMatch: [\n+  //   \"**/__tests__/**/*.[jt]s?(x)\",\n+  //   \"**/?(*.)+(spec|test).[tj]s?(x)\"\n+  // ],\n+\n+  // An array of regexp pattern strings that are matched against all test paths, matched tests are skipped\n+  // testPathIgnorePatterns: [\n+  //   \"/node_modules/\"\n+  // ],\n+\n+  // The regexp pattern or array of patterns that Jest uses to detect test files\n+  // testRegex: [],\n+\n+  // This option allows the use of a custom results processor\n+  // testResultsProcessor: undefined,\n+\n+  // This option allows use of a custom test runner\n+  // testRunner: \"jest-circus/runner\",\n+\n+  // A map from regular expressions to paths to transformers\n+  transform: {\n+    \".*\\\\.(tsx?|js)$\": \"ts-jest\",\n+  },\n+\n+  // An array of regexp pattern strings that are matched against all source file paths, matched files will skip transformation\n+  // transformIgnorePatterns: [\n+  //   \"/node_modules/\",\n+  //   \"\\\\.pnp\\\\.[^\\\\/]+$\"\n+  // ],\n+\n+  // An array of regexp pattern strings that are matched against all modules before the module loader will automatically return a mock for them\n+  // unmockedModulePathPatterns: undefined,\n+\n+  // Indicates whether each individual test should be reported during the run\n+  verbose: true,\n+\n+  // An array of regexp patterns that are matched against all source file paths before re-running tests in watch mode\n+  // watchPathIgnorePatterns: [],\n+\n+  // Whether to use watchman for file crawling\n+  // watchman: true,\n+};\n+\n+export default config;"
            },
            {
                "filename": "package.json",
                "additions": 16,
                "deletions": 1,
                "patch": "@@ -18,6 +18,17 @@\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n         \"deseed\": \"node src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\",\n+        \"test-js\": \"jest --testPathPattern /src/tests/unit\",\n+        \"test-js-integration\": \"jest --testPathPattern /src/tests/integration\",\n+        \"validate-poet-files\": \"cd ocf && yarn validate-poet-files\",\n+        \"test-onchain-cap-table-local\": \"node src/chain-operations/capTable.cjs local\",\n+        \"test-onchain-cap-table-optimism-goerli\": \"node src/chain-operations/capTable.cjs optimism-goerli\",\n+        \"test-onchain-cap-table-factory-local\": \"node src/chain-operations/capTableFactory.cjs local\",\n+        \"test-onchain-cap-table-factory-optimism-goerli\": \"node src/chain-operations/capTableFactory.cjs optimism-goerli\",\n+        \"forge-deploy-captable-local\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n+        \"forge-deploy-captable-optimism-goerli\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n+        \"forge-deploy-captable-factory-local\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n+        \"forge-deploy-captable-factory-optimism-goerli\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n         \"export-manifest\": \"cd src/db/samples && zip -r notPoet.zip notPoet && mv notPoet.zip $HOME/Downloads\"\n     },\n     \"dependencies\": {\n@@ -31,6 +42,7 @@\n         \"express\": \"^4.18.2\",\n         \"mongoose\": \"^7.4.2\",\n         \"solc\": \"^0.8.20\",\n+        \"typescript\": \"^5.3.3\",\n         \"xstate\": \"^4.38.2\",\n         \"yauzl\": \"^2.10.0\"\n     },\n@@ -40,18 +52,21 @@\n         \"maintained node versions\"\n     ],\n     \"devDependencies\": {\n+        \"@types/jest\": \"^29.5.11\",\n         \"@types/node\": \"^20.3.2\",\n         \"@types/uuid\": \"^9.0.2\",\n         \"eslint\": \"^8.21.0\",\n         \"eslint-config-next\": \"^13.1.6\",\n         \"eslint-config-prettier\": \"^8.5.0\",\n         \"eslint-plugin-import\": \"^2.26.0\",\n         \"husky\": \"^8.0.1\",\n+        \"jest\": \"^29.7.0\",\n         \"lint-staged\": \"^13.0.3\",\n         \"nodemon\": \"^3.0.1\",\n         \"prettier\": \"^2.7.1\",\n         \"solhint\": \"^3.4.1\",\n-        \"typescript\": \"^5.1.6\",\n+        \"ts-jest\": \"^29.1.1\",\n+        \"ts-node\": \"^10.9.2\",\n         \"uuid\": \"^9.0.0\"\n     }\n }"
            },
            {
                "filename": "src/chain-operations/seed.js",
                "additions": 2,
                "deletions": 3,
                "patch": "@@ -1,11 +1,10 @@\n import { convertAndReflectStakeholderOnchain } from \"../controllers/stakeholderController.js\";\n import { convertAndReflectStockClassOnchain } from \"../controllers/stockClassController.js\";\n-import { getAllIssuerDataById } from \"../db/operations/read.js\";\n+import { getAllIssuerDataById, readIssuerById } from \"../db/operations/read.js\";\n+import { preProcessorCache } from \"../utils/caches.ts\";\n import { convertTimeStampToUint40, toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import { convertBytes16ToUUID, convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import { extractArrays } from \"../utils/flattenPreprocessorCache.js\";\n-import { preProcessorCache } from \"../utils/caches.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n import sleep from \"../utils/sleep.js\";\n \n export const verifyIssuerAndSeed = async (contract, id) => {"
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 7,
                "deletions": 4,
                "patch": "@@ -1,9 +1,9 @@\n import { AbiCoder } from \"ethers\";\n-import connectDB from \"../db/config/mongoose.js\";\n-import { withGlobalTransaction } from \"../db/operations/atomic.js\";\n+import { connectDB } from \"../db/config/mongoose.ts\";\n+import { withGlobalTransaction } from \"../db/operations/atomic.ts\";\n import { readAllIssuers } from \"../db/operations/read.js\";\n import { updateIssuerById } from \"../db/operations/update.js\";\n-import { getIssuerContract } from \"../utils/caches.js\";\n+import { getIssuerContract } from \"../utils/caches.ts\";\n import { verifyIssuerAndSeed } from \"./seed.js\";\n import {\n     IssuerAuthorizedSharesAdjustment,\n@@ -56,6 +56,7 @@ const txFuncs = new Map(\n     Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_x, [name, _y, handleFunc]]) => [name, handleFunc])\n );\n \n+\n const sleep = (delay) => new Promise((resolve) => setTimeout(resolve, delay));\n \n export const startSynchronousEventProcessing = async () => {\n@@ -115,6 +116,8 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBl\n             continue;\n         }\n         // TODO: does txTypeIdx even come with the event??? need to test this...\n+        let txTypeIdx;\n+        let txData;\n         const [type, structType] = txMapper[txTypeIdx];\n         const decodedData = abiCoder.decode([structType], txData);\n         const { timestamp } = await provider.getBlock(event.blockNumber);\n@@ -176,7 +179,7 @@ const persistEvents = async (issuerId, events) => {\n     }\n };\n \n-const trimEvents = (events, maxEvents, endBlock) => {\n+export const trimEvents = (events, maxEvents, endBlock) => {\n     let index = 0;    \n     while (index < maxEvents && index < events.length) {\n         // Iterate through the entire next block"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 2,
                "deletions": 4,
                "patch": "@@ -1,11 +1,11 @@\n-import mongoose from \"mongoose\";\n import dotenv from \"dotenv\";\n+import mongoose from \"mongoose\";\n \n dotenv.config();\n \n const DATABASE_URL = process.env.DATABASE_URL;\n \n-const connectDB = async () => {\n+export const connectDB = async () => {\n     try {\n         await mongoose.connect(DATABASE_URL);\n         console.log(\"\u2705 | Mongo connected succesfully\");\n@@ -16,5 +16,3 @@ const connectDB = async () => {\n         process.exit(1);\n     }\n };\n-\n-export default connectDB;"
            },
            {
                "filename": "src/db/operations/atomic.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,7 +1,7 @@\n // Store a global mongo session to allows us to bundle CRUD operations into one transaction\n \n import { Connection, QueryOptions } from \"mongoose\";\n-import connectDB from \"../config/mongoose\";\n+import { connectDB } from \"../config/mongoose.ts\";\n type TQueryOptions = QueryOptions | null;\n \n let _globalSession = null;"
            },
            {
                "filename": "src/db/operations/create.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -10,7 +10,7 @@ import ConvertibleIssuance from \"../objects/transactions/issuance/ConvertibleIss\n import EquityCompensationIssuance from \"../objects/transactions/issuance/EquityCompensationIssuance.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import { save } from \"./atomic.js\";\n+import { save } from \"./atomic.ts\";\n \n export const createIssuer = (issuerData) => {\n     return save(new Issuer(issuerData));"
            },
            {
                "filename": "src/db/operations/read.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -9,7 +9,7 @@ import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import { countDocuments, find, findById } from \"./atomic.js\";\n+import { countDocuments, find, findById } from \"./atomic.ts\";\n \n // READ By ID\n export const readIssuerById = async (id) => {"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -8,8 +8,8 @@ import { countIssuers, readIssuerById } from \"../db/operations/read.js\";\n import { convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n-import { contractCache } from \"../utils/caches.js\";\n import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n+import { contractCache } from \"../utils/caches.ts\";\n \n const issuer = Router();\n "
            },
            {
                "filename": "src/server.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -19,7 +19,7 @@ import valuationRoutes from \"./routes/valuation.js\";\n import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n \n import { readIssuerById } from \"./db/operations/read.js\";\n-import { getIssuerContract } from \"./utils/caches.js\";\n+import { getIssuerContract } from \"./utils/caches.ts\";\n \n const app = express();\n "
            },
            {
                "filename": "src/state-machines/process.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,6 +1,6 @@\n import { interpret } from \"xstate\";\n+import { preProcessorCache } from \"../utils/caches.ts\";\n import { parentMachine } from \"./parent.js\";\n-import { preProcessorCache } from \"../utils/caches.js\";\n \n /*\n     @dev: Parent-Child machines are created to calculate current context then deleted."
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -0,0 +1 @@\n+"
            },
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 19,
                "deletions": 0,
                "patch": "@@ -0,0 +1,19 @@\n+import { trimEvents } from \"../../chain-operations/transactionPoller\";\n+\n+// TODO: if starts failing again: yarn add --dev jest-esm-transformer\n+\n+const myEvents = [5, 6, 6, 6, 7].map((x, i) => { return {blockNumber: x, i}; });\n+\n+test('trimEvents partial', () => {\n+    const [events, block] = trimEvents(myEvents, 2, 10);\n+    expect(events.length).toBe(4);\n+    expect(events).toBe(myEvents.slice(0, 4));\n+    expect(block).toBe(6);\n+});\n+\n+test('trimEvents full', () => {\n+    const [events, block] = trimEvents(myEvents, 2, 10);\n+    expect(events.length).toBe(myEvents.length);\n+    expect(events).toBe(myEvents);\n+    expect(block).toBe(10);\n+});"
            },
            {
                "filename": "tsconfig.json",
                "additions": 109,
                "deletions": 0,
                "patch": "@@ -0,0 +1,109 @@\n+{\n+  \"compilerOptions\": {\n+    /* Visit https://aka.ms/tsconfig to read more about this file */\n+\n+    /* Projects */\n+    // \"incremental\": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */\n+    // \"composite\": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */\n+    // \"tsBuildInfoFile\": \"./.tsbuildinfo\",              /* Specify the path to .tsbuildinfo incremental compilation file. */\n+    // \"disableSourceOfProjectReferenceRedirect\": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */\n+    // \"disableSolutionSearching\": true,                 /* Opt a project out of multi-project reference checking when editing. */\n+    // \"disableReferencedProjectLoad\": true,             /* Reduce the number of projects loaded automatically by TypeScript. */\n+\n+    /* Language and Environment */\n+    \"target\": \"ESNext\",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */\n+    // \"lib\": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */\n+    // \"jsx\": \"preserve\",                                /* Specify what JSX code is generated. */\n+    // \"experimentalDecorators\": true,                   /* Enable experimental support for legacy experimental decorators. */\n+    // \"emitDecoratorMetadata\": true,                    /* Emit design-type metadata for decorated declarations in source files. */\n+    // \"jsxFactory\": \"\",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */\n+    // \"jsxFragmentFactory\": \"\",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */\n+    // \"jsxImportSource\": \"\",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */\n+    // \"reactNamespace\": \"\",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */\n+    // \"noLib\": true,                                    /* Disable including any library files, including the default lib.d.ts. */\n+    // \"useDefineForClassFields\": true,                  /* Emit ECMAScript-standard-compliant class fields. */\n+    // \"moduleDetection\": \"auto\",                        /* Control what method is used to detect module-format JS files. */\n+\n+    /* Modules */\n+    \"module\": \"ESNext\",                                /* Specify what module code is generated. */\n+    // \"rootDir\": \"./\",                                  /* Specify the root folder within your source files. */\n+    \"moduleResolution\": \"node\",                     /* Specify how TypeScript looks up a file from a given module specifier. */\n+    // \"baseUrl\": \"./\",                                  /* Specify the base directory to resolve non-relative module names. */\n+    // \"paths\": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */\n+    // \"rootDirs\": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */\n+    // \"typeRoots\": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */\n+    // \"types\": [],                                      /* Specify type package names to be included without being referenced in a source file. */\n+    // \"allowUmdGlobalAccess\": true,                     /* Allow accessing UMD globals from modules. */\n+    // \"moduleSuffixes\": [],                             /* List of file name suffixes to search when resolving a module. */\n+    \"allowImportingTsExtensions\": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */\n+    // \"resolvePackageJsonExports\": true,                /* Use the package.json 'exports' field when resolving package imports. */\n+    // \"resolvePackageJsonImports\": true,                /* Use the package.json 'imports' field when resolving imports. */\n+    // \"customConditions\": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */\n+    // \"resolveJsonModule\": true,                        /* Enable importing .json files. */\n+    // \"allowArbitraryExtensions\": true,                 /* Enable importing files with any extension, provided a declaration file is present. */\n+    // \"noResolve\": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */\n+\n+    /* JavaScript Support */\n+    \"allowJs\": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */\n+    // \"checkJs\": true,                                  /* Enable error reporting in type-checked JavaScript files. */\n+    // \"maxNodeModuleJsDepth\": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */\n+\n+    /* Emit */\n+    // \"declaration\": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */\n+    // \"declarationMap\": true,                           /* Create sourcemaps for d.ts files. */\n+    // \"emitDeclarationOnly\": true,                      /* Only output d.ts files and not JavaScript files. */\n+    \"sourceMap\": true,                                /* Create source map files for emitted JavaScript files. */\n+    // \"inlineSourceMap\": true,                          /* Include sourcemap files inside the emitted JavaScript. */\n+    // \"outFile\": \"./\",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */\n+    // \"outDir\": \"./\",                                   /* Specify an output folder for all emitted files. */\n+    // \"removeComments\": true,                           /* Disable emitting comments. */\n+    // \"noEmit\": true,                                   /* Disable emitting files from a compilation. */\n+    // \"importHelpers\": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */\n+    // \"importsNotUsedAsValues\": \"remove\",               /* Specify emit/checking behavior for imports that are only used for types. */\n+    // \"downlevelIteration\": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */\n+    // \"sourceRoot\": \"\",                                 /* Specify the root path for debuggers to find the reference source code. */\n+    // \"mapRoot\": \"\",                                    /* Specify the location where debugger should locate map files instead of generated locations. */\n+    // \"inlineSources\": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */\n+    // \"emitBOM\": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */\n+    // \"newLine\": \"crlf\",                                /* Set the newline character for emitting files. */\n+    // \"stripInternal\": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */\n+    // \"noEmitHelpers\": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */\n+    // \"noEmitOnError\": true,                            /* Disable emitting files if any type checking errors are reported. */\n+    // \"preserveConstEnums\": true,                       /* Disable erasing 'const enum' declarations in generated code. */\n+    // \"declarationDir\": \"./\",                           /* Specify the output directory for generated declaration files. */\n+    // \"preserveValueImports\": true,                     /* Preserve unused imported values in the JavaScript output that would otherwise be removed. */\n+\n+    /* Interop Constraints */\n+    // \"isolatedModules\": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */\n+    // \"verbatimModuleSyntax\": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */\n+    // \"allowSyntheticDefaultImports\": true,             /* Allow 'import x from y' when a module doesn't have a default export. */\n+    \"esModuleInterop\": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */\n+    // \"preserveSymlinks\": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */\n+    \"forceConsistentCasingInFileNames\": true,            /* Ensure that casing is correct in imports. */\n+\n+    /* Type Checking */\n+    \"strict\": false,                                      /* Enable all strict type-checking options. */\n+    // \"noImplicitAny\": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */\n+    // \"strictNullChecks\": true,                         /* When type checking, take into account 'null' and 'undefined'. */\n+    // \"strictFunctionTypes\": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */\n+    // \"strictBindCallApply\": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */\n+    // \"strictPropertyInitialization\": true,             /* Check for class properties that are declared but not set in the constructor. */\n+    // \"noImplicitThis\": true,                           /* Enable error reporting when 'this' is given the type 'any'. */\n+    // \"useUnknownInCatchVariables\": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */\n+    // \"alwaysStrict\": true,                             /* Ensure 'use strict' is always emitted. */\n+    // \"noUnusedLocals\": true,                           /* Enable error reporting when local variables aren't read. */\n+    // \"noUnusedParameters\": true,                       /* Raise an error when a function parameter isn't read. */\n+    // \"exactOptionalPropertyTypes\": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */\n+    // \"noImplicitReturns\": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */\n+    // \"noFallthroughCasesInSwitch\": true,               /* Enable error reporting for fallthrough cases in switch statements. */\n+    // \"noUncheckedIndexedAccess\": true,                 /* Add 'undefined' to a type when accessed using an index. */\n+    // \"noImplicitOverride\": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */\n+    // \"noPropertyAccessFromIndexSignature\": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */\n+    // \"allowUnusedLabels\": true,                        /* Disable error reporting for unused labels. */\n+    // \"allowUnreachableCode\": true,                     /* Disable error reporting for unreachable code. */\n+\n+    /* Completeness */\n+    // \"skipDefaultLibCheck\": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */\n+    \"skipLibCheck\": true                                 /* Skip type checking all .d.ts files. */\n+  }\n+}"
            },
            {
                "filename": "yarn.lock",
                "additions": 2596,
                "deletions": 812,
                "patch": null
            }
        ]
    },
    {
        "sha": "4ed14668247620c7e6017f59093ec8a4effd289c",
        "author": "kentkolze",
        "date": "2024-01-11 16:57:05+00:00",
        "message": "add the logic for only processing up to maxEvents at a time",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 36,
                "deletions": 7,
                "patch": "@@ -52,7 +52,8 @@ const txMapper = {\n \n // Map(event.type => handler) derived from the above\n const txFuncs = new Map(\n-    Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_, [name, _1, handleFunc]]) => [name, handleFunc])\n+    // @ts-ignore\n+    Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_x, [name, _y, handleFunc]]) => [name, handleFunc])\n );\n \n const sleep = (delay) => new Promise((resolve) => setTimeout(resolve, delay));\n@@ -71,9 +72,12 @@ export const startSynchronousEventProcessing = async () => {\n             }\n         }\n     }\n-}\n+};\n \n-const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBlocks = 250, maxEvents = 100) => {\n+const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBlocks = 1500, maxEvents = 250) => {\n+    /*\n+    We process up to `maxEvents` across `maxBlocks` to ensure our transaction sizes dont get too big and bog down our db\n+    */\n     console.log(\" processEvents for issuer\", issuer);\n     let {_id: issuerId, last_processed_block: startBlock, tx_hash: deployedTx} = issuer;\n     if (startBlock === null) {\n@@ -85,7 +89,7 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBl\n         startBlock = await bootstrapTable(issuerId, receipt.blockNumber, contract, dbConn);\n     }\n     const {number: latestBlock} = await provider.getBlock('finalized');\n-    const endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n+    let endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n \n     let events: any[] = [];\n \n@@ -126,12 +130,13 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBl\n \n     // Process in the correct order\n     events.sort((a, b) => a.blockNumber - b.blockNumber || a.transactionIndex - b.transactionIndex);\n+    [events, endBlock] = trimEvents(events, maxEvents, endBlock);\n \n     await withGlobalTransaction(async () => {\n         await persistEvents(issuerId, events);\n         await updateLastProcessed(issuerId, endBlock);\n     }, dbConn);\n-}\n+};\n \n const bootstrapTable = async (issuerId, deployedBlockNumber, contract, dbConn) => {\n     // TODO: fix the copy-pasted query\n@@ -150,14 +155,15 @@ const bootstrapTable = async (issuerId, deployedBlockNumber, contract, dbConn) =\n     }, dbConn);\n \n     return tMinusOne;\n-}\n+};\n \n const persistEvents = async (issuerId, events) => {\n     // Persist all the necessary changes for each event gathered in process events\n     for (const event of events) {\n         const txHandleFunc = txFuncs.get(event.type);\n         console.log(\"persistEvent: \", event);\n         if (txHandleFunc) {\n+            // @ts-ignore\n             await txHandleFunc(event.data, issuerId, event.timestamp);\n             continue;\n         }\n@@ -168,7 +174,30 @@ const persistEvents = async (issuerId, events) => {\n         }\n         throw new Error(`Invalid transaction type: \"${event.type}\" for ${event}`);\n     }\n-}\n+};\n+\n+const trimEvents = (events, maxEvents, endBlock) => {\n+    let index = 0;    \n+    while (index < maxEvents && index < events.length) {\n+        // Iterate through the entire next block\n+        const includeBlock = events[index].blockNumber;\n+        index++;\n+        while (index < events.length && events[index].blockNumber === includeBlock) {\n+            index++;\n+        }\n+    }\n+\n+    // Nothing to trim!\n+    if (index >= (events.length - 1)) {\n+        return [events, endBlock];\n+    }\n+\n+    // Trim up to index (exclusive)\n+    // We processed up to the last events' blockNumber\n+    const useEvents = [...events.slice(0, index)];\n+    return [useEvents, trimEvents[useEvents.length - 1].blockNumber];\n+};\n+\n \n const updateLastProcessed = async (issuerId, lastProcessedBlock) => {\n     return updateIssuerById(issuerId, {lastProcessedBlock});"
            }
        ]
    },
    {
        "sha": "4d9af984437295d5444dc03e4e1ac22639144ac7",
        "author": "kentkolze",
        "date": "2024-01-11 15:56:00+00:00",
        "message": "first checkin of converting event based processing to poll based processing using transactions",
        "files": [
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -49,6 +49,7 @@ async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n         provider,\n         address: latestCapTableProxyContractAddress,\n         libraries,\n+        deployHash: tx.hash,\n     };\n }\n "
            },
            {
                "filename": "src/chain-operations/transactionListener.js",
                "additions": 21,
                "deletions": 15,
                "patch": "@@ -1,18 +1,11 @@\n-import { verifyIssuerAndSeed } from \"./seed.js\";\n-import {\n-    handleStockCancellation,\n-    handleIssuerAuthorizedSharesAdjusted,\n-    handleStockAcceptance,\n-    handleStockReissuance,\n-    handleStockRepurchase,\n-    handleStockRetraction,\n-    handleStockClass,\n-    handleStakeholder,\n-    handleStockIssuance,\n-    handleStockTransfer,\n-    handleStockClassAuthorizedSharesAdjusted,\n-} from \"./transactionHandlers.js\";\n+/*\n+DEPRECATED! DO NOT USE\n+TODO: delete \n+*/\n+\n+\n import { AbiCoder } from \"ethers\";\n+import { verifyIssuerAndSeed } from \"./seed.js\";\n import {\n     IssuerAuthorizedSharesAdjustment,\n     StockAcceptance,\n@@ -24,6 +17,19 @@ import {\n     StockRetraction,\n     StockTransfer,\n } from \"./structs.js\";\n+import {\n+    handleIssuerAuthorizedSharesAdjusted,\n+    handleStakeholder,\n+    handleStockAcceptance,\n+    handleStockCancellation,\n+    handleStockClass,\n+    handleStockClassAuthorizedSharesAdjusted,\n+    handleStockIssuance,\n+    handleStockReissuance,\n+    handleStockRepurchase,\n+    handleStockRetraction,\n+    handleStockTransfer,\n+} from \"./transactionHandlers.js\";\n \n const abiCoder = new AbiCoder();\n const eventQueue = [];\n@@ -120,4 +126,4 @@ async function processEventQueue() {\n     }\n }\n \n-export default startOnchainListeners;\n+// export default startOnchainListeners;"
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 175,
                "deletions": 0,
                "patch": "@@ -0,0 +1,175 @@\n+import { AbiCoder } from \"ethers\";\n+import connectDB from \"../db/config/mongoose.js\";\n+import { withGlobalTransaction } from \"../db/operations/atomic.js\";\n+import { readAllIssuers } from \"../db/operations/read.js\";\n+import { updateIssuerById } from \"../db/operations/update.js\";\n+import { getIssuerContract } from \"../utils/caches.js\";\n+import { verifyIssuerAndSeed } from \"./seed.js\";\n+import {\n+    IssuerAuthorizedSharesAdjustment,\n+    StockAcceptance,\n+    StockCancellation,\n+    StockClassAuthorizedSharesAdjustment,\n+    StockIssuance,\n+    StockReissuance,\n+    StockRepurchase,\n+    StockRetraction,\n+    StockTransfer,\n+} from \"./structs.js\";\n+import {\n+    handleIssuerAuthorizedSharesAdjusted,\n+    handleStakeholder,\n+    handleStockAcceptance,\n+    handleStockCancellation,\n+    handleStockClass,\n+    handleStockClassAuthorizedSharesAdjusted,\n+    handleStockIssuance,\n+    handleStockReissuance,\n+    handleStockRepurchase,\n+    handleStockRetraction,\n+    handleStockTransfer,\n+} from \"./transactionHandlers.js\";\n+\n+const abiCoder = new AbiCoder();\n+\n+const contractFuncs = new Map([\n+    [\"StakeholderCreated\", handleStakeholder],\n+    [\"StockClassCreated\", handleStockClass],\n+]);\n+\n+const txMapper = {\n+    0: [\"INVALID\"],\n+    1: [\"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\", IssuerAuthorizedSharesAdjustment, handleIssuerAuthorizedSharesAdjusted],\n+    2: [\"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\", StockClassAuthorizedSharesAdjustment, handleStockClassAuthorizedSharesAdjusted],\n+    3: [\"STOCK_ACCEPTANCE\", StockAcceptance, handleStockAcceptance],\n+    4: [\"STOCK_CANCELLATION\", StockCancellation, handleStockCancellation],\n+    5: [\"STOCK_ISSUANCE\", StockIssuance, handleStockIssuance],\n+    6: [\"STOCK_REISSUANCE\", StockReissuance, handleStockReissuance],\n+    7: [\"STOCK_REPURCHASE\", StockRepurchase, handleStockRepurchase],\n+    8: [\"STOCK_RETRACTION\", StockRetraction, handleStockRetraction],\n+    9: [\"STOCK_TRANSFER\", StockTransfer, handleStockTransfer],\n+};\n+\n+// Map(event.type => handler) derived from the above\n+const txFuncs = new Map(\n+    Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_, [name, _1, handleFunc]]) => [name, handleFunc])\n+);\n+\n+const sleep = (delay) => new Promise((resolve) => setTimeout(resolve, delay));\n+\n+export const startSynchronousEventProcessing = async () => {\n+    while (true) {\n+        await sleep(1 * 1000);\n+        const issuers = await readAllIssuers();\n+        const dbConn = await connectDB();\n+        // Process events synchronously for each issuer\n+        console.log(`Processing for ${issuers.length} issuers`);\n+        for (const issuer of issuers) {\n+            if (issuer.deployed_to) {\n+                const { contract, provider, libraries } = await getIssuerContract(issuer);\n+                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper);\n+            }\n+        }\n+    }\n+}\n+\n+const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBlocks = 250, maxEvents = 100) => {\n+    console.log(\" processEvents for issuer\", issuer);\n+    let {_id: issuerId, last_processed_block: startBlock, tx_hash: deployedTx} = issuer;\n+    if (startBlock === null) {\n+        const receipt = await provider.getTransactionReceipt(deployedTx);\n+        if (!receipt) {\n+            console.error(\"Transaction receipt not found\");\n+            return;\n+        }\n+        startBlock = await bootstrapTable(issuerId, receipt.blockNumber, contract, dbConn);\n+    }\n+    const {number: latestBlock} = await provider.getBlock('finalized');\n+    const endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n+\n+    let events: any[] = [];\n+\n+    // TODO: fix filter\n+    const contractEvents = await contract.queryFilter(\"*\", startBlock, endBlock);\n+    for (const event of contractEvents) {\n+        if (contractFuncs.has(event.type)) {\n+            // TODO: how to deserialize event.data?\n+            console.log(\"contract event: \", event);\n+            events.push(event);\n+        }\n+    }\n+\n+    // TODO: fix filter\n+    const txEvents = await txHelper.queryFilter(\"*\", startBlock, endBlock);\n+    for (const event of txEvents) {\n+        // TODO: the same processing as libraries.txHelper.on     \n+        // TODO:  emit TxCreated(transactions.length, txType, txData);\n+        //  how do we parse the event.data string of each event? \n+        //    https://www.npmjs.com/package/@ethersproject/abstract-provider?activeTab=code (line 102: Log.data is string-type)\n+        console.log(\"txHelper event: \", event);\n+        if (event.removed) {\n+            continue;\n+        }\n+        // TODO: does txTypeIdx even come with the event??? need to test this...\n+        const [type, structType] = txMapper[txTypeIdx];\n+        const decodedData = abiCoder.decode([structType], txData);\n+        const { timestamp } = await provider.getBlock(event.blockNumber);\n+        // TODO: I think the below needs a lot of work\n+        events.push({ ...event, type, timestamp, data: decodedData[0] });\n+    }\n+\n+    // Nothing to process\n+    if (events.length === 0) {\n+        await updateLastProcessed(issuerId, endBlock);\n+        return;\n+    }\n+\n+    // Process in the correct order\n+    events.sort((a, b) => a.blockNumber - b.blockNumber || a.transactionIndex - b.transactionIndex);\n+\n+    await withGlobalTransaction(async () => {\n+        await persistEvents(issuerId, events);\n+        await updateLastProcessed(issuerId, endBlock);\n+    }, dbConn);\n+}\n+\n+const bootstrapTable = async (issuerId, deployedBlockNumber, contract, dbConn) => {\n+    // TODO: fix the copy-pasted query\n+    const issuerCreatedFilter = contract.filters.IssuerCreated;\n+    const issuerEvents = await contract.queryFilter(issuerCreatedFilter);\n+    if (issuerEvents.length === 0) {\n+        throw new Error(`No issuer events found!`);\n+    }\n+    const issuerCreatedEventId = issuerEvents[0].args[0];\n+    console.log(\"IssuerCreated Event Emitted!\", issuerCreatedEventId);\n+    const tMinusOne = deployedBlockNumber - 1;\n+    \n+    await withGlobalTransaction(async () => {\n+        await verifyIssuerAndSeed(contract, issuerCreatedEventId);\n+        await updateLastProcessed(issuerId, tMinusOne);\n+    }, dbConn);\n+\n+    return tMinusOne;\n+}\n+\n+const persistEvents = async (issuerId, events) => {\n+    // Persist all the necessary changes for each event gathered in process events\n+    for (const event of events) {\n+        const txHandleFunc = txFuncs.get(event.type);\n+        console.log(\"persistEvent: \", event);\n+        if (txHandleFunc) {\n+            await txHandleFunc(event.data, issuerId, event.timestamp);\n+            continue;\n+        }\n+        const contractHandleFunc = contractFuncs.get(event.type);\n+        if (contractHandleFunc) {\n+            await contractHandleFunc(event.data);\n+            continue;\n+        }\n+        throw new Error(`Invalid transaction type: \"${event.type}\" for ${event}`);\n+    }\n+}\n+\n+const updateLastProcessed = async (issuerId, lastProcessedBlock) => {\n+    return updateIssuerById(issuerId, {lastProcessedBlock});\n+};"
            },
            {
                "filename": "src/db/config/mongoose.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -9,6 +9,7 @@ const connectDB = async () => {\n     try {\n         await mongoose.connect(DATABASE_URL);\n         console.log(\"\u2705 | Mongo connected succesfully\");\n+        return mongoose.connection;\n     } catch (error) {\n         console.error(\"\u274c | Error connecting to Mongo\", error.message);\n         // Exit process with failure"
            },
            {
                "filename": "src/db/objects/Issuer.js",
                "additions": 2,
                "deletions": 0,
                "patch": "@@ -17,6 +17,8 @@ const IssuerSchema = new mongoose.Schema({\n     initial_shares_authorized: String,\n     comments: [String],\n     deployed_to: String,\n+    tx_hash: String,\n+    last_processed_block: { type: Number, default: null },\n     is_manifest_created: { type: Boolean, default: false },\n }, { timestamps: true });\n "
            },
            {
                "filename": "src/db/operations/atomic.ts",
                "additions": 87,
                "deletions": 0,
                "patch": "@@ -0,0 +1,87 @@\n+// Store a global mongo session to allows us to bundle CRUD operations into one transaction\n+\n+import { Connection, QueryOptions } from \"mongoose\";\n+import connectDB from \"../config/mongoose\";\n+type TQueryOptions = QueryOptions | null;\n+\n+let _globalSession = null;\n+\n+\n+export const setGlobalSession = (session) => {\n+    if (_globalSession !== null) {\n+        throw new Error(\n+            `globalSession is already set! ${_globalSession}. \n+            Nested transactions are not supported`\n+        );\n+    }\n+    _globalSession = session;\n+}\n+\n+export const clearGlobalSession = () => {\n+    _globalSession = null;\n+}\n+\n+export const withGlobalTransaction = async (func: () => Promise<void>, useConn?: Connection) => {\n+    // Wrap a user defined `func` in a global transaction\n+    const db = useConn || await connectDB();\n+    await db.transaction(async (session) => {\n+        setGlobalSession(session);\n+        try {\n+            return await func();\n+        } finally {\n+            clearGlobalSession();\n+        }\n+    });\n+}\n+\n+\n+const includeSession = (options?: TQueryOptions) => {\n+    let useOptions = options || {};\n+    if (!_globalSession) {\n+        if (useOptions.session) {\n+            throw new Error(`options.session is already set!: ${useOptions}`);\n+        }\n+        useOptions.session = _globalSession;\n+    }\n+    return useOptions;\n+}\n+\n+/* \n+Wrapped mongoose db calls. All mongo interaction should go through a function below\n+*/\n+\n+// CREATE\n+\n+export const save = (model, options?: TQueryOptions) => {\n+    return model.save(includeSession(options));\n+}\n+\n+// UPDATE\n+\n+export const findByIdAndUpdate = (model, id, updatedData, options?: TQueryOptions) => {\n+    return model.findByIdAndUpdate(id, updatedData, includeSession(options));\n+}\n+\n+// DELETE\n+\n+export const findByIdAndDelete = (model, id, options?: TQueryOptions) => {\n+    return model.findByIdAndDelete(id, includeSession(options));\n+}\n+\n+// QUERY\n+\n+export const findById = (model, id, projection?, options?: TQueryOptions) => {\n+    return model.findById(id, projection, includeSession(options));\n+}\n+\n+export const findOne = (model, filter, projection?, options?: TQueryOptions) => {\n+    return model.findOne(filter, projection, includeSession(options));\n+}\n+\n+export const find = (model, filter, projection?, options?: TQueryOptions) => {\n+    return model.find(filter, projection, includeSession(options));\n+}\n+\n+export const countDocuments = (model, options?: TQueryOptions) => {\n+    return model.countDocuments(includeSession(options));\n+}"
            },
            {
                "filename": "src/db/operations/atomicity.js",
                "additions": 0,
                "deletions": 3,
                "patch": "@@ -1,3 +0,0 @@\n-/*\n-For database transactions to ensure each event is processed exactly once\n-*/\n\\ No newline at end of file"
            },
            {
                "filename": "src/db/operations/create.js",
                "additions": 16,
                "deletions": 27,
                "patch": "@@ -1,72 +1,61 @@\n+import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n+import ConvertibleIssuance from \"../objects/transactions/issuance/ConvertibleIssuance.js\";\n+import EquityCompensationIssuance from \"../objects/transactions/issuance/EquityCompensationIssuance.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n-import EquityCompensationIssuance from \"../objects/transactions/issuance/EquityCompensationIssuance.js\";\n-import ConvertibleIssuance from \"../objects/transactions/issuance/ConvertibleIssuance.js\";\n+import { save } from \"./atomic.js\";\n \n export const createIssuer = (issuerData) => {\n-    const issuer = new Issuer(issuerData);\n-    return issuer.save();\n+    return save(new Issuer(issuerData));\n };\n \n export const createStakeholder = (stakeholderData) => {\n-    const stakeholder = new Stakeholder(stakeholderData);\n-    return stakeholder.save();\n+    return save(new Stakeholder(stakeholderData));\n };\n \n export const createStockClass = (stockClassData) => {\n-    const stockClass = new StockClass(stockClassData);\n-    return stockClass.save();\n+    return save(new StockClass(stockClassData));\n };\n \n export const createStockLegendTemplate = (stockLegendTemplateData) => {\n-    const stockLegendTemplate = new StockLegendTemplate(stockLegendTemplateData);\n-    return stockLegendTemplate.save();\n+    return save(new StockLegendTemplate(stockLegendTemplateData));\n };\n \n export const createStockPlan = (stockPlanData) => {\n-    const stockPlan = new StockPlan(stockPlanData);\n-    return stockPlan.save();\n+    return save(new StockPlan(stockPlanData));\n };\n \n export const createValuation = (valuationData) => {\n-    const valuation = new Valuation(valuationData);\n-    return valuation.save();\n+    return save(new Valuation(valuationData));\n };\n \n export const createVestingTerms = (vestingTermsData) => {\n-    const vestingTerms = new VestingTerms(vestingTermsData);\n-    return vestingTerms.save();\n+    return save(new VestingTerms(vestingTermsData));\n };\n \n export const createHistoricalTransaction = (transactionHistoryData) => {\n-    const historicalTransaction = new HistoricalTransaction(transactionHistoryData);\n-    return historicalTransaction.save();\n+    return save(new HistoricalTransaction(transactionHistoryData));\n };\n \n export const createStockIssuance = (stockIssuanceData) => {\n-    const stockIssuance = new StockIssuance(stockIssuanceData);\n-    return stockIssuance.save();\n+    return save(new StockIssuance(stockIssuanceData));\n };\n \n export const createEquityCompensationIssuance = (issuanceData) => {\n-    const equityCompensationIssuance = new EquityCompensationIssuance(issuanceData);\n-    return equityCompensationIssuance.save();\n+    return save(new EquityCompensationIssuance(issuanceData));\n };\n \n export const createConvertibleIssuance = (issuanceData) => {\n-    const convertibleIssuance = new ConvertibleIssuance(issuanceData);\n-    return convertibleIssuance.save();\n+    return save(new ConvertibleIssuance(issuanceData));\n };\n \n export const createStockTransfer = (stockTransferData) => {\n-    const stockTransfer = new StockTransfer(stockTransferData);\n-    return stockTransfer.save();\n+    return save(new StockTransfer(stockTransferData));\n };"
            },
            {
                "filename": "src/db/operations/delete.js",
                "additions": 8,
                "deletions": 8,
                "patch": "@@ -10,33 +10,33 @@ import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n // TODO: since we're doing a time series db that's meant to be immutable, is this needed?\n \n export const deleteIssuerById = (issuerId) => {\n-    return Issuer.findByIdAndDelete(issuerId);\n+    return findByIdAndDelete(Issuer, issuerId);\n };\n \n export const deleteStakeholderById = (stakeholderId) => {\n-    return Stakeholder.findByIdAndDelete(stakeholderId);\n+    return findByIdAndDelete(Stakeholder, stakeholderId);\n };\n \n export const deleteStockClassById = (stockClassId) => {\n-    return StockClass.findByIdAndDelete(stockClassId);\n+    return findByIdAndDelete(StockClass, stockClassId);\n };\n \n export const deleteStockLegendTemplateById = (stockLegendTemplateId) => {\n-    return StockLegendTemplate.findByIdAndDelete(stockLegendTemplateId);\n+    return findByIdAndDelete(StockLegendTemplate, stockLegendTemplateId);\n };\n \n export const deleteStockPlanById = (stockPlanId) => {\n-    return StockPlan.findByIdAndDelete(stockPlanId);\n+    return findByIdAndDelete(StockPlan, stockPlanId);\n };\n \n export const deleteValuationById = (valuationId) => {\n-    return Valuation.findByIdAndDelete(valuationId);\n+    return findByIdAndDelete(Valuation, valuationId);\n };\n \n export const deleteVestingTermsById = (vestingTermsId) => {\n-    return VestingTerms.findByIdAndDelete(vestingTermsId);\n+    return findByIdAndDelete(VestingTerms, vestingTermsId);\n };\n \n export const deleteTransactionById = (transactionId) => {\n-    return StockIssuance.findByIdAndDelete(transactionId);\n+    return findByIdAndDelete(StockIssuance, transactionId);\n };"
            },
            {
                "filename": "src/db/operations/read.js",
                "additions": 25,
                "deletions": 40,
                "patch": "@@ -1,97 +1,84 @@\n+import Factory from \"../objects/Factory.js\";\n+import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import Factory from \"../objects/Factory.js\";\n+import { countDocuments, find, findById } from \"./atomic.js\";\n \n // READ By ID\n export const readIssuerById = async (id) => {\n-    const issuer = await Issuer.findById(id);\n-    return issuer;\n+    return await findById(Issuer, id);\n };\n \n export const readStakeholderById = async (id) => {\n-    const stakeholder = await Stakeholder.findById(id);\n-    return stakeholder;\n+    return await findById(Stakeholder, id);\n };\n \n export const readStockClassById = async (id) => {\n-    const stockClass = await StockClass.findById(id);\n-    return stockClass;\n+    return await findById(StockClass, id);\n };\n \n export const readStockLegendTemplateById = async (id) => {\n-    const stockLegendTemplate = await StockLegendTemplate.findById(id);\n-    return stockLegendTemplate;\n+    return await findById(StockLegendTemplate, id);\n };\n \n export const readStockPlanById = async (id) => {\n-    const stockPlan = await StockPlan.findById(id);\n-    return stockPlan;\n+    return await findById(StockPlan, id);\n };\n \n export const readValuationById = async (id) => {\n-    const valuation = await Valuation.findById(id);\n-    return valuation;\n+    return await findById(Valuation, id);\n };\n \n export const readVestingTermsById = async (id) => {\n-    const vestingTerms = await VestingTerms.findById(id);\n-    return vestingTerms;\n+    return await findById(VestingTerms, id);\n };\n \n+// READ Multiple\n export const readHistoricalTransactionByIssuerId = async (issuerId) => {\n-    const historicalTransactions = await HistoricalTransaction.find({ issuer: issuerId }).populate(\"transaction\");\n-    return historicalTransactions;\n+    return await find(HistoricalTransaction, { issuer: issuerId }).populate(\"transaction\");\n };\n \n // COUNT\n export const countIssuers = async () => {\n-    const totalIssuers = await Issuer.countDocuments();\n-    return totalIssuers;\n+    return await countDocuments(Issuer);\n };\n \n export const countStakeholders = async () => {\n-    const totalStakeholders = await Stakeholder.countDocuments();\n-    return totalStakeholders;\n+    return await countDocuments(Stakeholder);\n };\n \n export const countStockClasses = async () => {\n-    const totalStockClasses = await StockClass.countDocuments();\n-    return totalStockClasses;\n+    return await countDocuments(StockClass);\n };\n \n export const countStockLegendTemplates = async () => {\n-    const totalTemplates = await StockLegendTemplate.countDocuments();\n-    return totalTemplates;\n+    return await countDocuments(StockLegendTemplate);\n };\n \n export const countStockPlans = async () => {\n-    const totalStockPlans = await StockPlan.countDocuments();\n-    return totalStockPlans;\n+    return await countDocuments(StockPlan);\n };\n \n export const countValuations = async () => {\n-    const totalValuations = await Valuation.countDocuments();\n-    return totalValuations;\n+    return await countDocuments(Valuation);\n };\n \n export const countVestingTerms = async () => {\n-    const totalVestingTerms = await VestingTerms.countDocuments();\n-    return totalVestingTerms;\n+    return await countDocuments(VestingTerms);\n };\n \n export const getAllIssuerDataById = async (issuerId) => {\n-    const issuerStakeholders = await Stakeholder.find({ issuer: issuerId });\n-    const issuerStockClasses = await StockClass.find({ issuer: issuerId });\n-    const issuerStockIssuances = await StockIssuance.find({ issuer: issuerId });\n-    const issuerStockTransfers = await StockTransfer.find({ issuer: issuerId });\n+    const issuerStakeholders = await find(Stakeholder, { issuer: issuerId });\n+    const issuerStockClasses = await find(StockClass, { issuer: issuerId });\n+    const issuerStockIssuances = await find(StockIssuance, { issuer: issuerId });\n+    const issuerStockTransfers = await find(StockTransfer, { issuer: issuerId });\n \n     return {\n         stakeholders: issuerStakeholders,\n@@ -102,11 +89,9 @@ export const getAllIssuerDataById = async (issuerId) => {\n };\n \n export const readAllIssuers = async () => {\n-    const issuers = await Issuer.find();\n-    return issuers;\n+    return await find(Issuer);\n }\n \n export const readFactory = async () => {\n-    const factory = await Factory.find();\n-    return factory;\n+    return await find(Factory);\n }\n\\ No newline at end of file"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 24,
                "deletions": 38,
                "patch": "@@ -1,95 +1,81 @@\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n-import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n+import StockAcceptance from \"../objects/transactions/acceptance/StockAcceptance.js\";\n+import IssuerAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.js\";\n+import StockClassAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.js\";\n import StockCancellation from \"../objects/transactions/cancellation/StockCancellation.js\";\n-import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n+import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockReissuance from \"../objects/transactions/reissuance/StockReissuance.js\";\n import StockRepurchase from \"../objects/transactions/repurchase/StockRepurchase.js\";\n-import StockAcceptance from \"../objects/transactions/acceptance/StockAcceptance.js\";\n-import StockClassAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.js\";\n-import IssuerAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.js\";\n+import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n+import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n \n \n export const updateIssuerById = async (id, updatedData) => {\n-    const issuer = await Issuer.findByIdAndUpdate(id, updatedData, { new: true });\n-    return issuer;\n+    return await findByIdAndUpdate(Issuer, id, updatedData, { new: true });\n };\n \n export const updateStakeholderById = async (id, updatedData) => {\n-    const stakeholder = await Stakeholder.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stakeholder;\n+    return await findByIdAndUpdate(Stakeholder, id, updatedData, { new: true });\n };\n \n export const updateStockClassById = async (id, updatedData) => {\n-    const stockClass = await StockClass.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockClass;\n+    return await findByIdAndUpdate(StockClass, id, updatedData, { new: true });\n };\n \n export const updateStockLegendTemplateById = async (id, updatedData) => {\n-    const stockLegendTemplate = await StockLegendTemplate.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockLegendTemplate;\n+    return await findByIdAndUpdate(StockLegendTemplate, id, updatedData, { new: true });\n };\n \n export const updateStockPlanById = async (id, updatedData) => {\n-    const stockPlan = await StockPlan.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockPlan;\n+    return await findByIdAndUpdate(StockPlan, id, updatedData, { new: true });\n };\n \n export const updateValuationById = async (id, updatedData) => {\n-    const valuation = await Valuation.findByIdAndUpdate(id, updatedData, { new: true });\n-    return valuation;\n+    return await findByIdAndUpdate(Valuation, id, updatedData, { new: true });\n };\n \n export const updateVestingTermsById = async (id, updatedData) => {\n-    const vestingTerms = await VestingTerms.findByIdAndUpdate(id, updatedData, { new: true });\n-    return vestingTerms;\n+    return await findByIdAndUpdate(VestingTerms, id, updatedData, { new: true });\n };\n \n export const upsertStockIssuanceById = async (id, updatedData) => {\n-    const stockIssuance = await StockIssuance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockIssuance;\n+    return await findByIdAndUpdate(StockIssuance, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockTransferById = async (id, updatedData) => {\n-    const stockTransfer = await StockTransfer.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockTransfer;\n+    return await findByIdAndUpdate(StockTransfer, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockCancellationById = async (id, updatedData) => {\n-    const stockCancellation = await StockCancellation.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockCancellation;\n+    return await findByIdAndUpdate(StockCancellation, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockRetractionById = async (id, updatedData) => {\n-    const stockRetraction = await StockRetraction.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockRetraction;\n+    return await findByIdAndUpdate(StockRetraction, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockReissuanceById = async (id, updatedData) => {\n-    const stockReissuance = await StockReissuance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockReissuance;\n+    return await findByIdAndUpdate(StockReissuance, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockRepurchaseById = async (id, updatedData) => {\n-    const stockRepurchase = await StockRepurchase.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockRepurchase;\n+    return await findByIdAndUpdate(StockRepurchase, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockAcceptanceById = async (id, updatedData) => {\n-    const stockAcceptance = await StockAcceptance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockAcceptance;\n+    return await findByIdAndUpdate(StockAcceptance, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockClassAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await StockClassAuthorizedSharesAdjustment.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockClassAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true, returning: true });\n };\n-export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await IssuerAuthorizedSharesAdjustment.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n \n-}\n+export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n+    return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true, returning: true });\n+};"
            },
            {
                "filename": "src/routes/index.js",
                "additions": 3,
                "deletions": 11,
                "patch": "@@ -1,19 +1,16 @@\n import { Router } from \"express\";\n import deployCapTable from \"../chain-operations/deployCapTable.js\";\n+import { updateIssuerById } from \"../db/operations/update.js\";\n import seedDB from \"../db/scripts/seed.js\";\n-import { contractCache } from \"../utils/caches.js\";\n import { convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import processManifest from \"../utils/processManifest.js\";\n-import { updateIssuerById } from \"../db/operations/update.js\";\n-import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n \n const router = Router();\n \n router.get(\"/\", async (req, res) => {\n-    console.log(\"Welcome to TAP\")\n+    console.log(\"Welcome to TAP\");\n     res.status(200).send(`Welcome to the future of Transfer Agents \ud83d\udcb8`);\n-})\n-\n+});\n \n router.post(\"/mint-cap-table\", async (req, res) => {\n     try {\n@@ -25,11 +22,6 @@ router.post(\"/mint-cap-table\", async (req, res) => {\n         const { contract, address, provider, libraries } = await deployCapTable(issuerIdBytes16, issuer.legal_name, issuer.initial_shares_authorized);\n \n         const savedIssuerWithDeployedTo = await updateIssuerById(issuer._id, { deployed_to: address });\n-\n-        // add contract to the cache and start listener\n-        contractCache[issuer._id] = { contract, provider, libraries };\n-        await startOnchainListeners(contract, provider, issuer._id, libraries);\n-\n         res.status(200).send({ issuer: savedIssuerWithDeployedTo });\n     } catch (error) {\n         console.error(`error: ${error}`);"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 3,
                "deletions": 1,
                "patch": "@@ -56,7 +56,8 @@ issuer.post(\"/create\", async (req, res) => {\n \n         const issuerIdBytes16 = convertUUIDToBytes16(incomingIssuerToValidate.id);\n         console.log(\"\ud83d\udcbe | Issuer id in bytes16 \", issuerIdBytes16);\n-        const { contract, provider, address, libraries } = await deployCapTable(\n+        const { contract, provider, address, libraries, deployHash } = await deployCapTable(\n+            chain,\n             issuerIdBytes16,\n             incomingIssuerToValidate.legal_name,\n             incomingIssuerToValidate.initial_shares_authorized\n@@ -69,6 +70,7 @@ issuer.post(\"/create\", async (req, res) => {\n         const incomingIssuerForDB = {\n             ...incomingIssuerToValidate,\n             deployed_to: address,\n+            tx_hash: deployHash,\n         };\n \n         const issuer = await createIssuer(incomingIssuerForDB);"
            },
            {
                "filename": "src/server.js",
                "additions": 9,
                "deletions": 29,
                "patch": "@@ -4,8 +4,7 @@ config();\n \n import connectDB from \"./db/config/mongoose.js\";\n \n-import getContractInstance from \"./chain-operations/getContractInstances.js\";\n-import startOnchainListeners from \"./chain-operations/transactionListener.js\";\n+import startSynchronousEventProcessing from \"./chain-operations/transactionPoller.js\";\n \n // Routes\n import historicalTransactions from \"./routes/historicalTransactions.js\";\n@@ -19,8 +18,8 @@ import transactionRoutes from \"./routes/transactions.js\";\n import valuationRoutes from \"./routes/valuation.js\";\n import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n \n-import { readIssuerById, readAllIssuers } from \"./db/operations/read.js\";\n-import { contractCache } from \"./utils/caches.js\";\n+import { readIssuerById } from \"./db/operations/read.js\";\n+import { getIssuerContract } from \"./utils/caches.js\";\n \n const app = express();\n \n@@ -41,17 +40,9 @@ const contractMiddleware = async (req, res, next) => {\n     const issuer = await readIssuerById(req.body.issuerId);\n     if (!issuer) res.status(400).send(\"issuer not found \");\n \n-    // Check if contract instance already exists in cache\n-    if (!contractCache[req.body.issuerId]) {\n-        const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to);\n-        contractCache[req.body.issuerId] = { contract, provider, libraries };\n-\n-        // Initialize listener for this contract\n-        startOnchainListeners(contract, provider, req.body.issuerId, libraries);\n-    }\n-\n-    req.contract = contractCache[req.body.issuerId].contract;\n-    req.provider = contractCache[req.body.issuerId].provider;\n+    const { contract, provider } = await getIssuerContract(issuer);\n+    req.contract = contract;\n+    req.provider = provider;\n     next();\n };\n app.use(urlencoded({ limit: \"50mb\", extended: true }));\n@@ -73,18 +64,7 @@ app.use(\"/historical-transactions\", historicalTransactions);\n app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n \n app.listen(PORT, async () => {\n-    console.log(`\ud83d\ude80  Server successfully launched on port ${PORT}`);\n-     // Fetch all issuers\n-     const issuers = await readAllIssuers();\n-     if (issuers && issuers.length > 0) {\n-         for (const issuer of issuers) {\n-             if (issuer.deployed_to) {\n-                 // Create a new contract instance for each issuer\n-                 const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to);\n- \n-                 // Initialize listener for this contract\n-                 startOnchainListeners(contract, provider, issuer._id, libraries);\n-             }\n-         }\n-     }\n+    console.log(`\ud83d\ude80  Server successfully launched at: ${PORT}`);\n+    // Kick off asynchronous job to process changes\n+    startSynchronousEventProcessing();\n });"
            },
            {
                "filename": "src/utils/caches.js",
                "additions": 0,
                "deletions": 10,
                "patch": "@@ -1,10 +0,0 @@\n-// Centralized contract manager/cache\n-export const contractCache = {};\n-\n-/*\n-issuerId = {\n-        activePositions: {...},\n-        activeSecurityIdsByStockClass: {...},\n-    };\n-*/\n-export const preProcessorCache = {};"
            },
            {
                "filename": "src/utils/caches.ts",
                "additions": 32,
                "deletions": 0,
                "patch": "@@ -0,0 +1,32 @@\n+import getContractInstance from \"../chain-operations/getContractInstances\";\n+\n+const CHAIN = process.env.CHAIN;\n+\n+interface CachePayload {\n+    contract: any;\n+    provider: any;\n+    libraries: any;\n+}\n+\n+// Centralized contract manager/cache\n+const contractCache: {[key: string]: CachePayload} = {};\n+\n+const cacheIssuerContract = async (issuer, payload: CachePayload) => {\n+    contractCache[issuer._id] = payload;\n+}\n+\n+export const getIssuerContract = async (issuer) => {\n+    if (!contractCache[issuer._id]) {\n+        const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to); \n+        await cacheIssuerContract(issuer, { contract, provider, libraries });\n+    }\n+    return contractCache[issuer._id];\n+}\n+\n+/*\n+issuerId = {\n+        activePositions: {...},\n+        activeSecurityIdsByStockClass: {...},\n+    };\n+*/\n+export const preProcessorCache = {};"
            }
        ]
    },
    {
        "sha": "68268b80f990bb5034b5d1b7720572f6bb015270",
        "author": "kentkolze",
        "date": "2024-01-04 15:51:52+00:00",
        "message": "first push attempt",
        "files": [
            {
                "filename": "src/db/operations/atomicity.js",
                "additions": 3,
                "deletions": 0,
                "patch": "@@ -0,0 +1,3 @@\n+/*\n+For database transactions to ensure each event is processed exactly once\n+*/\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "14b5eaf110c32a3a517cddd87e231836796561ae",
        "author": "victormimo",
        "date": "2024-01-18 15:35:35+00:00",
        "message": "Merge pull request #123 from transfer-agent-protocol/vmimo/beta-new-issuances\n\nbugfix (fixing deployment)",
        "files": [
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 12,
                "deletions": 25,
                "patch": "@@ -2,56 +2,43 @@ import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import CAP_TABLE_FACTORY from \"../../chain/out/CapTableFactory.sol/CapTableFactory.json\" assert { type: \"json\" };\n+import { readFactory } from \"../db/operations/read.js\";\n import { toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n-import { readFactory } from \"../db/operations/read.js\";\n+import getProvider from \"./getProvider.js\";\n \n config();\n \n async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n-    const RPC_URL = process.env.RPC_URL;\n-    const CHAIN_ID = process.env.CHAIN_ID;\n-\n-    let customNetwork;\n-\n-    // Change the CHAIN_ID in the .env file to deploy to a different network\n-    if (RPC_URL === \"http://127.0.0.1:8545\") {\n-        customNetwork =  {\n-            chainId: parseInt(CHAIN_ID),\n-            name: \"local\"\n-        };\n-    } else {\n-        customNetwork =  {\n-            // Change the CHAIN_ID in the .env file to deploy to a different network\n-            chainId: parseInt(CHAIN_ID),\n-        };\n-    }\n \n-    const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n+    const provider = getProvider();\n+\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n \n     console.log(\"\ud83d\uddfd | Wallet address: \", wallet.address);\n \n     const factory = await readFactory();\n     const factoryAddress = factory[0].factory_address;\n \n-    console.log('factory ', factory)\n-    \n-    if(!factoryAddress) {\n+    console.log(\"factory \", factory);\n+\n+    if (!factoryAddress) {\n         throw new Error(`\u274c | Factory address not found`);\n     }\n-   \n+\n     const capTableFactory = new ethers.Contract(factoryAddress, CAP_TABLE_FACTORY.abi, wallet);\n \n     const tx = await capTableFactory.createCapTable(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));\n     await tx.wait();\n \n     const capTableCount = await capTableFactory.getCapTableCount();\n \n+    console.log(\"\ud83d\udcc4 | Cap table count: \", capTableCount);\n+\n     const latestCapTableProxyContractAddress = await capTableFactory.capTableProxies(capTableCount - BigInt(1));\n \n-    const contract = new ethers.Contract(latestCapTableProxyContractAddress, CAP_TABLE.abi, wallet); \n+    const contract = new ethers.Contract(latestCapTableProxyContractAddress, CAP_TABLE.abi, wallet);\n \n     console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n     console.log(\"\u2705 | Cap table contract address \", latestCapTableProxyContractAddress);\n@@ -65,4 +52,4 @@ async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     };\n }\n \n-export default deployCapTable;\n\\ No newline at end of file\n+export default deployCapTable;"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 2,
                "deletions": 17,
                "patch": "@@ -2,30 +2,15 @@ import { ethers } from \"ethers\";\n import { config } from \"dotenv\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n+import getProvider from \"./getProvider.js\";\n \n config();\n \n async function getContractInstance(address) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n-    const RPC_URL = process.env.RPC_URL;\n-    const CHAIN_ID = parseInt(process.env.CHAIN_ID); // Convert to integer\n \n-    let customNetwork;\n+    const provider = getProvider();\n \n-    // Change the CHAIN_ID in the .env file to deploy to a different network\n-    if (RPC_URL === \"http://127.0.0.1:8545\") {\n-        customNetwork =  {\n-            chainId: parseInt(CHAIN_ID),\n-            name: \"local\"\n-        };\n-    } else {\n-        customNetwork =  {\n-            // Change the CHAIN_ID in the .env file to deploy to a different network\n-            chainId: parseInt(CHAIN_ID),\n-        };\n-    }\n-\n-    const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n     const contract = new ethers.Contract(address, CAP_TABLE.abi, wallet);\n     const libraries = getTXLibContracts(contract.target, wallet);"
            },
            {
                "filename": "src/chain-operations/getProvider.js",
                "additions": 26,
                "deletions": 0,
                "patch": "@@ -0,0 +1,26 @@\n+import { config } from \"dotenv\";\n+import { ethers } from \"ethers\";\n+config();\n+\n+const RPC_URL = process.env.RPC_URL;\n+const CHAIN_ID = process.env.CHAIN_ID;\n+\n+const LOCAL_RPC = \"http://127.0.0.1:8545\";\n+\n+const getProvider = () => {\n+    let provider;\n+    if (RPC_URL === LOCAL_RPC) {\n+        console.log(\"\ud83d\udd17 | Connecting to local network: \", RPC_URL);\n+        const customNetwork = {\n+            chainId: parseInt(CHAIN_ID),\n+            name: \"local\",\n+        };\n+        provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n+    } else {\n+        console.log(\"\ud83d\udd17 | Connecting to network: \", RPC_URL);\n+        provider = new ethers.JsonRpcProvider(RPC_URL);\n+    }\n+    return provider;\n+};\n+\n+export default getProvider;"
            },
            {
                "filename": "src/db/operations/create.js",
                "additions": 12,
                "deletions": 0,
                "patch": "@@ -8,6 +8,8 @@ import VestingTerms from \"../objects/VestingTerms.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n+import EquityCompensationIssuance from \"../objects/transactions/issuance/EquityCompensationIssuance.js\";\n+import ConvertibleIssuance from \"../objects/transactions/issuance/ConvertibleIssuance.js\";\n \n export const createIssuer = (issuerData) => {\n     const issuer = new Issuer(issuerData);\n@@ -54,6 +56,16 @@ export const createStockIssuance = (stockIssuanceData) => {\n     return stockIssuance.save();\n };\n \n+export const createEquityCompensationIssuance = (issuanceData) => {\n+    const equityCompensationIssuance = new EquityCompensationIssuance(issuanceData);\n+    return equityCompensationIssuance.save();\n+};\n+\n+export const createConvertibleIssuance = (issuanceData) => {\n+    const convertibleIssuance = new ConvertibleIssuance(issuanceData);\n+    return convertibleIssuance.save();\n+};\n+\n export const createStockTransfer = (stockTransferData) => {\n     const stockTransfer = new StockTransfer(stockTransferData);\n     return stockTransfer.save();"
            },
            {
                "filename": "src/routes/index.js",
                "additions": 5,
                "deletions": 0,
                "patch": "@@ -9,6 +9,11 @@ import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n \n const router = Router();\n \n+router.get(\"/\", async (req, res) => {\n+    console.log(\"Welcome to TAP\")\n+    res.status(200).send(`Welcome to the future of Transfer Agents \ud83d\udcb8`);\n+})\n+\n \n router.post(\"/mint-cap-table\", async (req, res) => {\n     try {"
            },
            {
                "filename": "src/routes/transactions.js",
                "additions": 59,
                "deletions": 2,
                "patch": "@@ -9,6 +9,8 @@ import stockRepurchaseSchema from \"../../ocf/schema/objects/transactions/repurch\n import stockRetractionSchema from \"../../ocf/schema/objects/transactions/retraction/StockRetraction.schema.json\" assert { type: \"json\" };\n import stockClassAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n import issuerAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n+import equityCompensationIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/EquityCompensationIssuance.schema.json\" assert { type: \"json\" };\n+import convertibleIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/ConvertibleIssuance.schema.json\" assert { type: \"json\" };\n \n import { convertAndAdjustIssuerAuthorizedSharesOnChain } from \"../controllers/issuerController.js\";\n import { convertAndAdjustStockClassAuthorizedSharesOnchain } from \"../controllers/stockClassController.js\";\n@@ -19,6 +21,8 @@ import { convertAndCreateReissuanceStockOnchain } from \"../controllers/transacti\n import { convertAndCreateRepurchaseStockOnchain } from \"../controllers/transactions/repurchaseController.js\";\n import { convertAndCreateRetractionStockOnchain } from \"../controllers/transactions/retractionController.js\";\n import { convertAndCreateTransferStockOnchain } from \"../controllers/transactions/transferController.js\";\n+import { createEquityCompensationIssuance } from \"../db/operations/create.js\";\n+import { createConvertibleIssuance } from \"../db/operations/create.js\";\n \n import { readIssuerById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n@@ -272,8 +276,6 @@ transactions.post(\"/adjust/stock-class/authorized-shares\", async (req, res) => {\n \n         console.log(\"stockClassAuthorizedSharesAdjustment\", stockClassAuthorizedSharesAdjustment);\n \n-        // delete incomingStockClassAdjustment.stockClassId;\n-\n         // NOTE: schema validation does not include stakeholder, stockClassId, however these properties are needed on to be passed on chain\n         await validateInputAgainstOCF(stockClassAuthorizedSharesAdjustment, stockClassAuthorizedSharesAdjustmentSchema);\n \n@@ -289,4 +291,59 @@ transactions.post(\"/adjust/stock-class/authorized-shares\", async (req, res) => {\n     }\n });\n \n+transactions.post(\"/issuance/equity-compensation\", async (req, res) => {\n+    const { issuerId, data } = req.body;\n+\n+    try {\n+        // ensuring issuer exists\n+        await readIssuerById(issuerId);\n+\n+        const incomingEquityCompensationIssuance = {\n+            id: uuid(), // for OCF Validation\n+            security_id: uuid(), // for OCF Validation,\n+            date: new Date().toISOString().slice(0, 10), // for OCF Validation\n+            object_type: \"TX_EQUITY_COMPENSATION_ISSUANCE\",\n+            ...data,\n+        };\n+        await validateInputAgainstOCF(incomingEquityCompensationIssuance, equityCompensationIssuanceSchema);\n+\n+        // save to DB\n+        const createdIssuance = await createEquityCompensationIssuance(incomingEquityCompensationIssuance);\n+\n+        res.status(200).send({ equityCompensationIssuance: createdIssuance });\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+})\n+\n+\n+transactions.post(\"/issuance/convertible\", async (req, res) => {\n+    const { issuerId, data } = req.body;\n+\n+    try {\n+        // ensuring issuer exists\n+        await readIssuerById(issuerId);\n+\n+        const incomingConvertibleIssuance = {\n+            id: uuid(), // for OCF Validation\n+            security_id: uuid(), // for OCF Validation\n+            date: new Date().toISOString().slice(0, 10), // for OCF Validation\n+            object_type: \"TX_CONVERTIBLE_ISSUANCE\",\n+            ...data,\n+        };\n+\n+        console.log('incomingConvertibleIssuance', incomingConvertibleIssuance)\n+        await validateInputAgainstOCF(incomingConvertibleIssuance, convertibleIssuanceSchema);\n+\n+        // save to DB\n+        const createdIssuance = await createConvertibleIssuance(incomingConvertibleIssuance);\n+\n+        res.status(200).send({ convertibleIssuance: createdIssuance });\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+})\n+\n export default transactions;"
            },
            {
                "filename": "src/scripts/testMintingCapTable.js",
                "additions": 15,
                "deletions": 15,
                "patch": "@@ -10,32 +10,32 @@ const main = async () => {\n \n     console.log(\"\u2705 | Issuer response \", issuerResponse.data);\n \n-    // await sleep(3000);\n+    await sleep(3000);\n \n-    // console.log(\"\u23f3 | Creating first stakeholder\");\n+    console.log(\"\u23f3 | Creating first stakeholder\");\n \n-    // // create two stakeholders\n-    // const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerResponse.data.issuer._id));\n+    // create two stakeholders\n+    const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerResponse.data.issuer._id));\n \n-    // console.log(\"\u2705 | stakeholder1Response\", stakeholder1Response.data);\n-    // console.log(\"\u2705 | finished\");\n+    console.log(\"\u2705 | stakeholder1Response\", stakeholder1Response.data);\n+    console.log(\"\u2705 | finished\");\n \n-    // await sleep(3000);\n+    await sleep(3000);\n \n-    // console.log(\"\u23f3 | Creating second stakeholder\u2026\");\n+    console.log(\"\u23f3 | Creating second stakeholder\u2026\");\n \n-    // const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerResponse.data.issuer._id));\n+    const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerResponse.data.issuer._id));\n \n-    // console.log(\"\u2705 | stakeholder2Response\", stakeholder2Response.data);\n+    console.log(\"\u2705 | stakeholder2Response\", stakeholder2Response.data);\n \n-    // await sleep(3000);\n+    await sleep(3000);\n \n-    // console.log(\"\u23f3| Creating stock class\");\n+    console.log(\"\u23f3| Creating stock class\");\n \n-    // // create stockClass\n-    // const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerResponse.data.issuer._id));\n+    // create stockClass\n+    const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerResponse.data.issuer._id));\n \n-    // console.log(\"\u2705 | stockClassResponse\", stockClassResponse.data);\n+    console.log(\"\u2705 | stockClassResponse\", stockClassResponse.data);\n };\n \n main()"
            },
            {
                "filename": "src/server.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -73,7 +73,7 @@ app.use(\"/historical-transactions\", historicalTransactions);\n app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n \n app.listen(PORT, async () => {\n-    console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n+    console.log(`\ud83d\ude80  Server successfully launched on port ${PORT}`);\n      // Fetch all issuers\n      const issuers = await readAllIssuers();\n      if (issuers && issuers.length > 0) {"
            }
        ]
    },
    {
        "sha": "b60b2e16add93fb40a22b2a6941bbe08c17606ea",
        "author": "victormimo",
        "date": "2024-01-18 15:34:28+00:00",
        "message": "modulirizing provider getter",
        "files": [
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 13,
                "deletions": 25,
                "patch": "@@ -2,55 +2,43 @@ import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import CAP_TABLE_FACTORY from \"../../chain/out/CapTableFactory.sol/CapTableFactory.json\" assert { type: \"json\" };\n+import { readFactory } from \"../db/operations/read.js\";\n import { toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n-import { readFactory } from \"../db/operations/read.js\";\n+import getProvider from \"./getProvider.js\";\n \n config();\n \n async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n-    const RPC_URL = process.env.RPC_URL;\n-    const CHAIN_ID = process.env.CHAIN_ID;\n-\n-    let provider\n-\n-    // Change the CHAIN_ID in the .env file to deploy to a different network\n-    if (RPC_URL === \"http://127.0.0.1:8545\") {\n-        console.log(\"\ud83d\udd17 | Connecting to local network: \", RPC_URL)\n-       const  customNetwork =  {\n-            chainId: parseInt(CHAIN_ID),\n-            name: \"local\"\n-        };\n-         provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n-    } else {\n-            console.log(\"\ud83d\udd17 | Connecting to network: \", RPC_URL)\n-            provider = new ethers.JsonRpcProvider(RPC_URL);\n-    } \n-   \n+\n+    const provider = getProvider();\n+\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n \n     console.log(\"\ud83d\uddfd | Wallet address: \", wallet.address);\n \n     const factory = await readFactory();\n     const factoryAddress = factory[0].factory_address;\n \n-    console.log('factory ', factory)\n-    \n-    if(!factoryAddress) {\n+    console.log(\"factory \", factory);\n+\n+    if (!factoryAddress) {\n         throw new Error(`\u274c | Factory address not found`);\n     }\n-   \n+\n     const capTableFactory = new ethers.Contract(factoryAddress, CAP_TABLE_FACTORY.abi, wallet);\n \n     const tx = await capTableFactory.createCapTable(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));\n     await tx.wait();\n \n     const capTableCount = await capTableFactory.getCapTableCount();\n \n+    console.log(\"\ud83d\udcc4 | Cap table count: \", capTableCount);\n+\n     const latestCapTableProxyContractAddress = await capTableFactory.capTableProxies(capTableCount - BigInt(1));\n \n-    const contract = new ethers.Contract(latestCapTableProxyContractAddress, CAP_TABLE.abi, wallet); \n+    const contract = new ethers.Contract(latestCapTableProxyContractAddress, CAP_TABLE.abi, wallet);\n \n     console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n     console.log(\"\u2705 | Cap table contract address \", latestCapTableProxyContractAddress);\n@@ -64,4 +52,4 @@ async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     };\n }\n \n-export default deployCapTable;\n\\ No newline at end of file\n+export default deployCapTable;"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 2,
                "deletions": 16,
                "patch": "@@ -2,28 +2,14 @@ import { ethers } from \"ethers\";\n import { config } from \"dotenv\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n+import getProvider from \"./getProvider.js\";\n \n config();\n \n async function getContractInstance(address) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n-    const RPC_URL = process.env.RPC_URL;\n-    const CHAIN_ID = process.env.CHAIN_ID;\n \n-    let provider\n-\n-    // Change the CHAIN_ID in the .env file to deploy to a different network\n-    if (RPC_URL === \"http://127.0.0.1:8545\") {\n-        console.log(\"\ud83d\udd17 | Connecting to local network: \", RPC_URL)\n-       const  customNetwork =  {\n-            chainId: parseInt(CHAIN_ID),\n-            name: \"local\"\n-        };\n-         provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n-    } else {\n-            console.log(\"\ud83d\udd17 | Connecting to network: \", RPC_URL)\n-            provider = new ethers.JsonRpcProvider(RPC_URL);\n-    } \n+    const provider = getProvider();\n \n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n     const contract = new ethers.Contract(address, CAP_TABLE.abi, wallet);"
            },
            {
                "filename": "src/chain-operations/getProvider.js",
                "additions": 26,
                "deletions": 0,
                "patch": "@@ -0,0 +1,26 @@\n+import { config } from \"dotenv\";\n+import { ethers } from \"ethers\";\n+config();\n+\n+const RPC_URL = process.env.RPC_URL;\n+const CHAIN_ID = process.env.CHAIN_ID;\n+\n+const LOCAL_RPC = \"http://127.0.0.1:8545\";\n+\n+const getProvider = () => {\n+    let provider;\n+    if (RPC_URL === LOCAL_RPC) {\n+        console.log(\"\ud83d\udd17 | Connecting to local network: \", RPC_URL);\n+        const customNetwork = {\n+            chainId: parseInt(CHAIN_ID),\n+            name: \"local\",\n+        };\n+        provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n+    } else {\n+        console.log(\"\ud83d\udd17 | Connecting to network: \", RPC_URL);\n+        provider = new ethers.JsonRpcProvider(RPC_URL);\n+    }\n+    return provider;\n+};\n+\n+export default getProvider;"
            }
        ]
    },
    {
        "sha": "ff7779fd6fb670514902163f312e88db4a8aa9be",
        "author": "kentkolze",
        "date": "2024-01-17 19:46:27+00:00",
        "message": "final touches to get the latest snapshot of the cap table!",
        "files": [
            {
                "filename": "src/chain-operations/transactionListener.js",
                "additions": 0,
                "deletions": 129,
                "patch": "@@ -1,129 +0,0 @@\n-/*\n-DEPRECATED! DO NOT USE\n-TODO: delete \n-*/\n-\n-\n-import { AbiCoder } from \"ethers\";\n-import { verifyIssuerAndSeed } from \"./seed.js\";\n-import {\n-    IssuerAuthorizedSharesAdjustment,\n-    StockAcceptance,\n-    StockCancellation,\n-    StockClassAuthorizedSharesAdjustment,\n-    StockIssuance,\n-    StockReissuance,\n-    StockRepurchase,\n-    StockRetraction,\n-    StockTransfer,\n-} from \"./structs.js\";\n-import {\n-    handleIssuerAuthorizedSharesAdjusted,\n-    handleStakeholder,\n-    handleStockAcceptance,\n-    handleStockCancellation,\n-    handleStockClass,\n-    handleStockClassAuthorizedSharesAdjusted,\n-    handleStockIssuance,\n-    handleStockReissuance,\n-    handleStockRepurchase,\n-    handleStockRetraction,\n-    handleStockTransfer,\n-} from \"./transactionHandlers.js\";\n-\n-const abiCoder = new AbiCoder();\n-const eventQueue = [];\n-let issuerEventFired = false;\n-\n-const txMapper = {\n-    0: [\"INVALID\"],\n-    1: [\"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\", IssuerAuthorizedSharesAdjustment],\n-    2: [\"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\", StockClassAuthorizedSharesAdjustment],\n-    3: [\"STOCK_ACCEPTANCE\", StockAcceptance],\n-    4: [\"STOCK_CANCELLATION\", StockCancellation],\n-    5: [\"STOCK_ISSUANCE\", StockIssuance],\n-    6: [\"STOCK_REISSUANCE\", StockReissuance],\n-    7: [\"STOCK_REPURCHASE\", StockRepurchase],\n-    8: [\"STOCK_RETRACTION\", StockRetraction],\n-    9: [\"STOCK_TRANSFER\", StockTransfer],\n-};\n-\n-async function startOnchainListeners(contract, provider, issuerId, libraries) {\n-    console.log(\"\ud83c\udf10 | Initiating on-chain event listeners for issuer\", issuerId, \"at address\", contract.target);\n-\n-    libraries.txHelper.on(\"TxCreated\", async (_, txTypeIdx, txData, event) => {\n-        const [type, structType] = txMapper[txTypeIdx];\n-        const decodedData = abiCoder.decode([structType], txData);\n-        const { timestamp } = await provider.getBlock(event.blockNumber);\n-        eventQueue.push({ type, data: decodedData[0], issuerId, timestamp });\n-    });\n-\n-    contract.on(\"StakeholderCreated\", async (id, _) => {\n-        eventQueue.push({ type: \"STAKEHOLDER_CREATED\", data: id });\n-    });\n-\n-    contract.on(\"StockClassCreated\", async (id, _) => {\n-        eventQueue.push({ type: \"STOCK_CLASS_CREATED\", data: id });\n-    });\n-\n-    const issuerCreatedFilter = contract.filters.IssuerCreated;\n-    const issuerEvents = await contract.queryFilter(issuerCreatedFilter);\n-\n-    if (issuerEvents.length > 0 && !issuerEventFired) {\n-        const id = issuerEvents[0].args[0];\n-        console.log(\"IssuerCreated Event Emitted!\", id);\n-\n-        await verifyIssuerAndSeed(contract, id);\n-        issuerEventFired = true;\n-    }\n-\n-    setInterval(processEventQueue, 5000); // Process every 5 seconds\n-}\n-\n-async function processEventQueue() {\n-    const sortedEventQueue = eventQueue.sort((a, b) => a.timestamp - b.timestamp);\n-    while (sortedEventQueue.length > 0) {\n-        const event = eventQueue[0];\n-        switch (event.type) {\n-            case \"STAKEHOLDER_CREATED\":\n-                await handleStakeholder(event.data);\n-                break;\n-            case \"STOCK_CLASS_CREATED\":\n-                await handleStockClass(event.data);\n-                break;\n-            case \"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\":\n-                await handleIssuerAuthorizedSharesAdjusted(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\":\n-                await handleStockClassAuthorizedSharesAdjusted(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_ACCEPTANCE\":\n-                await handleStockAcceptance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_CANCELLATION\":\n-                await handleStockCancellation(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_ISSUANCE\":\n-                await handleStockIssuance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_REISSUANCE\":\n-                await handleStockReissuance(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_REPURCHASE\":\n-                await handleStockRepurchase(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_RETRACTION\":\n-                await handleStockRetraction(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"STOCK_TRANSFER\":\n-                await handleStockTransfer(event.data, event.issuerId, event.timestamp);\n-                break;\n-            case \"INVALID\":\n-                throw new Error(\"Invalid transaction type\");\n-                break;\n-        }\n-        sortedEventQueue.shift();\n-    }\n-}\n-\n-// export default startOnchainListeners;"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 62,
                "deletions": 27,
                "patch": "@@ -4,7 +4,8 @@ import Stakeholder from \"../db/objects/Stakeholder\";\n import StockClass from \"../db/objects/StockClass\";\n import { StockIssuance } from \"../db/objects/transactions/issuance\";\n import { getIssuerContract } from \"../utils/caches\";\n-import { convertBytes16ToUUID, convertUUIDToBytes16 } from \"../utils/convertUUID\";\n+import { decimalScaleValue } from \"../utils/convertToFixedPointDecimals\";\n+import { convertUUIDToBytes16 } from \"../utils/convertUUID\";\n \n export const capTable = Router();\n \n@@ -13,40 +14,74 @@ capTable.get(\"/\", async (req, res) => {\n });\n \n capTable.get(\"/latest\", async (req, res) => {\n+    /* \n+    TODO: handle this in the polling process? or maybe just cache it once in a while?\n+     It will get slow once we have 50+ stakeholders\n+    */\n     const issuerId = req.query.issuerId;\n     try {\n         const stakeholders = await Stakeholder.find({issuer: issuerId});\n-        const issuances = await StockIssuance.find({issuer: issuerId});\n         const stockClasses = await StockClass.find({issuer: issuerId});\n         const issuer = await Issuer.findById(issuerId);\n-        const {contract} = await getIssuerContract(issuer);\n-        \n-        // TODO: switch to array of promises for speed\n+        // Grouping by stakeholder_id and stock_class_id, grab the records with the largest createdAt time\n+        const issuances = await StockIssuance.aggregate([\n+            {\n+                $group: {\n+                    _id: {\n+                        stakeholder_id: \"$stakeholder_id\",\n+                        stock_class_id: \"$stock_class_id\"\n+                    },\n+                    maxDate: { $max: \"$createdAt\" }\n+                }\n+            },\n+            {\n+                $lookup: {\n+                    from: \"stockissuances\",\n+                    let: { stakeholder_id: \"$_id.stakeholder_id\", stock_class_id: \"$_id.stock_class_id\", maxDate: \"$maxDate\" },\n+                    pipeline: [\n+                        {\n+                            $match: {\n+                                $expr: {\n+                                    $and: [\n+                                        { $eq: [\"$stakeholder_id\", \"$$stakeholder_id\"] },\n+                                        { $eq: [\"$stock_class_id\", \"$$stock_class_id\"] },\n+                                        { $eq: [\"$createdAt\", \"$$maxDate\"] }\n+                                    ]\n+                                }\n+                            }\n+                        }\n+                    ],\n+                    as: \"issuanceData\"\n+                }\n+            },\n+            { $unwind: \"$issuanceData\" },\n+            { $replaceRoot: { newRoot: \"$issuanceData\" } }\n+        ]);\n+      \n+        // We need to hit web3 to see which are actually valid\n+        const { contract } = await getIssuerContract(issuer);\n         let holdings = [];\n-        for (const stakeholder of stakeholders) {\n-            for (const issuance of issuances) {\n-                const issuanceId = issuance._id;\n-                const stakeholderId = stakeholder._id;\n-                const stakeHolderIdBytes16 = convertUUIDToBytes16(stakeholderId);\n-                const secIdBytes16 = convertUUIDToBytes16(issuance.security_id);\n-                const [stockClassIdBytes16, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n-                    stakeHolderIdBytes16,\n-                    secIdBytes16,\n-                );\n-                const stockClassId = convertBytes16ToUUID(stockClassIdBytes16);\n-                holdings.push({\n-                    stockClassId,\n-                    quantity: Number(quantity),\n-                    sharePrice: Number(sharePrice),\n-                    timestamp: Number(timestamp),\n-                    stakeholderId,\n-                    issuanceId,\n-                });\n+        const stakeholderMap = Object.fromEntries(stakeholders.map((x) => { return [x._id, x]; }));\n+        const stockClassMap = Object.fromEntries(stockClasses.map((x) => { return [x._id, x]; }));\n+        for (const issuance of issuances) {\n+            const { stakeholder_id, security_id, stock_class_id } = issuance;\n+            const [_, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+                convertUUIDToBytes16(stakeholder_id),\n+                convertUUIDToBytes16(security_id),\n+            );\n+            if (quantity == 0) {\n+                continue;\n             }\n+            holdings.push({\n+                issuance,\n+                stockClass: stockClassMap[stock_class_id],\n+                stakeholder: stakeholderMap[stakeholder_id],\n+                quantity: Number(quantity) / decimalScaleValue,\n+                sharePrice: Number(sharePrice) / decimalScaleValue,\n+                timestamp: Number(timestamp) * 1000,\n+            });\n         }\n-\n-        // TODO: munging on server side\n-        res.send({ stakeholders, holdings, stockClasses, issuances });\n+        res.send({ holdings, stockClasses, issuer });\n     } catch (error) {\n         console.error(`error: ${error}`);\n         res.status(500).send(`${error}`);"
            },
            {
                "filename": "src/scripts/sampleData.js",
                "additions": 18,
                "deletions": 0,
                "patch": "@@ -174,6 +174,24 @@ export const stakeholder2 = (issuerId) => {\n         },\n     };\n };\n+\n+export const stakeholder3 = (issuerId) => {\n+    return {\n+        issuerId,\n+        data: {\n+            name: {\n+                legal_name: \"Kent Kolze\",\n+                first_name: \"Kent\",\n+                last_name: \"Kolze\",\n+            },\n+            issuer_assigned_id: \"\",\n+            stakeholder_type: \"INDIVIDUAL\",\n+            current_relationship: \"EMPLOYEE\",\n+            comments: [],\n+        },\n+    };\n+};\n+\n export const stockClass = (issuerId) => {\n     return {\n         issuerId,"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 27,
                "deletions": 10,
                "patch": "@@ -2,13 +2,13 @@ import axios from \"axios\";\n import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n import Factory from \"../../db/objects/Factory\";\n import { web3WaitTime } from \"../../db/operations/update\";\n-import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n+import { issuer as exampleIssuer, stakeholder1, stakeholder2, stakeholder3, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n import sleep from \"../../utils/sleep\";\n import { SERVER_BASE, runLocalServer, shutdownLocalServer } from \"./utils\";\n \n \n // Pro-tip: set this to iterate faster in dev after `seedExampleData` finishes\n-const HARDCODED_ISSUER_ID = \"4e25e7b9-9718-4c95-9f74-57a729b9cfb2\";\n+const HARDCODED_ISSUER_ID = null;\n \n beforeAll(async () => {\n     await runLocalServer(!HARDCODED_ISSUER_ID);\n@@ -48,6 +48,11 @@ const seedExampleData = async () => {\n     console.log(\"\u2705 | stakeholder2Response\", s2Id, stakeholder2Response.data);\n     await allowPropagate();\n     \n+    const stakeholder3Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder3(issuerId));\n+    const s3Id = stakeholder3Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder3Response\", s3Id, stakeholder3Response.data);\n+    await allowPropagate();\n+    \n     const stockClassResponse = await axios.post(`${SERVER_BASE}/stock-class/create`, stockClass(issuerId));\n     const stockClassId = stockClassResponse.data.stockClass._id;\n     console.log(\"\u2705 | stockClassResponse\", stockClassId, stockClassResponse.data);\n@@ -61,7 +66,7 @@ const seedExampleData = async () => {\n     console.log(\"\u2705 | stockIssuanceResponse\", issuance);\n     await allowPropagate();\n     \n-    // TODO: Victor going to finalize these?\n+    // TODO: Victor acceptance of issuance?\n     // const { security_id } = issuance;\n     // const stockIssuanceAcceptanceResp = await axios.post(\n     //     `${SERVER_BASE}/transactions/accept/stock`,\n@@ -70,20 +75,29 @@ const seedExampleData = async () => {\n     // console.log(\"\u2705 | Stock issuance acceptance response\", stockIssuanceAcceptanceResp.data);\n     // await allowPropagate();\n     \n-    const stockTransferResponse = await axios.post(\n+    const stockTransfer1Response = await axios.post(\n         `${SERVER_BASE}/transactions/transfer/stock`,\n         stockTransfer(issuerId, \"200\", s1Id, s2Id, stockClassId, \"4.20\")\n     );\n-    console.log(\"\u2705 | stockTransferResponse\", stockTransferResponse.data);\n+    console.log(\"\u2705 | stockTransfer1Response\", stockTransfer1Response.data);\n     await allowPropagate();\n     \n-    // TODO: Victor going to finalize these?\n+    // TODO: Victor acceptance of transfer1?\n     // const stockTransferAcceptanceResp = await axios.post(\n     //     `${SERVER_BASE}/transactions/accept/stock`,\n     //     stockAccept(issuerId, s2Id, stockClassId, security_id, [\"Accepted\"])\n     // );\n     // console.log(\"\u2705 | Stock transfer acceptance response\", stockTransferAcceptanceResp.data);\n     // await allowPropagate();\n+    \n+    const stockTransfer2Response = await axios.post(\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n+        stockTransfer(issuerId, \"300\", s1Id, s3Id, stockClassId, \"10.66\")\n+    );\n+    console.log(\"\u2705 | stockTransfer2Response\", stockTransfer2Response.data);\n+    await allowPropagate();\n+\n+    // TODO: acceptance of transfer2?\n \n     // Allow time for poller process to catch up\n     await sleep(pollingSleepTime + web3WaitTime + 2000);\n@@ -92,10 +106,13 @@ const seedExampleData = async () => {\n }\n \n const checkRecs = async (issuerId) => {\n-    // TODO: aggregate docs across activePositions to \n-    const { data: capTable } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n-    console.log(\"cap Table Latest: \", capTable);\n-\n+    const { data: {holdings} } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n+    let portions = holdings.map(({quantity, sharePrice, stakeholder}) => { return {quantity, sharePrice, name: stakeholder.name.legal_name}; });\n+    portions.sort((a, b) => b.quantity - a.quantity);\n+    expect(portions).toStrictEqual([\n+        {quantity: 300, sharePrice: 10.66, name: \"Kent Kolze\"},\n+        {quantity: 200, sharePrice: 4.2, name: \"Victor Mimo\"},\n+    ]);\n }\n \n test('end to end with event processing', async () => {"
            },
            {
                "filename": "src/utils/convertToFixedPointDecimals.js",
                "additions": 4,
                "deletions": 2,
                "patch": "@@ -1,16 +1,18 @@\n import { toBigInt } from \"ethers\";\n \n+export const decimalScaleValue = 1e10;\n+\n // Convert a price to a BigInt\n function toScaledBigNumber(price) {\n-    return toBigInt(Math.round(price * 1e10).toString());\n+    return toBigInt(Math.round(price * decimalScaleValue).toString());\n }\n \n // TODO: might not be refactored correctly from ethers v5 to v6\n // Convert a BigInt back to a decimal price\n function toDecimal(scaledPriceBigInt) {\n     if (typeof scaledPriceBigInt === \"bigint\") {\n         const numberString = scaledPriceBigInt.toString();\n-        return parseFloat(numberString / 1e10).toString();\n+        return parseFloat(numberString / decimalScaleValue).toString();\n     } else {\n         return scaledPriceBigInt;\n     }"
            }
        ]
    },
    {
        "sha": "1ad5a0b336bb8f81acbc7fc864e8b6ad55cca93f",
        "author": "kentkolze",
        "date": "2024-01-17 16:17:01+00:00",
        "message": "refactor integration server utils to separate file so it can be used in other tests\nadd web3 position getters",
        "files": [
            {
                "filename": "chain/src/CapTable.sol",
                "additions": 6,
                "deletions": 0,
                "patch": "@@ -423,6 +423,12 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n         return stockClasses.length;\n     }\n \n+    /// @inheritdoc ICapTable\n+    function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40) {\n+        ActivePosition storage position = positions.activePositions[stakeholderId][securityId];\n+        return (position.stock_class_id, position.quantity, position.share_price, position.timestamp);\n+    }\n+\n     /* Role Based Access Control */\n     modifier onlyOperator() {\n         /// @notice Admins are also considered Operators"
            },
            {
                "filename": "chain/src/interfaces/ICapTable.sol",
                "additions": 3,
                "deletions": 0,
                "patch": "@@ -74,6 +74,9 @@ interface ICapTable {\n \n     function getTotalActiveSecuritiesCount() external view returns (uint256);\n \n+    // Function to get the timestamp of an active position\n+    function getActivePosition(bytes16 stakeholderId, bytes16 securityId) external view returns (bytes16, uint, uint, uint40);\n+\n     function issueStock(StockIssuanceParams calldata params) external;\n \n     function repurchaseStock(StockParams calldata params, uint256 quantity, uint256 price) external;"
            },
            {
                "filename": "src/db/scripts/deseed.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,4 +1,4 @@\n-import { deseedDatabase } from \"../../tests/deseed\";\n+import { deseedDatabase } from \"../../tests/integration/utils.js\";\n \n const runDeseed = async () => {\n     try {"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 36,
                "deletions": 23,
                "patch": "@@ -4,7 +4,7 @@ import Stakeholder from \"../db/objects/Stakeholder\";\n import StockClass from \"../db/objects/StockClass\";\n import { StockIssuance } from \"../db/objects/transactions/issuance\";\n import { getIssuerContract } from \"../utils/caches\";\n-import { convertUUIDToBytes16 } from \"../utils/convertUUID\";\n+import { convertBytes16ToUUID, convertUUIDToBytes16 } from \"../utils/convertUUID\";\n \n export const capTable = Router();\n \n@@ -14,28 +14,41 @@ capTable.get(\"/\", async (req, res) => {\n \n capTable.get(\"/latest\", async (req, res) => {\n     const issuerId = req.query.issuerId;\n-    const stakeholders = await Stakeholder.find({issuer: issuerId});\n-    const issuances = await StockIssuance.find({issuer: issuerId});\n-    const stockClasses = await StockClass.find({issuer: issuerId});\n-    const issuer = await Issuer.findById(issuerId);\n-    const {contract} = await getIssuerContract(issuer);\n-    \n-    // TODO: switch to many promises at once for speed?\n-    let holdings = [];\n-    for (const stakeholder of stakeholders) {\n-        for (const issuance of issuances) {\n-            const issuanceId = issuance._id;\n-            const stakeholderId = stakeholder._id;\n-            const stakeHolderIdBytes16 = convertUUIDToBytes16(stakeholderId);\n-            const secIdBytes16 = convertUUIDToBytes16(issuance.security_id);\n-            const [stockClassIdBytes16, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n-                stakeHolderIdBytes16,\n-                secIdBytes16,\n-            );\n-            holdings.push({stockClassIdBytes16, quantity, sharePrice, timestamp, stakeholderId, issuanceId})\n+    try {\n+        const stakeholders = await Stakeholder.find({issuer: issuerId});\n+        const issuances = await StockIssuance.find({issuer: issuerId});\n+        const stockClasses = await StockClass.find({issuer: issuerId});\n+        const issuer = await Issuer.findById(issuerId);\n+        const {contract} = await getIssuerContract(issuer);\n+        \n+        // TODO: switch to array of promises for speed\n+        let holdings = [];\n+        for (const stakeholder of stakeholders) {\n+            for (const issuance of issuances) {\n+                const issuanceId = issuance._id;\n+                const stakeholderId = stakeholder._id;\n+                const stakeHolderIdBytes16 = convertUUIDToBytes16(stakeholderId);\n+                const secIdBytes16 = convertUUIDToBytes16(issuance.security_id);\n+                const [stockClassIdBytes16, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+                    stakeHolderIdBytes16,\n+                    secIdBytes16,\n+                );\n+                const stockClassId = convertBytes16ToUUID(stockClassIdBytes16);\n+                holdings.push({\n+                    stockClassId,\n+                    quantity: Number(quantity),\n+                    sharePrice: Number(sharePrice),\n+                    timestamp: Number(timestamp),\n+                    stakeholderId,\n+                    issuanceId,\n+                });\n+            }\n         }\n-    }\n \n-    // TODO: munging on server side?\n-    res.send({ stakeholders, holdings, stockClasses, issuances });\n+        // TODO: munging on server side\n+        res.send({ stakeholders, holdings, stockClasses, issuances });\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n })"
            },
            {
                "filename": "src/tests/deseed.js",
                "additions": 0,
                "deletions": 37,
                "patch": "@@ -1,37 +0,0 @@\n-import { connectDB } from \"../db/config/mongoose.ts\";\n-import HistoricalTransaction from \"../db/objects/HistoricalTransaction.js\";\n-import Issuer from \"../db/objects/Issuer.js\";\n-import Stakeholder from \"../db/objects/Stakeholder.js\";\n-import StockClass from \"../db/objects/StockClass.js\";\n-import StockLegendTemplate from \"../db/objects/StockLegendTemplate.js\";\n-import StockPlan from \"../db/objects/StockPlan.js\";\n-import Valuation from \"../db/objects/Valuation.js\";\n-import VestingTerms from \"../db/objects/VestingTerms.js\";\n-import { typeToModelType } from \"../db/operations/transactions.js\"; // Import the typeToModelType object to delete all transactions\n-\n-const deleteAllTransactions = async () => {\n-    for (const ModelType of Object.values(typeToModelType)) {\n-        await ModelType.deleteMany({});\n-    }\n-};\n-\n-const deleteAll = async () => {\n-    // Delete all documents from the collections\n-    await Issuer.deleteMany({});\n-    await Stakeholder.deleteMany({});\n-    await StockClass.deleteMany({});\n-    await StockLegendTemplate.deleteMany({});\n-    await StockPlan.deleteMany({});\n-    await Valuation.deleteMany({});\n-    await VestingTerms.deleteMany({});\n-    await HistoricalTransaction.deleteMany({});\n-    await deleteAllTransactions(); // Delete all transactions\n-};\n-\n-export const deseedDatabase = async () => {\n-    const connection = await connectDB();\n-    console.log(\"Deseeding from database: \", connection.name);\n-    await deleteAll();\n-    console.log(\"\u2705 Database deseeded successfully\");\n-    await connection.close();\n-};"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 5,
                "deletions": 18,
                "patch": "@@ -1,31 +1,20 @@\n import axios from \"axios\";\n-import { shutdownServer, startServer } from \"../../app\";\n import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n import Factory from \"../../db/objects/Factory\";\n import { web3WaitTime } from \"../../db/operations/update\";\n import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n import sleep from \"../../utils/sleep\";\n-import { deseedDatabase } from \"../deseed\";\n+import { SERVER_BASE, runLocalServer, shutdownLocalServer } from \"./utils\";\n \n \n-const SERVER_BASE = `http://localhost:${process.env.PORT}`;\n-// Pro-tip: use this for faster iteration in dev after seedExampleData is done\n-const HARDCODED_ISSUER_ID = null;\n-let _server = null\n-\n+// Pro-tip: set this to iterate faster in dev after `seedExampleData` finishes\n+const HARDCODED_ISSUER_ID = \"4e25e7b9-9718-4c95-9f74-57a729b9cfb2\";\n \n beforeAll(async () => {\n-    if (!HARDCODED_ISSUER_ID) {\n-        await deseedDatabase();\n-    }\n-    console.log(\"starting server\");\n-    _server = await startServer(false);\n+    await runLocalServer(!HARDCODED_ISSUER_ID);\n }, 10000);\n \n-afterAll(async () => {\n-    console.log(\"shutting down server\");\n-    await shutdownServer(_server);\n-}, 10000);\n+afterAll(shutdownLocalServer, 10000);\n \n const WAIT_TIME = 1000;\n \n@@ -103,8 +92,6 @@ const seedExampleData = async () => {\n }\n \n const checkRecs = async (issuerId) => {\n-    // TODO: there is a timing issue when running with `latest` instead of `finalized` \n-\n     // TODO: aggregate docs across activePositions to \n     const { data: capTable } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n     console.log(\"cap Table Latest: \", capTable);"
            },
            {
                "filename": "src/tests/integration/utils.ts",
                "additions": 58,
                "deletions": 0,
                "patch": "@@ -0,0 +1,58 @@\n+import { shutdownServer, startServer } from \"../../app\";\n+import { connectDB } from \"../../db/config/mongoose\";\n+import HistoricalTransaction from \"../../db/objects/HistoricalTransaction\";\n+import Issuer from \"../../db/objects/Issuer\";\n+import Stakeholder from \"../../db/objects/Stakeholder\";\n+import StockClass from \"../../db/objects/StockClass\";\n+import StockLegendTemplate from \"../../db/objects/StockLegendTemplate\";\n+import StockPlan from \"../../db/objects/StockPlan\";\n+import Valuation from \"../../db/objects/Valuation\";\n+import VestingTerms from \"../../db/objects/VestingTerms\";\n+import { typeToModelType } from \"../../db/operations/transactions\"; // Import the typeToModelType object to delete all transactions\n+\n+export const SERVER_BASE = `http://localhost:${process.env.PORT}`;\n+\n+let _server = null;\n+\n+export const runLocalServer = async (deseed) => {\n+    if (deseed) {\n+        await deseedDatabase();\n+    }\n+    console.log(\"starting server\");\n+    _server = await startServer(false);\n+}\n+\n+\n+export const shutdownLocalServer = async () => {\n+    console.log(\"shutting down server\");\n+    await shutdownServer(_server);\n+}\n+\n+\n+const deleteAllTransactions = async () => {\n+    for (const ModelType of Object.values(typeToModelType)) {\n+        // @ts-ignore\n+        await ModelType.deleteMany({});\n+    }\n+};\n+\n+const deleteAll = async () => {\n+    // Delete all documents from the collections\n+    await Issuer.deleteMany({});\n+    await Stakeholder.deleteMany({});\n+    await StockClass.deleteMany({});\n+    await StockLegendTemplate.deleteMany({});\n+    await StockPlan.deleteMany({});\n+    await Valuation.deleteMany({});\n+    await VestingTerms.deleteMany({});\n+    await HistoricalTransaction.deleteMany({});\n+    await deleteAllTransactions(); // Delete all transactions\n+};\n+\n+export const deseedDatabase = async () => {\n+    const connection = await connectDB();\n+    console.log(\"Deseeding from database: \", connection.name);\n+    await deleteAll();\n+    console.log(\"\u2705 Database deseeded successfully\");\n+    await connection.close();\n+};"
            }
        ]
    },
    {
        "sha": "9e57502eec28a609b09c6536d3d430f303edff07",
        "author": "kentkolze",
        "date": "2024-01-17 15:34:41+00:00",
        "message": "switch to retries instead of arbitrary waiting on missed updates",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 0,
                "deletions": 8,
                "patch": "@@ -77,7 +77,6 @@ export const stopEventProcessing = async () => {\n }\n \n export const pollingSleepTime = 1000;\n-export const web3WaitTime = 5000;\n \n export const startEventProcessing = async (finalizedOnly: boolean) => {\n     _keepProcessing = true;\n@@ -155,13 +154,6 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, final\n     // Process only up to a certain amount\n     [events, endBlock] = trimEvents(events, maxEvents, endBlock);\n \n-    if (!finalizedOnly) {\n-        // kkolze: when running against `latest`, our server routes need to wait \n-        //  for the web3 operations to complete before they write to mongo. therefore \n-        //  we need to wait here to ensure the server routes have written to mongo \n-        await sleep(web3WaitTime);\n-    }\n-\n     await withGlobalTransaction(async () => {\n         await persistEvents(issuerId, events);\n         await updateLastProcessed(issuerId, endBlock);"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 32,
                "deletions": 11,
                "patch": "@@ -1,3 +1,4 @@\n+import sleep from \"../../utils/sleep.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n@@ -17,16 +18,36 @@ import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n import { findByIdAndUpdate } from \"./atomic.ts\";\n \n \n+export const web3WaitTime = 5000;\n+\n+\n+const retryOnMiss = async (updateFunc, numRetries = 5, waitBase = null) => {\n+    /* kkolze: When polling `latest` instead of `finalized` web3 blocks, web3 can get ahead of mongo \n+      For example, see the `issuer.post(\"/create\"` code: the issuer is created in mongo after deployCapTable is called  \n+      We add retries to ensure the server routes have written to mongo  */\n+    let tried = 0;\n+    const waitMultiplier = waitBase || web3WaitTime;\n+    while (tried <= numRetries) {\n+        const res = await updateFunc();\n+        if (res !== null) {\n+            return res;\n+        }\n+        tried++;\n+        await sleep(tried * waitMultiplier, \"Returned null, retrying in \");\n+    }\n+}\n+\n+\n export const updateIssuerById = async (id, updatedData) => {\n     return await findByIdAndUpdate(Issuer, id, updatedData, { new: true });\n };\n \n export const updateStakeholderById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(Stakeholder, id, updatedData, { new: true });\n+    return await retryOnMiss(async () => findByIdAndUpdate(Stakeholder, id, updatedData, { new: true }));\n };\n \n export const updateStockClassById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockClass, id, updatedData, { new: true });\n+    return await retryOnMiss(async () => findByIdAndUpdate(StockClass, id, updatedData, { new: true }));\n };\n \n export const updateStockLegendTemplateById = async (id, updatedData) => {\n@@ -46,37 +67,37 @@ export const updateVestingTermsById = async (id, updatedData) => {\n };\n \n export const upsertStockIssuanceById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockIssuance, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockIssuance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockTransferById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockTransfer, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockTransfer, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockCancellationById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockCancellation, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockCancellation, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockRetractionById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockRetraction, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockRetraction, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockReissuanceById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockReissuance, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockReissuance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockRepurchaseById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockRepurchase, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockRepurchase, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockAcceptanceById = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockAcceptance, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockAcceptance, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertStockClassAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await findByIdAndUpdate(StockClassAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockClassAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n };\n \n export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true });\n };"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -1,7 +1,8 @@\n import axios from \"axios\";\n import { shutdownServer, startServer } from \"../../app\";\n-import { pollingSleepTime, web3WaitTime } from \"../../chain-operations/transactionPoller\";\n+import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n import Factory from \"../../db/objects/Factory\";\n+import { web3WaitTime } from \"../../db/operations/update\";\n import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n import sleep from \"../../utils/sleep\";\n import { deseedDatabase } from \"../deseed\";"
            },
            {
                "filename": "src/utils/sleep.js",
                "additions": 4,
                "deletions": 1,
                "patch": "@@ -1,4 +1,7 @@\n-function sleep(ms) {\n+function sleep(ms, logPrefix = null) {\n+    if (logPrefix) {\n+        console.log(`${logPrefix} ${(ms / 1000).toFixed(1)} seconds`);\n+    }\n     return new Promise((resolve) => setTimeout(resolve, ms));\n }\n "
            }
        ]
    },
    {
        "sha": "4b801bf87f685089ae95ecb7d0eab5320102d625",
        "author": "kentkolze",
        "date": "2024-01-17 14:40:47+00:00",
        "message": "bug fixes around allowing web3 to catch up when trailing latest block in polling process\nfirst cut of publishing the cap table",
        "files": [
            {
                "filename": ".env.example",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -1,5 +1,6 @@\n DATABASE_URL=\"mongodb://tap:tap@localhost:27017/mongo?authSource=admin&retryWrites=true&w=majority\"\n DATABASE_OVERRIDE=\"\"  # use a database other than the default in DATABASE_URL\n+DATABASE_REPLSET=\"0\"  # set to \"1\" if using --replSet option in mongo. this allows transactions\n OPTIMISM_GOERLI_RPC_URL=UPDATE_ME\n LOCAL_RPC_URL=http://127.0.0.1:8545\n PRIVATE_KEY_FAKE_ACCOUNT=UPDATE_ME"
            },
            {
                "filename": "README.md",
                "additions": 4,
                "deletions": 4,
                "patch": "@@ -177,13 +177,13 @@ Deploy a cap table to local anvil server through a local web2 server. The chain\n \n `yarn test-js-integration`\n \n-Condensed steps from no active processes running:\n+Integration test setup from no active processes:\n \n -   Terminal 1: `docker compose up`\n -   Terminal 2: `anvil`\n--   Terminal 3: `cd chain && forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast` \n-    -   To bootstrap `jest-integration` MongoDB collections: `cd .. && yarn test-js-integration` NOTE: we use `jest-integration`\n-    -   Then using MongoDB compass, create/update the record in `jest-integration.factories` with the implementation_address and factory_address\n+-   Terminal 3: `cd chain && forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast`\n+    -   In MongoDB compass, create/update `implementation_address` and `factory_address` in `jest-integration.factories`\n+        -   If the `jest-integration` MongoDB databases dont exist: `cd .. && yarn test-js-integration`\n     -   Run `yarn test-js-integration`!\n \n ## Contributing"
            },
            {
                "filename": "src/app.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -7,7 +7,7 @@ import { connectDB } from \"./db/config/mongoose.ts\";\n import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n \n // Routes\n-import { capTable as capTableRoutes } from \"./routes/capTable.js\";\n+import { capTable as capTableRoutes } from \"./routes/capTable.ts\";\n import historicalTransactions from \"./routes/historicalTransactions.js\";\n import mainRoutes from \"./routes/index.js\";\n import issuerRoutes from \"./routes/issuer.js\";\n@@ -70,7 +70,7 @@ app.use(\"/historical-transactions\", historicalTransactions);\n // transactions\n app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n \n-export const startServer = async (processTo = \"finalized\") => {\n+export const startServer = async (finalizedOnly = true) => {\n     /*\n     processTo can be \"latest\" or \"finalized\". Latest helps during testing bc we dont have to wait for blocks to finalize\n     */\n@@ -81,7 +81,7 @@ export const startServer = async (processTo = \"finalized\") => {\n     const server = app.listen(PORT, async () => {\n         console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n         // Asynchronous job to track web3 events in web2\n-        startEventProcessing(processTo);\n+        startEventProcessing(finalizedOnly);\n     });\n \n     return server;"
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 13,
                "deletions": 4,
                "patch": "@@ -77,8 +77,9 @@ export const stopEventProcessing = async () => {\n }\n \n export const pollingSleepTime = 1000;\n+export const web3WaitTime = 5000;\n \n-export const startEventProcessing = async (processTo: \"latest\" | \"finalized\") => {\n+export const startEventProcessing = async (finalizedOnly: boolean) => {\n     _keepProcessing = true;\n     _finishedProcessing = false;\n     const dbConn = await connectDB();\n@@ -88,20 +89,20 @@ export const startEventProcessing = async (processTo: \"latest\" | \"finalized\") =>\n         for (const issuer of issuers) {\n             if (issuer.deployed_to) {\n                 const { contract, provider, libraries } = await getIssuerContract(issuer);\n-                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper, processTo);\n+                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper, finalizedOnly);\n             }\n         }\n         await sleep(pollingSleepTime);\n     }\n     _finishedProcessing = true;\n };\n \n-const processEvents = async (dbConn, contract, provider, issuer, txHelper, processTo, maxBlocks = 1500, maxEvents = 250) => {\n+const processEvents = async (dbConn, contract, provider, issuer, txHelper, finalizedOnly, maxBlocks = 1500, maxEvents = 250) => {\n     /*\n     We process up to `maxEvents` across `maxBlocks` to ensure our transaction sizes dont get too big and bog down our db\n     */\n     let {_id: issuerId, last_processed_block: lastProcessedBlock, tx_hash: deployedTxHash} = issuer;\n-    const {number: latestBlock} = await provider.getBlock(processTo);\n+    const {number: latestBlock} = await provider.getBlock(finalizedOnly ? \"finalized\" : \"latest\");\n     // console.log(\"Processing for issuer\", {issuerId, lastProcessedBlock, deployedTxHash, latestBlock});\n     if (lastProcessedBlock === null) {\n         const receipt = await provider.getTransactionReceipt(deployedTxHash);\n@@ -153,6 +154,14 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, proce\n \n     // Process only up to a certain amount\n     [events, endBlock] = trimEvents(events, maxEvents, endBlock);\n+\n+    if (!finalizedOnly) {\n+        // kkolze: when running against `latest`, our server routes need to wait \n+        //  for the web3 operations to complete before they write to mongo. therefore \n+        //  we need to wait here to ensure the server routes have written to mongo \n+        await sleep(web3WaitTime);\n+    }\n+\n     await withGlobalTransaction(async () => {\n         await persistEvents(issuerId, events);\n         await updateLastProcessed(issuerId, endBlock);"
            },
            {
                "filename": "src/db/operations/atomic.ts",
                "additions": 16,
                "deletions": 3,
                "patch": "@@ -21,10 +21,23 @@ export const clearGlobalSession = () => {\n     _globalSession = null;\n }\n \n+const isReplSet = () => {\n+    if (process.env.DATABASE_REPLSET === \"1\") {\n+        return true;\n+    } \n+    return false;\n+}\n+\n export const withGlobalTransaction = async (func: () => Promise<void>, useConn?: Connection) => {\n+    if (!isReplSet()) {\n+        // Transactions in mongo only work when running with --replSet\n+        //  https://www.mongodb.com/docs/manual/tutorial/convert-standalone-to-replica-set/\n+        return await func();\n+    }\n+\n     // Wrap a user defined `func` in a global transaction\n-    const db = useConn || await connectDB();\n-    await db.transaction(async (session) => {\n+    const dbConn = useConn || await connectDB();\n+    await dbConn.transaction(async (session) => {\n         setGlobalSession(session);\n         try {\n             return await func();\n@@ -37,7 +50,7 @@ export const withGlobalTransaction = async (func: () => Promise<void>, useConn?:\n \n const includeSession = (options?: TQueryOptions) => {\n     let useOptions = options || {};\n-    if (!_globalSession) {\n+    if (_globalSession !== null) {\n         if (useOptions.session) {\n             throw new Error(`options.session is already set!: ${useOptions}`);\n         }"
            },
            {
                "filename": "src/routes/capTable.js",
                "additions": 0,
                "deletions": 7,
                "patch": "@@ -1,7 +0,0 @@\n-import { Router } from \"express\";\n-\n-export const capTable = Router();\n-\n-capTable.get(\"/\", async (req, res) => {\n-    res.send(\"Hello Cap Table!\");\n-});"
            },
            {
                "filename": "src/routes/capTable.ts",
                "additions": 41,
                "deletions": 0,
                "patch": "@@ -0,0 +1,41 @@\n+import { Router } from \"express\";\n+import Issuer from \"../db/objects/Issuer\";\n+import Stakeholder from \"../db/objects/Stakeholder\";\n+import StockClass from \"../db/objects/StockClass\";\n+import { StockIssuance } from \"../db/objects/transactions/issuance\";\n+import { getIssuerContract } from \"../utils/caches\";\n+import { convertUUIDToBytes16 } from \"../utils/convertUUID\";\n+\n+export const capTable = Router();\n+\n+capTable.get(\"/\", async (req, res) => {\n+    res.send(\"Hello Cap Table!\");\n+});\n+\n+capTable.get(\"/latest\", async (req, res) => {\n+    const issuerId = req.query.issuerId;\n+    const stakeholders = await Stakeholder.find({issuer: issuerId});\n+    const issuances = await StockIssuance.find({issuer: issuerId});\n+    const stockClasses = await StockClass.find({issuer: issuerId});\n+    const issuer = await Issuer.findById(issuerId);\n+    const {contract} = await getIssuerContract(issuer);\n+    \n+    // TODO: switch to many promises at once for speed?\n+    let holdings = [];\n+    for (const stakeholder of stakeholders) {\n+        for (const issuance of issuances) {\n+            const issuanceId = issuance._id;\n+            const stakeholderId = stakeholder._id;\n+            const stakeHolderIdBytes16 = convertUUIDToBytes16(stakeholderId);\n+            const secIdBytes16 = convertUUIDToBytes16(issuance.security_id);\n+            const [stockClassIdBytes16, quantity, sharePrice, timestamp] = await contract.getActivePosition(\n+                stakeHolderIdBytes16,\n+                secIdBytes16,\n+            );\n+            holdings.push({stockClassIdBytes16, quantity, sharePrice, timestamp, stakeholderId, issuanceId})\n+        }\n+    }\n+\n+    // TODO: munging on server side?\n+    res.send({ stakeholders, holdings, stockClasses, issuances });\n+})"
            },
            {
                "filename": "src/routes/stakeholder.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -10,8 +10,8 @@ import {\n \n import stakeholderSchema from \"../../ocf/schema/objects/Stakeholder.schema.json\" assert { type: \"json\" };\n import { createStakeholder } from \"../db/operations/create.js\";\n-import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n import { readIssuerById } from \"../db/operations/read.js\";\n+import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n const stakeholder = Router();\n "
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 8,
                "deletions": 10,
                "patch": "@@ -1,8 +1,7 @@\n import axios from \"axios\";\n import { shutdownServer, startServer } from \"../../app\";\n-import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n+import { pollingSleepTime, web3WaitTime } from \"../../chain-operations/transactionPoller\";\n import Factory from \"../../db/objects/Factory\";\n-import Issuer from \"../../db/objects/Issuer\";\n import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n import sleep from \"../../utils/sleep\";\n import { deseedDatabase } from \"../deseed\";\n@@ -19,13 +18,13 @@ beforeAll(async () => {\n         await deseedDatabase();\n     }\n     console.log(\"starting server\");\n-    _server = await startServer(\"latest\");\n-});\n+    _server = await startServer(false);\n+}, 10000);\n \n afterAll(async () => {\n     console.log(\"shutting down server\");\n     await shutdownServer(_server);\n-});\n+}, 10000);\n \n const WAIT_TIME = 1000;\n \n@@ -97,18 +96,17 @@ const seedExampleData = async () => {\n     // await allowPropagate();\n \n     // Allow time for poller process to catch up\n-    await sleep(pollingSleepTime + 3000);\n+    await sleep(pollingSleepTime + web3WaitTime + 2000);\n \n     return issuerId;\n }\n \n const checkRecs = async (issuerId) => {\n-    const issuer = Issuer.findById(issuerId);\n+    // TODO: there is a timing issue when running with `latest` instead of `finalized` \n \n     // TODO: aggregate docs across activePositions to \n-    const resp = await axios.get(`${SERVER_BASE}/cap-table/`);\n-    console.log(resp);\n-\n+    const { data: capTable } = await axios.get(`${SERVER_BASE}/cap-table/latest?issuerId=${issuerId}`);\n+    console.log(\"cap Table Latest: \", capTable);\n \n }\n "
            },
            {
                "filename": "src/utils/caches.ts",
                "additions": 2,
                "deletions": 6,
                "patch": "@@ -11,14 +11,10 @@ interface CachePayload {\n // Centralized contract manager/cache\n const contractCache: {[key: string]: CachePayload} = {};\n \n-const cacheIssuerContract = async (issuer, payload: CachePayload) => {\n-    contractCache[issuer._id] = payload;\n-}\n-\n-export const getIssuerContract = async (issuer) => {\n+export const getIssuerContract = async (issuer): Promise<CachePayload> => {\n     if (!contractCache[issuer._id]) {\n         const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to); \n-        await cacheIssuerContract(issuer, { contract, provider, libraries });\n+        contractCache[issuer._id] = { contract, provider, libraries };\n     }\n     return contractCache[issuer._id];\n }"
            }
        ]
    },
    {
        "sha": "c9cab7153a0a8ba753fe73b0acb2801f0095c5f8",
        "author": "kentkolze",
        "date": "2024-01-16 22:04:19+00:00",
        "message": "add beginnings of the cap table endpoints",
        "files": [
            {
                "filename": "src/app.js",
                "additions": 10,
                "deletions": 4,
                "patch": "@@ -7,6 +7,7 @@ import { connectDB } from \"./db/config/mongoose.ts\";\n import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n \n // Routes\n+import { capTable as capTableRoutes } from \"./routes/capTable.js\";\n import historicalTransactions from \"./routes/historicalTransactions.js\";\n import mainRoutes from \"./routes/index.js\";\n import issuerRoutes from \"./routes/issuer.js\";\n@@ -55,6 +56,7 @@ app.use(json({ limit: \"50mb\" }));\n app.enable(\"trust proxy\");\n \n app.use(\"/\", chainMiddleware, mainRoutes);\n+app.use(\"/cap-table\", chainMiddleware, capTableRoutes);\n app.use(\"/issuer\", chainMiddleware, issuerRoutes);\n app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n@@ -90,9 +92,13 @@ export const shutdownServer = async (server) => {\n         console.log(\"Shutting down app server...\");\n         server.close();\n     }\n-    console.log(\"Stopping event processing...\");\n-    stopEventProcessing();\n-    console.log(\"Disconnecting from mongo...\");\n-    await mongoose.disconnect();\n+    \n+    console.log(\"Waiting for event processing to stop...\");\n+    await stopEventProcessing();\n+\n+    if (mongoose.connection?.readyState === mongoose.STATES.connected) {\n+        console.log(\"Disconnecting from mongo...\");\n+        await mongoose.disconnect();\n+    }\n }\n "
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 11,
                "deletions": 3,
                "patch": "@@ -67,13 +67,20 @@ export const txFuncs = Object.fromEntries(\n );\n \n let _keepProcessing = true;\n+let _finishedProcessing = false;\n \n-export const stopEventProcessing = () => {\n+export const stopEventProcessing = async () => {\n     _keepProcessing = false;\n+    while (!_finishedProcessing) {\n+        await sleep(50);\n+    }\n }\n \n+export const pollingSleepTime = 1000;\n+\n export const startEventProcessing = async (processTo: \"latest\" | \"finalized\") => {\n-    _keepProcessing = true\n+    _keepProcessing = true;\n+    _finishedProcessing = false;\n     const dbConn = await connectDB();\n     while (_keepProcessing) {\n         const issuers = await readAllIssuers();\n@@ -84,8 +91,9 @@ export const startEventProcessing = async (processTo: \"latest\" | \"finalized\") =>\n                 await processEvents(dbConn, contract, provider, issuer, libraries.txHelper, processTo);\n             }\n         }\n-        await sleep(1 * 1000);\n+        await sleep(pollingSleepTime);\n     }\n+    _finishedProcessing = true;\n };\n \n const processEvents = async (dbConn, contract, provider, issuer, txHelper, processTo, maxBlocks = 1500, maxEvents = 250) => {"
            },
            {
                "filename": "src/routes/capTable.js",
                "additions": 7,
                "deletions": 0,
                "patch": "@@ -0,0 +1,7 @@\n+import { Router } from \"express\";\n+\n+export const capTable = Router();\n+\n+capTable.get(\"/\", async (req, res) => {\n+    res.send(\"Hello Cap Table!\");\n+});"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 25,
                "deletions": 13,
                "patch": "@@ -1,22 +1,29 @@\n import axios from \"axios\";\n import { shutdownServer, startServer } from \"../../app\";\n+import { pollingSleepTime } from \"../../chain-operations/transactionPoller\";\n import Factory from \"../../db/objects/Factory\";\n import Issuer from \"../../db/objects/Issuer\";\n import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n import sleep from \"../../utils/sleep\";\n import { deseedDatabase } from \"../deseed\";\n \n \n+const SERVER_BASE = `http://localhost:${process.env.PORT}`;\n+// Pro-tip: use this for faster iteration in dev after seedExampleData is done\n+const HARDCODED_ISSUER_ID = null;\n let _server = null\n \n+\n beforeAll(async () => {\n-    await deseedDatabase();\n+    if (!HARDCODED_ISSUER_ID) {\n+        await deseedDatabase();\n+    }\n     console.log(\"starting server\");\n     _server = await startServer(\"latest\");\n });\n \n afterAll(async () => {\n-    console.log(\"shuting down server\");\n+    console.log(\"shutting down server\");\n     await shutdownServer(_server);\n });\n \n@@ -37,28 +44,28 @@ const seedExampleData = async () => {\n         );\n     }\n \n-    const issuerResponse = await axios.post(\"http://localhost:8080/issuer/create\", exampleIssuer);\n+    const issuerResponse = await axios.post(`${SERVER_BASE}/issuer/create`, exampleIssuer);\n     const issuerId = issuerResponse.data.issuer._id;\n     console.log(\"\u2705 | Issuer response \", issuerId, issuerResponse.data);\n     await allowPropagate();\n        \n-    const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerId));\n+    const stakeholder1Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder1(issuerId));\n     const s1Id = stakeholder1Response.data.stakeholder._id;\n     console.log(\"\u2705 | stakeholder1Response\", s1Id, stakeholder1Response.data);\n     await allowPropagate();\n     \n-    const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerId));\n+    const stakeholder2Response = await axios.post(`${SERVER_BASE}/stakeholder/create`, stakeholder2(issuerId));\n     const s2Id = stakeholder2Response.data.stakeholder._id;\n     console.log(\"\u2705 | stakeholder2Response\", s2Id, stakeholder2Response.data);\n     await allowPropagate();\n     \n-    const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerId));\n+    const stockClassResponse = await axios.post(`${SERVER_BASE}/stock-class/create`, stockClass(issuerId));\n     const stockClassId = stockClassResponse.data.stockClass._id;\n     console.log(\"\u2705 | stockClassResponse\", stockClassId, stockClassResponse.data);\n     await allowPropagate();\n     \n     const stockIssuanceResponse = await axios.post(\n-        \"http://localhost:8080/transactions/issuance/stock\",\n+        `${SERVER_BASE}/transactions/issuance/stock`,\n         stockIssuance(issuerId, s1Id, stockClassId, \"500\", \"1.2\")\n     );\n     const issuance = stockIssuanceResponse.data.stockIssuance;\n@@ -68,40 +75,45 @@ const seedExampleData = async () => {\n     // TODO: Victor going to finalize these?\n     // const { security_id } = issuance;\n     // const stockIssuanceAcceptanceResp = await axios.post(\n-    //     \"http://localhost:8080/transactions/accept/stock\",\n+    //     `${SERVER_BASE}/transactions/accept/stock`,\n     //     stockAccept(issuerId, s1Id, stockClassId, security_id, [\"Accepted\"])\n     // );\n     // console.log(\"\u2705 | Stock issuance acceptance response\", stockIssuanceAcceptanceResp.data);\n     // await allowPropagate();\n     \n     const stockTransferResponse = await axios.post(\n-        \"http://localhost:8080/transactions/transfer/stock\",\n+        `${SERVER_BASE}/transactions/transfer/stock`,\n         stockTransfer(issuerId, \"200\", s1Id, s2Id, stockClassId, \"4.20\")\n     );\n     console.log(\"\u2705 | stockTransferResponse\", stockTransferResponse.data);\n     await allowPropagate();\n     \n     // TODO: Victor going to finalize these?\n     // const stockTransferAcceptanceResp = await axios.post(\n-    //     \"http://localhost:8080/transactions/accept/stock\",\n+    //     `${SERVER_BASE}/transactions/accept/stock`,\n     //     stockAccept(issuerId, s2Id, stockClassId, security_id, [\"Accepted\"])\n     // );\n     // console.log(\"\u2705 | Stock transfer acceptance response\", stockTransferAcceptanceResp.data);\n     // await allowPropagate();\n \n+    // Allow time for poller process to catch up\n+    await sleep(pollingSleepTime + 3000);\n+\n     return issuerId;\n }\n \n const checkRecs = async (issuerId) => {\n     const issuer = Issuer.findById(issuerId);\n \n     // TODO: aggregate docs across activePositions to \n+    const resp = await axios.get(`${SERVER_BASE}/cap-table/`);\n+    console.log(resp);\n+\n+\n }\n \n test('end to end with event processing', async () => {\n-    const issuerId = await seedExampleData();\n-    // Allow time for background process to catch up\n-    await sleep(15000);\n+    const issuerId = HARDCODED_ISSUER_ID || await seedExampleData();\n     await checkRecs(issuerId);\n     \n }, WAIT_TIME * 100);"
            }
        ]
    },
    {
        "sha": "ec8edb917dccc298461b0c3cf54f776bae8a315b",
        "author": "kentkolze",
        "date": "2024-01-16 21:25:05+00:00",
        "message": "correct the issues with polling for blockchain events and get the integration test working properly",
        "files": [
            {
                "filename": "README.md",
                "additions": 41,
                "deletions": 18,
                "patch": "@@ -2,9 +2,9 @@\n \n Developed by:\n \n-- [Poet](https://poet.network/)\n-- [Plural Energy](https://www.pluralenergy.co/)\n-- [Fairmint](https://www.fairmint.com/)\n+-   [Poet](https://poet.network/)\n+-   [Plural Energy](https://www.pluralenergy.co/)\n+-   [Fairmint](https://www.fairmint.com/)\n \n This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety. In development, it's meant to be run in a Docker container with a local MongoDB instance. While in active development, it's meant to be run with [Anvil](https://book.getfoundry.sh/anvil/) and [Forge](https://book.getfoundry.sh/forge/).\n \n@@ -16,28 +16,28 @@ This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap\n \n ## Dependencies\n \n-- [Docker](https://docs.docker.com/get-docker/)\n+-   [Docker](https://docs.docker.com/get-docker/)\n \n-- [Foundry](https://getfoundry.sh/)\n+-   [Foundry](https://getfoundry.sh/)\n \n ```sh\n curl -L https://foundry.paradigm.xyz | bash\n ```\n \n-- [Mongo Compass](https://www.mongodb.com/try/download/compass)\n+-   [Mongo Compass](https://www.mongodb.com/try/download/compass)\n \n-- [Postman App](https://www.postman.com/downloads/)\n+-   [Postman App](https://www.postman.com/downloads/)\n \n-- [Node.js v18.16.0](https://nodejs.org/en/download/)\n+-   [Node.js v18.16.0](https://nodejs.org/en/download/)\n \n-- [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n+-   [Yarn v1.22.19](https://classic.yarnpkg.com/en/docs/install/#mac-stable)\n \n We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo) for the local development database. You can find the [Docker Compose file](./docker-compose.yml) in the root of this repository.\n \n ## Official links\n \n-- [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n-- [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n+-   [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n+-   [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n \n ## Getting started\n \n@@ -98,7 +98,6 @@ To deploy these libraries:\n \n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n-\n ## Running the cap table server\n \n After the deployment script is completed, start the server with nodemon:\n@@ -152,16 +151,40 @@ We're shipping code fast. If you run into an issue, particularly one that result\n \n Inside of `/chain`:\n \n-- Restart anvil\n-- Run `forge clean`\n-- Move back to the root directory, then run `yarn build`\n+-   Restart anvil\n+-   Run `forge clean`\n+-   Move back to the root directory, then run `yarn build`\n \n After, you can seed and deploy the cap table with either of the above options. If the bug persists, please open an issue with an attached screenshot and steps to reproduce.\n \n-## Testing\n+## Testing Web3\n+\n+Run all smart contracts tests\n+\n+`yarn test`\n+\n+## Testing Web2\n+\n+### Unit tests\n+\n+Run all javascript unit tests with jest\n+\n+`yarn test-js`\n+\n+### Integration tests\n+\n+Deploy a cap table to local anvil server through a local web2 server. The chain event listener is also run to ensure the events are properly mirrored into the mongo database. NOTE: running this deletes your local mongo collections first\n+\n+`yarn test-js-integration`\n+\n+Condensed steps from no active processes running:\n \n-To run tests for the smart contracts, run `yarn test`.\n-This will run all the tests defined in the test suite and output the results to the console.\n+-   Terminal 1: `docker compose up`\n+-   Terminal 2: `anvil`\n+-   Terminal 3: `cd chain && forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast` \n+    -   To bootstrap `jest-integration` MongoDB collections: `cd .. && yarn test-js-integration` NOTE: we use `jest-integration`\n+    -   Then using MongoDB compass, create/update the record in `jest-integration.factories` with the implementation_address and factory_address\n+    -   Run `yarn test-js-integration`!\n \n ## Contributing\n "
            },
            {
                "filename": "jest.config.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -4,7 +4,7 @@\n  */\n \n // This allows us to avoid messing with the state of people's standard dev database\n-process.env['DATABASE_OVERRIDE'] = 'jest';\n+process.env['DATABASE_OVERRIDE'] = 'jest-integration';\n \n /** @type {import('jest').Config} */\n const config = {"
            },
            {
                "filename": "src/app.js",
                "additions": 10,
                "deletions": 5,
                "patch": "@@ -68,26 +68,31 @@ app.use(\"/historical-transactions\", historicalTransactions);\n // transactions\n app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n \n-export const startServer = async () => {\n+export const startServer = async (processTo = \"finalized\") => {\n+    /*\n+    processTo can be \"latest\" or \"finalized\". Latest helps during testing bc we dont have to wait for blocks to finalize\n+    */\n+\n     // Connect to MongoDB\n     await connectDB();\n \n     const server = app.listen(PORT, async () => {\n         console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n         // Asynchronous job to track web3 events in web2\n-        startEventProcessing();\n+        startEventProcessing(processTo);\n     });\n \n     return server;\n };\n \n export const shutdownServer = async (server) => {\n-    console.log(\"Shutting down app server...\");\n-    server.close();\n+    if (server) {\n+        console.log(\"Shutting down app server...\");\n+        server.close();\n+    }\n     console.log(\"Stopping event processing...\");\n     stopEventProcessing();\n     console.log(\"Disconnecting from mongo...\");\n     await mongoose.disconnect();\n-    console.log(\" done!\");\n }\n "
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 83,
                "deletions": 77,
                "patch": "@@ -1,4 +1,4 @@\n-import { AbiCoder } from \"ethers\";\n+import { AbiCoder, EventLog } from \"ethers\";\n import { connectDB } from \"../db/config/mongoose.ts\";\n import { withGlobalTransaction } from \"../db/operations/atomic.ts\";\n import { readAllIssuers } from \"../db/operations/read.js\";\n@@ -33,28 +33,37 @@ import {\n \n const abiCoder = new AbiCoder();\n \n+interface QueuedEvent {\n+    type: string;\n+    timestamp: Date;\n+    data: any;\n+    o: EventLog;\n+}\n+\n const contractFuncs = new Map([\n     [\"StakeholderCreated\", handleStakeholder],\n     [\"StockClassCreated\", handleStockClass],\n ]);\n \n const txMapper = {\n-    0: [\"INVALID\"],\n-    1: [\"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\", IssuerAuthorizedSharesAdjustment, handleIssuerAuthorizedSharesAdjusted],\n-    2: [\"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\", StockClassAuthorizedSharesAdjustment, handleStockClassAuthorizedSharesAdjusted],\n-    3: [\"STOCK_ACCEPTANCE\", StockAcceptance, handleStockAcceptance],\n-    4: [\"STOCK_CANCELLATION\", StockCancellation, handleStockCancellation],\n-    5: [\"STOCK_ISSUANCE\", StockIssuance, handleStockIssuance],\n-    6: [\"STOCK_REISSUANCE\", StockReissuance, handleStockReissuance],\n-    7: [\"STOCK_REPURCHASE\", StockRepurchase, handleStockRepurchase],\n-    8: [\"STOCK_RETRACTION\", StockRetraction, handleStockRetraction],\n-    9: [\"STOCK_TRANSFER\", StockTransfer, handleStockTransfer],\n+    1: [IssuerAuthorizedSharesAdjustment, handleIssuerAuthorizedSharesAdjusted],\n+    2: [StockClassAuthorizedSharesAdjustment, handleStockClassAuthorizedSharesAdjusted],\n+    3: [StockAcceptance, handleStockAcceptance],\n+    4: [StockCancellation, handleStockCancellation],\n+    5: [StockIssuance, handleStockIssuance],\n+    6: [StockReissuance, handleStockReissuance],\n+    7: [StockRepurchase, handleStockRepurchase],\n+    8: [StockRetraction, handleStockRetraction],\n+    9: [StockTransfer, handleStockTransfer],\n };\n-\n-// Map(event.type => handler) derived from the above\n-const txFuncs = new Map(\n+// (idx => type name) derived from txMapper\n+export const txTypes = Object.fromEntries(\n     // @ts-ignore\n-    Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_x, [name, _y, handleFunc]]) => [name, handleFunc])\n+    Object.entries(txMapper).map(([i, [_, f]]) => [i, f.name.replace(\"handle\", \"\")])\n+);\n+// (name => handler) derived from txMapper\n+export const txFuncs = Object.fromEntries(\n+    Object.entries(txMapper).map(([i, [_, f]]) => [txTypes[i], f])\n );\n \n let _keepProcessing = true;\n@@ -63,72 +72,69 @@ export const stopEventProcessing = () => {\n     _keepProcessing = false;\n }\n \n-export const startEventProcessing = async () => {\n+export const startEventProcessing = async (processTo: \"latest\" | \"finalized\") => {\n     _keepProcessing = true\n     const dbConn = await connectDB();\n     while (_keepProcessing) {\n         const issuers = await readAllIssuers();\n-        console.log(`Processing synchronously for ${issuers.length} issuers`);\n+        // console.log(`Processing synchronously for ${issuers.length} issuers`);\n         for (const issuer of issuers) {\n             if (issuer.deployed_to) {\n                 const { contract, provider, libraries } = await getIssuerContract(issuer);\n-                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper);\n+                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper, processTo);\n             }\n         }\n-        await sleep(10 * 1000);\n+        await sleep(1 * 1000);\n     }\n };\n \n-const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBlocks = 1500, maxEvents = 250) => {\n+const processEvents = async (dbConn, contract, provider, issuer, txHelper, processTo, maxBlocks = 1500, maxEvents = 250) => {\n     /*\n     We process up to `maxEvents` across `maxBlocks` to ensure our transaction sizes dont get too big and bog down our db\n     */\n-    let {_id: issuerId, last_processed_block: startBlock, tx_hash: deployedTxHash} = issuer;\n-    console.log(\"Processing for issuer\", issuerId, startBlock, deployedTxHash);\n-    if (startBlock === null) {\n+    let {_id: issuerId, last_processed_block: lastProcessedBlock, tx_hash: deployedTxHash} = issuer;\n+    const {number: latestBlock} = await provider.getBlock(processTo);\n+    // console.log(\"Processing for issuer\", {issuerId, lastProcessedBlock, deployedTxHash, latestBlock});\n+    if (lastProcessedBlock === null) {\n         const receipt = await provider.getTransactionReceipt(deployedTxHash);\n-        const tx = await provider.getTransaction(deployedTxHash);\n-        console.log(\"tx\", tx);\n         if (!receipt) {\n-            console.error(\"Transaction receipt not found\");\n+            console.error(\"Deployment receipt not found\");\n             return;\n         }\n-        startBlock = await bootstrapIssuer(issuerId, receipt.blockNumber, contract, dbConn);\n+        if (receipt.blockNumber > latestBlock) {\n+            console.log(\"Deployment tx not finalized\", {receipt, lastFinalizedBlock: latestBlock});\n+            return;\n+        }\n+        lastProcessedBlock = await issuerDeployed(issuerId, receipt, contract, dbConn);\n     }\n-    const {number: latestBlock} = await provider.getBlock('finalized');\n+    const startBlock = lastProcessedBlock + 1;\n     let endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n+    if (startBlock >= endBlock) {\n+        return;\n+    }\n+    \n+    // console.log(\" processing from\", { startBlock, endBlock });\n+    let events: QueuedEvent[] = [];\n \n-    let events: any[] = [];\n-\n-    // TODO: fix filter\n-    const contractEvents = await contract.queryFilter(\"*\", startBlock, endBlock);\n+    const contractEvents: EventLog[] = await contract.queryFilter(\"*\", startBlock, endBlock);\n     for (const event of contractEvents) {\n-        if (contractFuncs.has(event.type)) {\n-            // TODO: how to deserialize event.data?\n-            console.log(\"contract event: \", event);\n-            events.push(event);\n+        const type = event?.fragment?.name;\n+        if (contractFuncs.has(type)) {\n+            const { timestamp } = await provider.getBlock(event.blockNumber);\n+            events.push({type, timestamp, data: event.args[0], o: event });\n         }\n     }\n \n-    // TODO: fix filter\n-    const txEvents = await txHelper.queryFilter(\"*\", startBlock, endBlock);\n+    const txEvents: EventLog[] = await txHelper.queryFilter(txHelper.filters.TxCreated, startBlock, endBlock);\n     for (const event of txEvents) {\n-        // TODO: the same processing as libraries.txHelper.on     \n-        // TODO:  emit TxCreated(transactions.length, txType, txData);\n-        //  how do we parse the event.data string of each event? \n-        //    https://www.npmjs.com/package/@ethersproject/abstract-provider?activeTab=code (line 102: Log.data is string-type)\n-        console.log(\"txHelper event: \", event);\n         if (event.removed) {\n             continue;\n         }\n-        // TODO: does txTypeIdx even come with the event??? need to test this...\n-        let txTypeIdx;\n-        let txData;\n-        const [type, structType] = txMapper[txTypeIdx];\n+        const [_len, typeIdx, txData] = event.args;\n+        const [structType, _] = txMapper[typeIdx];\n         const decodedData = abiCoder.decode([structType], txData);\n         const { timestamp } = await provider.getBlock(event.blockNumber);\n-        // TODO: I think the below needs a lot of work\n-        events.push({ ...event, type, timestamp, data: decodedData[0] });\n+        events.push({ type: txTypes[typeIdx], timestamp, data: decodedData[0], o: event });\n     }\n \n     // Nothing to process\n@@ -137,62 +143,62 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBl\n         return;\n     }\n \n-    // Process in the correct order\n-    events.sort((a, b) => a.blockNumber - b.blockNumber || a.transactionIndex - b.transactionIndex);\n+    // Process only up to a certain amount\n     [events, endBlock] = trimEvents(events, maxEvents, endBlock);\n-\n     await withGlobalTransaction(async () => {\n         await persistEvents(issuerId, events);\n         await updateLastProcessed(issuerId, endBlock);\n     }, dbConn);\n };\n \n-const bootstrapIssuer = async (issuerId, deployedBlockNumber, contract, dbConn) => {\n-    console.log(\"Bootstrapping issuer\");\n-    // TODO: fix the copy-pasted query\n-    const issuerCreatedFilter = contract.filters.IssuerCreated;\n-    const issuerEvents = await contract.queryFilter(issuerCreatedFilter);\n-    if (issuerEvents.length === 0) {\n+const issuerDeployed = async (issuerId, receipt, contract, dbConn) => {\n+    console.log(\"New issuer was deployed\", {issuerId});\n+    const events = await contract.queryFilter(contract.filters.IssuerCreated);\n+    if (events.length === 0) {\n         throw new Error(`No issuer events found!`);\n     }\n-    const issuerCreatedEventId = issuerEvents[0].args[0];\n-    console.log(\"IssuerCreated Event Emitted!\", issuerCreatedEventId);\n-    const tMinusOne = deployedBlockNumber - 1;\n-    \n+    const issuerCreatedEventId = events[0].args[0];\n+    console.log(\"IssuerCreated event captured!\", {issuerCreatedEventId});\n+    const lastProcessedBlock = receipt.blockNumber - 1;\n     await withGlobalTransaction(async () => {\n         await verifyIssuerAndSeed(contract, issuerCreatedEventId);\n-        await updateLastProcessed(issuerId, tMinusOne);\n+        await updateLastProcessed(issuerId, lastProcessedBlock);\n     }, dbConn);\n-\n-    return tMinusOne;\n+    return lastProcessedBlock;\n };\n \n-const persistEvents = async (issuerId, events) => {\n+const persistEvents = async (issuerId, events: QueuedEvent[]) => {\n     // Persist all the necessary changes for each event gathered in process events\n+    console.log(`${events.length} events to process for issuerId ${issuerId}`);\n     for (const event of events) {\n-        const txHandleFunc = txFuncs.get(event.type);\n-        console.log(\"persistEvent: \", event);\n+        const {type, data, timestamp} = event;\n+        const txHandleFunc = txFuncs[type];\n+        // console.log(\"persistEvent: \", {type, data, timestamp});\n         if (txHandleFunc) {\n             // @ts-ignore\n-            await txHandleFunc(event.data, issuerId, event.timestamp);\n+            await txHandleFunc(data, issuerId, timestamp);\n             continue;\n         }\n-        const contractHandleFunc = contractFuncs.get(event.type);\n+        const contractHandleFunc = contractFuncs.get(type);\n         if (contractHandleFunc) {\n-            await contractHandleFunc(event.data);\n+            await contractHandleFunc(data);\n             continue;\n         }\n-        throw new Error(`Invalid transaction type: \"${event.type}\" for ${event}`);\n+        console.error(\"Invalid transaction type: \", type, event);\n+        throw new Error(`Invalid transaction type: \"${type}\"`);\n     }\n };\n \n-export const trimEvents = (events, maxEvents, endBlock) => {\n+export const trimEvents = (origEvents: QueuedEvent[], maxEvents, endBlock) => {\n+    // Sort for correct execution order\n+    let events = [...origEvents];\n+    events.sort((a, b) => a.o.blockNumber - b.o.blockNumber || a.o.transactionIndex - b.o.transactionIndex || a.o.index - b.o.index);\n     let index = 0;    \n     while (index < maxEvents && index < events.length) {\n         // Include the entire next block\n-        const includeBlock = events[index].blockNumber;\n+        const includeBlock = events[index].o.blockNumber;\n         index++;\n-        while (index < events.length && events[index].blockNumber === includeBlock) {\n+        while (index < events.length && events[index].o.blockNumber === includeBlock) {\n             index++;\n         }\n     }\n@@ -203,10 +209,10 @@ export const trimEvents = (events, maxEvents, endBlock) => {\n     // We processed up to the last events' blockNumber\n     // `index` is *exclusive* when trimming\n     const useEvents = [...events.slice(0, index)];\n-    return [useEvents, useEvents[useEvents.length - 1].blockNumber];\n+    return [useEvents, useEvents[useEvents.length - 1].o.blockNumber];\n };\n \n \n const updateLastProcessed = async (issuerId, lastProcessedBlock) => {\n-    return updateIssuerById(issuerId, {lastProcessedBlock});\n+    return updateIssuerById(issuerId, {last_processed_block: lastProcessedBlock});\n };"
            },
            {
                "filename": "src/db/operations/delete.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -6,6 +6,7 @@ import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n+import { findByIdAndDelete } from \"./atomic.ts\";\n \n // TODO: since we're doing a time series db that's meant to be immutable, is this needed?\n "
            },
            {
                "filename": "src/db/operations/transactions.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,8 +1,8 @@\n import * as Acceptance from \"../objects/transactions/acceptance/index.js\";\n import * as Adjustment from \"../objects/transactions/adjustment/index.js\";\n import * as Cancellation from \"../objects/transactions/cancellation/index.js\";\n-import * as Exercise from \"../objects/transactions/exercise/index.js\";\n import * as Conversion from \"../objects/transactions/conversion/index.js\";\n+import * as Exercise from \"../objects/transactions/exercise/index.js\";\n import * as Issuance from \"../objects/transactions/issuance/index.js\";\n import * as Reissuance from \"../objects/transactions/reissuance/index.js\";\n import * as Release from \"../objects/transactions/release/index.js\";\n@@ -12,6 +12,7 @@ import * as ReturnToPool from \"../objects/transactions/return_to_pool/index.js\";\n import * as Split from \"../objects/transactions/split/index.js\";\n import * as Transfer from \"../objects/transactions/transfer/index.js\";\n import * as Vesting from \"../objects/transactions/vesting/index.js\";\n+import { save } from \"./atomic.ts\";\n \n const typeToModelType = {\n     // Acceptance\n@@ -91,7 +92,7 @@ const addTransactions = async (inputTransactions, issuerId) => {\n         inputTransaction = { ...inputTransaction, issuer: issuerId };\n         const ModelType = typeToModelType[inputTransaction.object_type];\n         if (ModelType) {\n-            const transaction = await new ModelType(inputTransaction).save();\n+            const transaction = await save(new ModelType(inputTransaction));\n             console.log(`${inputTransaction.object_type} transaction added. Details:`, JSON.stringify(transaction, null, 2));\n         } else {\n             console.log(`Unknown object type for transaction:`, JSON.stringify(inputTransaction, null, 2));"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -14,6 +14,7 @@ import StockReissuance from \"../objects/transactions/reissuance/StockReissuance.\n import StockRepurchase from \"../objects/transactions/repurchase/StockRepurchase.js\";\n import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n+import { findByIdAndUpdate } from \"./atomic.ts\";\n \n \n export const updateIssuerById = async (id, updatedData) => {"
            },
            {
                "filename": "src/db/scripts/deseed.js",
                "additions": 4,
                "deletions": 40,
                "patch": "@@ -1,47 +1,11 @@\n-import mongoose from \"mongoose\";\n-import { connectDB } from \"../config/mongoose.ts\";\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n-import Issuer from \"../objects/Issuer.js\";\n-import Stakeholder from \"../objects/Stakeholder.js\";\n-import StockClass from \"../objects/StockClass.js\";\n-import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n-import StockPlan from \"../objects/StockPlan.js\";\n-import Valuation from \"../objects/Valuation.js\";\n-import VestingTerms from \"../objects/VestingTerms.js\";\n-import { typeToModelType } from \"../operations/transactions.js\"; // Import the typeToModelType object to delete all transactions\n+import { deseedDatabase } from \"../../tests/deseed\";\n \n-const deleteAllTransactions = async () => {\n-    for (const ModelType of Object.values(typeToModelType)) {\n-        await ModelType.deleteMany({});\n-    }\n-};\n-\n-const deleteAll = async () => {\n-    // Delete all documents from the collections\n-    await Issuer.deleteMany({});\n-    await Stakeholder.deleteMany({});\n-    await StockClass.deleteMany({});\n-    await StockLegendTemplate.deleteMany({});\n-    await StockPlan.deleteMany({});\n-    await Valuation.deleteMany({});\n-    await VestingTerms.deleteMany({});\n-    await HistoricalTransaction.deleteMany({});\n-    await deleteAllTransactions(); // Delete all transactions\n-};\n-\n-const deseedDatabase = async () => {\n+const runDeseed = async () => {\n     try {\n-        connectDB();\n-\n-        await deleteAll();\n-\n-        console.log(\"\u2705 Database deseeded successfully\");\n-\n-        // Close the database connection\n-        await mongoose.connection.close();\n+        await deseedDatabase();\n     } catch (err) {\n         console.log(\"\u274c Error deseeding database:\", err);\n     }\n };\n \n-deseedDatabase();\n+runDeseed();"
            },
            {
                "filename": "src/scripts/testAcceptance.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,5 +1,5 @@\n import axios from \"axios\";\n-import { connectDB } from \"../config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockAccept } from \"./sampleData.js\";\n "
            },
            {
                "filename": "src/tests/deseed.js",
                "additions": 37,
                "deletions": 0,
                "patch": "@@ -0,0 +1,37 @@\n+import { connectDB } from \"../db/config/mongoose.ts\";\n+import HistoricalTransaction from \"../db/objects/HistoricalTransaction.js\";\n+import Issuer from \"../db/objects/Issuer.js\";\n+import Stakeholder from \"../db/objects/Stakeholder.js\";\n+import StockClass from \"../db/objects/StockClass.js\";\n+import StockLegendTemplate from \"../db/objects/StockLegendTemplate.js\";\n+import StockPlan from \"../db/objects/StockPlan.js\";\n+import Valuation from \"../db/objects/Valuation.js\";\n+import VestingTerms from \"../db/objects/VestingTerms.js\";\n+import { typeToModelType } from \"../db/operations/transactions.js\"; // Import the typeToModelType object to delete all transactions\n+\n+const deleteAllTransactions = async () => {\n+    for (const ModelType of Object.values(typeToModelType)) {\n+        await ModelType.deleteMany({});\n+    }\n+};\n+\n+const deleteAll = async () => {\n+    // Delete all documents from the collections\n+    await Issuer.deleteMany({});\n+    await Stakeholder.deleteMany({});\n+    await StockClass.deleteMany({});\n+    await StockLegendTemplate.deleteMany({});\n+    await StockPlan.deleteMany({});\n+    await Valuation.deleteMany({});\n+    await VestingTerms.deleteMany({});\n+    await HistoricalTransaction.deleteMany({});\n+    await deleteAllTransactions(); // Delete all transactions\n+};\n+\n+export const deseedDatabase = async () => {\n+    const connection = await connectDB();\n+    console.log(\"Deseeding from database: \", connection.name);\n+    await deleteAll();\n+    console.log(\"\u2705 Database deseeded successfully\");\n+    await connection.close();\n+};"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 90,
                "deletions": 17,
                "patch": "@@ -1,34 +1,107 @@\n import axios from \"axios\";\n import { shutdownServer, startServer } from \"../../app\";\n-import { connectDB } from \"../../db/config/mongoose\";\n-import { issuer } from \"../../scripts/sampleData\";\n+import Factory from \"../../db/objects/Factory\";\n+import Issuer from \"../../db/objects/Issuer\";\n+import { issuer as exampleIssuer, stakeholder1, stakeholder2, stockClass, stockIssuance, stockTransfer } from \"../../scripts/sampleData\";\n+import sleep from \"../../utils/sleep\";\n+import { deseedDatabase } from \"../deseed\";\n \n \n let _server = null\n \n-const deleteAllCollections = async () => {\n-    const dbConn = await connectDB();\n-    console.log(\"Dropping mongo database: \", dbConn.name);\n-    await dbConn.dropDatabase();\n-}\n-\n beforeAll(async () => {\n-    await deleteAllCollections();\n+    await deseedDatabase();\n     console.log(\"starting server\");\n-    _server = await startServer();\n+    _server = await startServer(\"latest\");\n });\n \n afterAll(async () => {\n     console.log(\"shuting down server\");\n     await shutdownServer(_server);\n });\n \n+const WAIT_TIME = 1000;\n+\n+const allowPropagate = async () => {\n+    // Ensure ethers has enough time to catch up\n+    await sleep(WAIT_TIME);\n+}\n+\n+const seedExampleData = async () => {\n+    const rec = await Factory.findOne();\n+    if (!rec) {\n+        throw new Error(\n+            `Manually create the {\"implementation_adress\": ..., \"factory_address\": ...} record \n+            in \"factories\" collection. Run the \"forge script ...\" command from the comment \n+            in \"chain/script/CapTableFactory.s.sol\"`\n+        );\n+    }\n+\n+    const issuerResponse = await axios.post(\"http://localhost:8080/issuer/create\", exampleIssuer);\n+    const issuerId = issuerResponse.data.issuer._id;\n+    console.log(\"\u2705 | Issuer response \", issuerId, issuerResponse.data);\n+    await allowPropagate();\n+       \n+    const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerId));\n+    const s1Id = stakeholder1Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder1Response\", s1Id, stakeholder1Response.data);\n+    await allowPropagate();\n+    \n+    const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerId));\n+    const s2Id = stakeholder2Response.data.stakeholder._id;\n+    console.log(\"\u2705 | stakeholder2Response\", s2Id, stakeholder2Response.data);\n+    await allowPropagate();\n+    \n+    const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerId));\n+    const stockClassId = stockClassResponse.data.stockClass._id;\n+    console.log(\"\u2705 | stockClassResponse\", stockClassId, stockClassResponse.data);\n+    await allowPropagate();\n+    \n+    const stockIssuanceResponse = await axios.post(\n+        \"http://localhost:8080/transactions/issuance/stock\",\n+        stockIssuance(issuerId, s1Id, stockClassId, \"500\", \"1.2\")\n+    );\n+    const issuance = stockIssuanceResponse.data.stockIssuance;\n+    console.log(\"\u2705 | stockIssuanceResponse\", issuance);\n+    await allowPropagate();\n+    \n+    // TODO: Victor going to finalize these?\n+    // const { security_id } = issuance;\n+    // const stockIssuanceAcceptanceResp = await axios.post(\n+    //     \"http://localhost:8080/transactions/accept/stock\",\n+    //     stockAccept(issuerId, s1Id, stockClassId, security_id, [\"Accepted\"])\n+    // );\n+    // console.log(\"\u2705 | Stock issuance acceptance response\", stockIssuanceAcceptanceResp.data);\n+    // await allowPropagate();\n+    \n+    const stockTransferResponse = await axios.post(\n+        \"http://localhost:8080/transactions/transfer/stock\",\n+        stockTransfer(issuerId, \"200\", s1Id, s2Id, stockClassId, \"4.20\")\n+    );\n+    console.log(\"\u2705 | stockTransferResponse\", stockTransferResponse.data);\n+    await allowPropagate();\n+    \n+    // TODO: Victor going to finalize these?\n+    // const stockTransferAcceptanceResp = await axios.post(\n+    //     \"http://localhost:8080/transactions/accept/stock\",\n+    //     stockAccept(issuerId, s2Id, stockClassId, security_id, [\"Accepted\"])\n+    // );\n+    // console.log(\"\u2705 | Stock transfer acceptance response\", stockTransferAcceptanceResp.data);\n+    // await allowPropagate();\n+\n+    return issuerId;\n+}\n+\n+const checkRecs = async (issuerId) => {\n+    const issuer = Issuer.findById(issuerId);\n+\n+    // TODO: aggregate docs across activePositions to \n+}\n+\n test('end to end with event processing', async () => {\n-    // TODO: talk to Victor to get a good set of example data going\n-    //  ??\n-    // Deploy a cap table and seed the database\n-    const issuerResponse = await axios.post(\"http://localhost:8080/issuer/create\", issuer);\n+    const issuerId = await seedExampleData();\n+    // Allow time for background process to catch up\n+    await sleep(15000);\n+    await checkRecs(issuerId);\n     \n-    // TODO: check that mongo has the appropriate updates\n-    // \n-});\n+}, WAIT_TIME * 100);"
            },
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 10,
                "deletions": 2,
                "patch": "@@ -1,11 +1,12 @@\n-import { trimEvents } from \"../../chain-operations/transactionPoller\";\n+import { trimEvents, txFuncs, txTypes } from \"../../chain-operations/transactionPoller\";\n \n // TODO: if starts failing again run: yarn add --dev jest-esm-transformer\n // https://jestjs.io/docs/using-matchers for more docs on `expect`\n \n-const myEvents = [5, 6, 6, 6, 7, 7, 7].map((x, i) => { return {blockNumber: x, i}; });\n+const myEvents = [5, 6, 6, 6, 7, 7, 7].map((x, i) => { return {i, o: {blockNumber: x}}; });\n \n test('trimEvents partial', () => {\n+    // @ts-ignore\n     const [events, block] = trimEvents(myEvents, 2, 10);\n     expect(events.length).toBe(4);\n     expect(events).toStrictEqual(myEvents.slice(0, 4));\n@@ -15,9 +16,16 @@ test('trimEvents partial', () => {\n test('trimEvents full', () => {\n     // We allow more than maxEvents in order to include all events of the last block\n     for (const maxEvents of [5, 6, 7, 15]) {\n+        // @ts-ignore\n         const [events, block] = trimEvents(myEvents, maxEvents, 10);\n         expect(events.length).toBe(myEvents.length);\n         expect(events).toStrictEqual(myEvents);\n         expect(block).toBe(10);\n     }\n });\n+\n+test('txMapper to maps', () => {\n+    // @ts-ignore\n+    expect(txTypes[3n]).toBe(\"StockAcceptance\");\n+    expect(txFuncs[\"StockAcceptance\"].name).toBe(\"handleStockAcceptance\");\n+});"
            }
        ]
    },
    {
        "sha": "37a68e7008e3116aec645f9666d1e2dba0e38aa9",
        "author": "kentkolze",
        "date": "2024-01-14 02:10:36+00:00",
        "message": "setup jest for running unit and integration tests in javascript\nget the service working with the mix of ts and js",
        "files": [
            {
                "filename": ".env.example",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -1,4 +1,5 @@\n DATABASE_URL=\"mongodb://tap:tap@localhost:27017/mongo?authSource=admin&retryWrites=true&w=majority\"\n+DATABASE_OVERRIDE=\"\"  # use a database other than the default in DATABASE_URL\n OPTIMISM_GOERLI_RPC_URL=UPDATE_ME\n LOCAL_RPC_URL=http://127.0.0.1:8545\n PRIVATE_KEY_FAKE_ACCOUNT=UPDATE_ME"
            },
            {
                "filename": "jest.config.js",
                "additions": 3,
                "deletions": 0,
                "patch": "@@ -3,6 +3,9 @@\n  * https://jestjs.io/docs/configuration\n  */\n \n+// This allows us to avoid messing with the state of people's standard dev database\n+process.env['DATABASE_OVERRIDE'] = 'jest';\n+\n /** @type {import('jest').Config} */\n const config = {\n   // All imported modules in your tests should be mocked automatically"
            },
            {
                "filename": "package.json",
                "additions": 4,
                "deletions": 4,
                "patch": "@@ -7,7 +7,7 @@\n     \"description\": \"Transfer Agent Protocol compliant cap table\",\n     \"type\": \"module\",\n     \"scripts\": {\n-        \"start\": \"nodemon src/server.js\",\n+        \"start\": \"nodemon --loader ts-node/esm src/server.js\",\n         \"eslint\": \"eslint --cache --cache-location=node_modules/.cache/.eslintcache --fix\",\n         \"lint\": \"yarn run eslint . --ext .js,.jsx,.ts,.tsx\",\n         \"format\": \"prettier '**/*' --ignore-unknown --write\",\n@@ -19,8 +19,8 @@\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n         \"deseed\": \"node src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\",\n-        \"test-js\": \"jest --testPathPattern /src/tests/unit\",\n-        \"test-js-integration\": \"jest --testPathPattern /src/tests/integration\",\n+        \"test-js\": \"jest --testPathPattern src/tests/unit\",\n+        \"test-js-integration\": \"jest --testPathPattern src/tests/integration\",\n         \"validate-poet-files\": \"cd ocf && yarn validate-poet-files\",\n         \"test-onchain-cap-table-local\": \"node src/chain-operations/capTable.cjs local\",\n         \"test-onchain-cap-table-optimism-goerli\": \"node src/chain-operations/capTable.cjs optimism-goerli\",\n@@ -43,7 +43,6 @@\n         \"express\": \"^4.18.2\",\n         \"mongoose\": \"^7.4.2\",\n         \"solc\": \"^0.8.20\",\n-        \"typescript\": \"^5.3.3\",\n         \"xstate\": \"^4.38.2\",\n         \"yauzl\": \"^2.10.0\"\n     },\n@@ -68,6 +67,7 @@\n         \"solhint\": \"^3.4.1\",\n         \"ts-jest\": \"^29.1.1\",\n         \"ts-node\": \"^10.9.2\",\n+        \"typescript\": \"^5.3.3\",\n         \"uuid\": \"^9.0.0\"\n     }\n }"
            },
            {
                "filename": "src/app.js",
                "additions": 93,
                "deletions": 0,
                "patch": "@@ -0,0 +1,93 @@\n+import { config } from \"dotenv\";\n+import express, { json, urlencoded } from \"express\";\n+config();\n+\n+import { connectDB } from \"./db/config/mongoose.ts\";\n+\n+import { startEventProcessing, stopEventProcessing } from \"./chain-operations/transactionPoller.ts\";\n+\n+// Routes\n+import historicalTransactions from \"./routes/historicalTransactions.js\";\n+import mainRoutes from \"./routes/index.js\";\n+import issuerRoutes from \"./routes/issuer.js\";\n+import stakeholderRoutes from \"./routes/stakeholder.js\";\n+import stockClassRoutes from \"./routes/stockClass.js\";\n+import stockLegendRoutes from \"./routes/stockLegend.js\";\n+import stockPlanRoutes from \"./routes/stockPlan.js\";\n+import transactionRoutes from \"./routes/transactions.js\";\n+import valuationRoutes from \"./routes/valuation.js\";\n+import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n+\n+import mongoose from \"mongoose\";\n+import { readIssuerById } from \"./db/operations/read.js\";\n+import { getIssuerContract } from \"./utils/caches.ts\";\n+\n+const app = express();\n+\n+const PORT = process.env.PORT;\n+const CHAIN = process.env.CHAIN;\n+\n+// Middlewares\n+const chainMiddleware = (req, res, next) => {\n+    req.chain = CHAIN;\n+    next();\n+};\n+\n+// Middleware to get or create contract instance\n+// the listener is first started on deployment, then here as a backup\n+const contractMiddleware = async (req, res, next) => {\n+    if (!req.body.issuerId) {\n+        console.log(\"\u274c | No issuer ID\");\n+        res.status(400).send(\"issuerId is required\");\n+    }\n+\n+    // fetch issuer to ensure it exists\n+    const issuer = await readIssuerById(req.body.issuerId);\n+    if (!issuer) res.status(400).send(\"issuer not found \");\n+\n+    const {contract, provider} = await getIssuerContract(issuer);\n+    req.contract = contract;\n+    req.provider = provider;\n+    next();\n+};\n+app.use(urlencoded({ limit: \"50mb\", extended: true }));\n+app.use(json({ limit: \"50mb\" }));\n+app.enable(\"trust proxy\");\n+\n+app.use(\"/\", chainMiddleware, mainRoutes);\n+app.use(\"/issuer\", chainMiddleware, issuerRoutes);\n+app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n+app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n+// No middleware required since these are only created offchain\n+app.use(\"/stock-legend\", stockLegendRoutes);\n+app.use(\"/stock-plan\", stockPlanRoutes);\n+app.use(\"/valuation\", valuationRoutes);\n+app.use(\"/vesting-terms\", vestingTermsRoutes);\n+app.use(\"/historical-transactions\", historicalTransactions);\n+\n+// transactions\n+app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n+\n+export const startServer = async () => {\n+    // Connect to MongoDB\n+    await connectDB();\n+\n+    const server = app.listen(PORT, async () => {\n+        console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n+        // Asynchronous job to track web3 events in web2\n+        startEventProcessing();\n+    });\n+\n+    return server;\n+};\n+\n+export const shutdownServer = async (server) => {\n+    console.log(\"Shutting down app server...\");\n+    server.close();\n+    console.log(\"Stopping event processing...\");\n+    stopEventProcessing();\n+    console.log(\"Disconnecting from mongo...\");\n+    await mongoose.disconnect();\n+    console.log(\" done!\");\n+}\n+"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 4,
                "deletions": 6,
                "patch": "@@ -1,12 +1,12 @@\n-import { ethers } from \"ethers\";\n import { config } from \"dotenv\";\n+import { ethers } from \"ethers\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n \n config();\n \n \n-async function getLocalContractInstance(address) {\n+const getLocalContractInstance = async (address) => {\n     const CONTRACT_ADDRESS_LOCAL = address;\n \n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n@@ -24,7 +24,7 @@ async function getLocalContractInstance(address) {\n     return { contract, provider, libraries };\n }\n \n-async function getOptimismGoerliContractInstance(address) {\n+const getOptimismGoerliContractInstance = async (address) => {\n     const CONTRACT_ADDRESS_OPTIMISM_GOERLI = address;\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_POET_TEST;\n \n@@ -37,7 +37,7 @@ async function getOptimismGoerliContractInstance(address) {\n     return { contract, provider, libraries};\n }\n \n-async function getContractInstance(chain, address) {\n+export const getContractInstance = async (chain, address) => {\n     if (chain === \"local\") {\n         return getLocalContractInstance(address);\n     } else if (chain === \"optimism-goerli\") {\n@@ -46,5 +46,3 @@ async function getContractInstance(chain, address) {\n         throw new Error(`Unsupported chain: ${chain}`);\n     }\n }\n-\n-export default getContractInstance;"
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 19,
                "deletions": 12,
                "patch": "@@ -4,6 +4,7 @@ import { withGlobalTransaction } from \"../db/operations/atomic.ts\";\n import { readAllIssuers } from \"../db/operations/read.js\";\n import { updateIssuerById } from \"../db/operations/update.js\";\n import { getIssuerContract } from \"../utils/caches.ts\";\n+import sleep from \"../utils/sleep.js\";\n import { verifyIssuerAndSeed } from \"./seed.js\";\n import {\n     IssuerAuthorizedSharesAdjustment,\n@@ -56,38 +57,43 @@ const txFuncs = new Map(\n     Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_x, [name, _y, handleFunc]]) => [name, handleFunc])\n );\n \n+let _keepProcessing = true;\n \n-const sleep = (delay) => new Promise((resolve) => setTimeout(resolve, delay));\n+export const stopEventProcessing = () => {\n+    _keepProcessing = false;\n+}\n \n-export const startSynchronousEventProcessing = async () => {\n-    while (true) {\n-        await sleep(1 * 1000);\n+export const startEventProcessing = async () => {\n+    _keepProcessing = true\n+    const dbConn = await connectDB();\n+    while (_keepProcessing) {\n         const issuers = await readAllIssuers();\n-        const dbConn = await connectDB();\n-        // Process events synchronously for each issuer\n-        console.log(`Processing for ${issuers.length} issuers`);\n+        console.log(`Processing synchronously for ${issuers.length} issuers`);\n         for (const issuer of issuers) {\n             if (issuer.deployed_to) {\n                 const { contract, provider, libraries } = await getIssuerContract(issuer);\n                 await processEvents(dbConn, contract, provider, issuer, libraries.txHelper);\n             }\n         }\n+        await sleep(10 * 1000);\n     }\n };\n \n const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBlocks = 1500, maxEvents = 250) => {\n     /*\n     We process up to `maxEvents` across `maxBlocks` to ensure our transaction sizes dont get too big and bog down our db\n     */\n-    console.log(\" processEvents for issuer\", issuer);\n-    let {_id: issuerId, last_processed_block: startBlock, tx_hash: deployedTx} = issuer;\n+    let {_id: issuerId, last_processed_block: startBlock, tx_hash: deployedTxHash} = issuer;\n+    console.log(\"Processing for issuer\", issuerId, startBlock, deployedTxHash);\n     if (startBlock === null) {\n-        const receipt = await provider.getTransactionReceipt(deployedTx);\n+        const receipt = await provider.getTransactionReceipt(deployedTxHash);\n+        const tx = await provider.getTransaction(deployedTxHash);\n+        console.log(\"tx\", tx);\n         if (!receipt) {\n             console.error(\"Transaction receipt not found\");\n             return;\n         }\n-        startBlock = await bootstrapTable(issuerId, receipt.blockNumber, contract, dbConn);\n+        startBlock = await bootstrapIssuer(issuerId, receipt.blockNumber, contract, dbConn);\n     }\n     const {number: latestBlock} = await provider.getBlock('finalized');\n     let endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n@@ -141,7 +147,8 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBl\n     }, dbConn);\n };\n \n-const bootstrapTable = async (issuerId, deployedBlockNumber, contract, dbConn) => {\n+const bootstrapIssuer = async (issuerId, deployedBlockNumber, contract, dbConn) => {\n+    console.log(\"Bootstrapping issuer\");\n     // TODO: fix the copy-pasted query\n     const issuerCreatedFilter = contract.filters.IssuerCreated;\n     const issuerEvents = await contract.queryFilter(issuerCreatedFilter);"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 3,
                "deletions": 1,
                "patch": "@@ -4,10 +4,12 @@ import mongoose from \"mongoose\";\n dotenv.config();\n \n const DATABASE_URL = process.env.DATABASE_URL;\n+const DATABASE_OVERRIDE = process.env.DATABASE_OVERRIDE;\n \n export const connectDB = async () => {\n+    const connectOptions = DATABASE_OVERRIDE ? {dbName: DATABASE_OVERRIDE} : {};\n     try {\n-        await mongoose.connect(DATABASE_URL);\n+        await mongoose.connect(DATABASE_URL, connectOptions);\n         console.log(\"\u2705 | Mongo connected succesfully\");\n         return mongoose.connection;\n     } catch (error) {"
            },
            {
                "filename": "src/db/scripts/deseed.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,5 +1,6 @@\n import mongoose from \"mongoose\";\n-import connectDB from \"../config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n@@ -8,7 +9,6 @@ import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n import { typeToModelType } from \"../operations/transactions.js\"; // Import the typeToModelType object to delete all transactions\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n \n const deleteAllTransactions = async () => {\n     for (const ModelType of Object.values(typeToModelType)) {"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 1,
                "deletions": 8,
                "patch": "@@ -8,9 +8,6 @@ import { countIssuers, readIssuerById } from \"../db/operations/read.js\";\n import { convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n-import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n-import { contractCache } from \"../utils/caches.ts\";\n-\n const issuer = Router();\n \n issuer.get(\"/\", async (req, res) => {\n@@ -58,17 +55,13 @@ issuer.post(\"/create\", async (req, res) => {\n \n         const issuerIdBytes16 = convertUUIDToBytes16(incomingIssuerToValidate.id);\n         console.log(\"\ud83d\udcbe | Issuer id in bytes16 \", issuerIdBytes16);\n-        const { contract, provider, address, libraries, deployHash } = await deployCapTable(\n+        const { address, deployHash } = await deployCapTable(\n             chain,\n             issuerIdBytes16,\n             incomingIssuerToValidate.legal_name,\n             incomingIssuerToValidate.initial_shares_authorized\n         );\n \n-        // add contract to the cache and start listener\n-        contractCache[incomingIssuerToValidate.id] = { contract, provider, libraries };\n-        startOnchainListeners(contract, provider, incomingIssuerToValidate.id, libraries);\n-\n         const incomingIssuerForDB = {\n             ...incomingIssuerToValidate,\n             deployed_to: address,"
            },
            {
                "filename": "src/scripts/testAcceptance.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.js\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockAccept } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testCancellation.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -1,7 +1,7 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { issuer, stakeholder1, stakeholder2, stockCancel, stockClass, stockIssuance, stockTransfer } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { stockCancel } from \"./sampleData.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testIssuance.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,9 +1,9 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import Issuer from \"../db/objects/Issuer.js\";\n import Stakeholder from \"../db/objects/Stakeholder.js\";\n import StockClass from \"../db/objects/StockClass.js\";\n-import axios from \"axios\";\n import { stockIssuance } from \"./sampleData.js\";\n-import connectDB from \"../db/config/mongoose.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testIssuerAdjustment.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -1,7 +1,7 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { stockClass, stockClassAuthorizedSharesAdjust, issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testReissuance.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockReissue } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testRepurchase.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,7 +1,7 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockRepurchase } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/scripts/testRetraction.js",
                "additions": 3,
                "deletions": 2,
                "patch": "@@ -1,7 +1,8 @@\n+import axios from \"axios\";\n+import { connectDB } from \"../config/mongoose.ts\";\n import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n import { stockRetract } from \"./sampleData.js\";\n-import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+\n // Connect to MongoDB\n connectDB();\n "
            },
            {
                "filename": "src/scripts/testStockClassAdjustment.js",
                "additions": 4,
                "deletions": 3,
                "patch": "@@ -1,7 +1,8 @@\n-import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n-import { stockClass, stockClassAuthorizedSharesAdjust, issuerAuthorizedSharesAdjust } from \"./sampleData.js\";\n import axios from \"axios\";\n-import connectDB from \"../db/config/mongoose.js\";\n+import { connectDB } from \"../config/mongoose.ts\";\n+import StockIssuance from \"../db/objects/transactions/issuance/StockIssuance.js\";\n+import { stockClassAuthorizedSharesAdjust } from \"./sampleData.js\";\n+\n connectDB();\n \n const main = async () => {"
            },
            {
                "filename": "src/server.js",
                "additions": 2,
                "deletions": 76,
                "patch": "@@ -1,77 +1,3 @@\n-import { config } from \"dotenv\";\n-import express, { json, urlencoded } from \"express\";\n-config();\n+import { startServer } from \"./app.js\";\n \n-import connectDB from \"./db/config/mongoose.js\";\n-\n-import startSynchronousEventProcessing from \"./chain-operations/transactionPoller.js\";\n-\n-// Routes\n-import historicalTransactions from \"./routes/historicalTransactions.js\";\n-import mainRoutes from \"./routes/index.js\";\n-import issuerRoutes from \"./routes/issuer.js\";\n-import stakeholderRoutes from \"./routes/stakeholder.js\";\n-import stockClassRoutes from \"./routes/stockClass.js\";\n-import stockLegendRoutes from \"./routes/stockLegend.js\";\n-import stockPlanRoutes from \"./routes/stockPlan.js\";\n-import transactionRoutes from \"./routes/transactions.js\";\n-import valuationRoutes from \"./routes/valuation.js\";\n-import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n-\n-import { readIssuerById } from \"./db/operations/read.js\";\n-import { getIssuerContract } from \"./utils/caches.ts\";\n-\n-const app = express();\n-\n-// Connect to MongoDB\n-connectDB();\n-\n-const PORT = process.env.PORT;\n-const CHAIN = process.env.CHAIN;\n-\n-// Middlewares\n-const chainMiddleware = (req, res, next) => {\n-    req.chain = CHAIN;\n-    next();\n-};\n-\n-// Middleware to get or create contract instance\n-// the listener is first started on deployment, then here as a backup\n-const contractMiddleware = async (req, res, next) => {\n-    if (!req.body.issuerId) {\n-        console.log(\"\u274c | No issuer ID\");\n-        res.status(400).send(\"issuerId is required\");\n-    }\n-\n-    // fetch issuer to ensure it exists\n-    const issuer = await readIssuerById(req.body.issuerId);\n-    if (!issuer) res.status(400).send(\"issuer not found \");\n-\n-    const {contract, provider} = await getIssuerContract(issuer);\n-    req.contract = contract;\n-    req.provider = provider;\n-    next();\n-};\n-app.use(urlencoded({ limit: \"50mb\", extended: true }));\n-app.use(json({ limit: \"50mb\" }));\n-app.enable(\"trust proxy\");\n-\n-app.use(\"/\", chainMiddleware, mainRoutes);\n-app.use(\"/issuer\", chainMiddleware, issuerRoutes);\n-app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n-app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n-// No middleware required since these are only created offchain\n-app.use(\"/stock-legend\", stockLegendRoutes);\n-app.use(\"/stock-plan\", stockPlanRoutes);\n-app.use(\"/valuation\", valuationRoutes);\n-app.use(\"/vesting-terms\", vestingTermsRoutes);\n-app.use(\"/historical-transactions\", historicalTransactions);\n-\n-// transactions\n-app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n-\n-app.listen(PORT, async () => {\n-    console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n-    // Kick off asynchronous job to process changes\n-    startSynchronousEventProcessing();\n-});\n+startServer();"
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 33,
                "deletions": 0,
                "patch": "@@ -1 +1,34 @@\n+import axios from \"axios\";\n+import { shutdownServer, startServer } from \"../../app\";\n+import { connectDB } from \"../../db/config/mongoose\";\n+import { issuer } from \"../../scripts/sampleData\";\n \n+\n+let _server = null\n+\n+const deleteAllCollections = async () => {\n+    const dbConn = await connectDB();\n+    console.log(\"Dropping mongo database: \", dbConn.name);\n+    await dbConn.dropDatabase();\n+}\n+\n+beforeAll(async () => {\n+    await deleteAllCollections();\n+    console.log(\"starting server\");\n+    _server = await startServer();\n+});\n+\n+afterAll(async () => {\n+    console.log(\"shuting down server\");\n+    await shutdownServer(_server);\n+});\n+\n+test('end to end with event processing', async () => {\n+    // TODO: talk to Victor to get a good set of example data going\n+    //  ??\n+    // Deploy a cap table and seed the database\n+    const issuerResponse = await axios.post(\"http://localhost:8080/issuer/create\", issuer);\n+    \n+    // TODO: check that mongo has the appropriate updates\n+    // \n+});"
            },
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -1,6 +1,7 @@\n import { trimEvents } from \"../../chain-operations/transactionPoller\";\n \n-// TODO: if starts failing again: yarn add --dev jest-esm-transformer\n+// TODO: if starts failing again run: yarn add --dev jest-esm-transformer\n+// https://jestjs.io/docs/using-matchers for more docs on `expect`\n \n const myEvents = [5, 6, 6, 6, 7, 7, 7].map((x, i) => { return {blockNumber: x, i}; });\n "
            },
            {
                "filename": "src/utils/caches.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,4 +1,4 @@\n-import getContractInstance from \"../chain-operations/getContractInstances\";\n+import { getContractInstance } from \"../chain-operations/getContractInstances.js\";\n \n const CHAIN = process.env.CHAIN;\n "
            }
        ]
    },
    {
        "sha": "17e4896b69deeb6d6cc1ceced2a4619845511c35",
        "author": "victormimo",
        "date": "2024-01-12 23:32:20+00:00",
        "message": "fixing network bug",
        "files": [
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 8,
                "deletions": 9,
                "patch": "@@ -13,22 +13,21 @@ async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     const RPC_URL = process.env.RPC_URL;\n     const CHAIN_ID = process.env.CHAIN_ID;\n \n-    let customNetwork;\n+    let provider\n \n     // Change the CHAIN_ID in the .env file to deploy to a different network\n     if (RPC_URL === \"http://127.0.0.1:8545\") {\n-        customNetwork =  {\n+        console.log(\"\ud83d\udd17 | Connecting to local network: \", RPC_URL)\n+       const  customNetwork =  {\n             chainId: parseInt(CHAIN_ID),\n             name: \"local\"\n         };\n+         provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n     } else {\n-        customNetwork =  {\n-            // Change the CHAIN_ID in the .env file to deploy to a different network\n-            chainId: parseInt(CHAIN_ID),\n-        };\n-    }\n-\n-    const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n+            console.log(\"\ud83d\udd17 | Connecting to network: \", RPC_URL)\n+            provider = new ethers.JsonRpcProvider(RPC_URL);\n+    } \n+   \n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n \n     console.log(\"\ud83d\uddfd | Wallet address: \", wallet.address);"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 8,
                "deletions": 9,
                "patch": "@@ -8,24 +8,23 @@ config();\n async function getContractInstance(address) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n     const RPC_URL = process.env.RPC_URL;\n-    const CHAIN_ID = parseInt(process.env.CHAIN_ID); // Convert to integer\n+    const CHAIN_ID = process.env.CHAIN_ID;\n \n-    let customNetwork;\n+    let provider\n \n     // Change the CHAIN_ID in the .env file to deploy to a different network\n     if (RPC_URL === \"http://127.0.0.1:8545\") {\n-        customNetwork =  {\n+        console.log(\"\ud83d\udd17 | Connecting to local network: \", RPC_URL)\n+       const  customNetwork =  {\n             chainId: parseInt(CHAIN_ID),\n             name: \"local\"\n         };\n+         provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n     } else {\n-        customNetwork =  {\n-            // Change the CHAIN_ID in the .env file to deploy to a different network\n-            chainId: parseInt(CHAIN_ID),\n-        };\n-    }\n+            console.log(\"\ud83d\udd17 | Connecting to network: \", RPC_URL)\n+            provider = new ethers.JsonRpcProvider(RPC_URL);\n+    } \n \n-    const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n     const contract = new ethers.Contract(address, CAP_TABLE.abi, wallet);\n     const libraries = getTXLibContracts(contract.target, wallet);"
            }
        ]
    },
    {
        "sha": "1d8f61757aa1531cd3806352c351bfa1266fa733",
        "author": "victormimo",
        "date": "2024-01-12 21:28:35+00:00",
        "message": "updating base route",
        "files": [
            {
                "filename": "src/routes/index.js",
                "additions": 5,
                "deletions": 0,
                "patch": "@@ -9,6 +9,11 @@ import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n \n const router = Router();\n \n+router.get(\"/\", async (req, res) => {\n+    console.log(\"Welcome to TAP\")\n+    res.status(200).send(`Welcome to the future of Transfer Agents \ud83d\udcb8`);\n+})\n+\n \n router.post(\"/mint-cap-table\", async (req, res) => {\n     try {"
            }
        ]
    },
    {
        "sha": "84591f2f22518a7174d3ff6c8eaedefef0ef5282",
        "author": "victormimo",
        "date": "2024-01-12 21:27:00+00:00",
        "message": "updating logs",
        "files": [
            {
                "filename": "src/server.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -73,7 +73,7 @@ app.use(\"/historical-transactions\", historicalTransactions);\n app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n \n app.listen(PORT, async () => {\n-    console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n+    console.log(`\ud83d\ude80  Server successfully launched on port ${PORT}`);\n      // Fetch all issuers\n      const issuers = await readAllIssuers();\n      if (issuers && issuers.length > 0) {"
            }
        ]
    },
    {
        "sha": "5c16913b06983039d5d074620d4b51be0fd07987",
        "author": "kentkolze",
        "date": "2024-01-11 18:45:38+00:00",
        "message": "better docs and test",
        "files": [
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 4,
                "deletions": 3,
                "patch": "@@ -2,7 +2,7 @@ import { trimEvents } from \"../../chain-operations/transactionPoller\";\n \n // TODO: if starts failing again: yarn add --dev jest-esm-transformer\n \n-const myEvents = [5, 6, 6, 6, 7].map((x, i) => { return {blockNumber: x, i}; });\n+const myEvents = [5, 6, 6, 6, 7, 7, 7].map((x, i) => { return {blockNumber: x, i}; });\n \n test('trimEvents partial', () => {\n     const [events, block] = trimEvents(myEvents, 2, 10);\n@@ -12,8 +12,9 @@ test('trimEvents partial', () => {\n });\n \n test('trimEvents full', () => {\n-    for (const max of [5, 6, 15]) {\n-        const [events, block] = trimEvents(myEvents, max, 10);\n+    // We allow more than maxEvents in order to include all events of the last block\n+    for (const maxEvents of [5, 6, 7, 15]) {\n+        const [events, block] = trimEvents(myEvents, maxEvents, 10);\n         expect(events.length).toBe(myEvents.length);\n         expect(events).toStrictEqual(myEvents);\n         expect(block).toBe(10);"
            }
        ]
    },
    {
        "sha": "7314bf30a0c651c6d4be8ca66f5176a9f2168e99",
        "author": "kentkolze",
        "date": "2024-01-11 18:43:33+00:00",
        "message": "fix trimEvents and tests",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 4,
                "deletions": 6,
                "patch": "@@ -182,23 +182,21 @@ const persistEvents = async (issuerId, events) => {\n export const trimEvents = (events, maxEvents, endBlock) => {\n     let index = 0;    \n     while (index < maxEvents && index < events.length) {\n-        // Iterate through the entire next block\n+        // Include the entire next block\n         const includeBlock = events[index].blockNumber;\n         index++;\n         while (index < events.length && events[index].blockNumber === includeBlock) {\n             index++;\n         }\n     }\n-\n     // Nothing to trim!\n-    if (index >= (events.length - 1)) {\n+    if (index >= events.length) {\n         return [events, endBlock];\n     }\n-\n-    // Trim up to index (exclusive)\n     // We processed up to the last events' blockNumber\n+    // `index` is *exclusive* when trimming\n     const useEvents = [...events.slice(0, index)];\n-    return [useEvents, trimEvents[useEvents.length - 1].blockNumber];\n+    return [useEvents, useEvents[useEvents.length - 1].blockNumber];\n };\n \n "
            },
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 7,
                "deletions": 5,
                "patch": "@@ -7,13 +7,15 @@ const myEvents = [5, 6, 6, 6, 7].map((x, i) => { return {blockNumber: x, i}; });\n test('trimEvents partial', () => {\n     const [events, block] = trimEvents(myEvents, 2, 10);\n     expect(events.length).toBe(4);\n-    expect(events).toBe(myEvents.slice(0, 4));\n+    expect(events).toStrictEqual(myEvents.slice(0, 4));\n     expect(block).toBe(6);\n });\n \n test('trimEvents full', () => {\n-    const [events, block] = trimEvents(myEvents, 2, 10);\n-    expect(events.length).toBe(myEvents.length);\n-    expect(events).toBe(myEvents);\n-    expect(block).toBe(10);\n+    for (const max of [5, 6, 15]) {\n+        const [events, block] = trimEvents(myEvents, max, 10);\n+        expect(events.length).toBe(myEvents.length);\n+        expect(events).toStrictEqual(myEvents);\n+        expect(block).toBe(10);\n+    }\n });"
            }
        ]
    },
    {
        "sha": "a32fa39736ba39bffebc12f644b2ce47587a1245",
        "author": "victormimo",
        "date": "2024-01-11 18:33:07+00:00",
        "message": "adding scaffold for equity compensation and covertible issuance",
        "files": [
            {
                "filename": "src/db/operations/create.js",
                "additions": 12,
                "deletions": 0,
                "patch": "@@ -8,6 +8,8 @@ import VestingTerms from \"../objects/VestingTerms.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n+import EquityCompensationIssuance from \"../objects/transactions/issuance/EquityCompensationIssuance.js\";\n+import ConvertibleIssuance from \"../objects/transactions/issuance/ConvertibleIssuance.js\";\n \n export const createIssuer = (issuerData) => {\n     const issuer = new Issuer(issuerData);\n@@ -54,6 +56,16 @@ export const createStockIssuance = (stockIssuanceData) => {\n     return stockIssuance.save();\n };\n \n+export const createEquityCompensationIssuance = (issuanceData) => {\n+    const equityCompensationIssuance = new EquityCompensationIssuance(issuanceData);\n+    return equityCompensationIssuance.save();\n+};\n+\n+export const createConvertibleIssuance = (issuanceData) => {\n+    const convertibleIssuance = new ConvertibleIssuance(issuanceData);\n+    return convertibleIssuance.save();\n+};\n+\n export const createStockTransfer = (stockTransferData) => {\n     const stockTransfer = new StockTransfer(stockTransferData);\n     return stockTransfer.save();"
            },
            {
                "filename": "src/routes/transactions.js",
                "additions": 59,
                "deletions": 2,
                "patch": "@@ -9,6 +9,8 @@ import stockRepurchaseSchema from \"../../ocf/schema/objects/transactions/repurch\n import stockRetractionSchema from \"../../ocf/schema/objects/transactions/retraction/StockRetraction.schema.json\" assert { type: \"json\" };\n import stockClassAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n import issuerAuthorizedSharesAdjustmentSchema from \"../../ocf/schema/objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.schema.json\" assert { type: \"json\" };\n+import equityCompensationIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/EquityCompensationIssuance.schema.json\" assert { type: \"json\" };\n+import convertibleIssuanceSchema from \"../../ocf/schema/objects/transactions/issuance/ConvertibleIssuance.schema.json\" assert { type: \"json\" };\n \n import { convertAndAdjustIssuerAuthorizedSharesOnChain } from \"../controllers/issuerController.js\";\n import { convertAndAdjustStockClassAuthorizedSharesOnchain } from \"../controllers/stockClassController.js\";\n@@ -19,6 +21,8 @@ import { convertAndCreateReissuanceStockOnchain } from \"../controllers/transacti\n import { convertAndCreateRepurchaseStockOnchain } from \"../controllers/transactions/repurchaseController.js\";\n import { convertAndCreateRetractionStockOnchain } from \"../controllers/transactions/retractionController.js\";\n import { convertAndCreateTransferStockOnchain } from \"../controllers/transactions/transferController.js\";\n+import { createEquityCompensationIssuance } from \"../db/operations/create.js\";\n+import { createConvertibleIssuance } from \"../db/operations/create.js\";\n \n import { readIssuerById } from \"../db/operations/read.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n@@ -272,8 +276,6 @@ transactions.post(\"/adjust/stock-class/authorized-shares\", async (req, res) => {\n \n         console.log(\"stockClassAuthorizedSharesAdjustment\", stockClassAuthorizedSharesAdjustment);\n \n-        // delete incomingStockClassAdjustment.stockClassId;\n-\n         // NOTE: schema validation does not include stakeholder, stockClassId, however these properties are needed on to be passed on chain\n         await validateInputAgainstOCF(stockClassAuthorizedSharesAdjustment, stockClassAuthorizedSharesAdjustmentSchema);\n \n@@ -289,4 +291,59 @@ transactions.post(\"/adjust/stock-class/authorized-shares\", async (req, res) => {\n     }\n });\n \n+transactions.post(\"/issuance/equity-compensation\", async (req, res) => {\n+    const { issuerId, data } = req.body;\n+\n+    try {\n+        // ensuring issuer exists\n+        await readIssuerById(issuerId);\n+\n+        const incomingEquityCompensationIssuance = {\n+            id: uuid(), // for OCF Validation\n+            security_id: uuid(), // for OCF Validation,\n+            date: new Date().toISOString().slice(0, 10), // for OCF Validation\n+            object_type: \"TX_EQUITY_COMPENSATION_ISSUANCE\",\n+            ...data,\n+        };\n+        await validateInputAgainstOCF(incomingEquityCompensationIssuance, equityCompensationIssuanceSchema);\n+\n+        // save to DB\n+        const createdIssuance = await createEquityCompensationIssuance(incomingEquityCompensationIssuance);\n+\n+        res.status(200).send({ equityCompensationIssuance: createdIssuance });\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+})\n+\n+\n+transactions.post(\"/issuance/convertible\", async (req, res) => {\n+    const { issuerId, data } = req.body;\n+\n+    try {\n+        // ensuring issuer exists\n+        await readIssuerById(issuerId);\n+\n+        const incomingConvertibleIssuance = {\n+            id: uuid(), // for OCF Validation\n+            security_id: uuid(), // for OCF Validation\n+            date: new Date().toISOString().slice(0, 10), // for OCF Validation\n+            object_type: \"TX_CONVERTIBLE_ISSUANCE\",\n+            ...data,\n+        };\n+\n+        console.log('incomingConvertibleIssuance', incomingConvertibleIssuance)\n+        await validateInputAgainstOCF(incomingConvertibleIssuance, convertibleIssuanceSchema);\n+\n+        // save to DB\n+        const createdIssuance = await createConvertibleIssuance(incomingConvertibleIssuance);\n+\n+        res.status(200).send({ convertibleIssuance: createdIssuance });\n+    } catch (error) {\n+        console.error(`error: ${error}`);\n+        res.status(500).send(`${error}`);\n+    }\n+})\n+\n export default transactions;"
            }
        ]
    },
    {
        "sha": "d09add58eb7a44a8d2f2cbaa172115396c1f287e",
        "author": "victormimo",
        "date": "2024-01-11 18:32:27+00:00",
        "message": "uncommenting",
        "files": [
            {
                "filename": "src/scripts/testMintingCapTable.js",
                "additions": 15,
                "deletions": 15,
                "patch": "@@ -10,32 +10,32 @@ const main = async () => {\n \n     console.log(\"\u2705 | Issuer response \", issuerResponse.data);\n \n-    // await sleep(3000);\n+    await sleep(3000);\n \n-    // console.log(\"\u23f3 | Creating first stakeholder\");\n+    console.log(\"\u23f3 | Creating first stakeholder\");\n \n-    // // create two stakeholders\n-    // const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerResponse.data.issuer._id));\n+    // create two stakeholders\n+    const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerResponse.data.issuer._id));\n \n-    // console.log(\"\u2705 | stakeholder1Response\", stakeholder1Response.data);\n-    // console.log(\"\u2705 | finished\");\n+    console.log(\"\u2705 | stakeholder1Response\", stakeholder1Response.data);\n+    console.log(\"\u2705 | finished\");\n \n-    // await sleep(3000);\n+    await sleep(3000);\n \n-    // console.log(\"\u23f3 | Creating second stakeholder\u2026\");\n+    console.log(\"\u23f3 | Creating second stakeholder\u2026\");\n \n-    // const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerResponse.data.issuer._id));\n+    const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerResponse.data.issuer._id));\n \n-    // console.log(\"\u2705 | stakeholder2Response\", stakeholder2Response.data);\n+    console.log(\"\u2705 | stakeholder2Response\", stakeholder2Response.data);\n \n-    // await sleep(3000);\n+    await sleep(3000);\n \n-    // console.log(\"\u23f3| Creating stock class\");\n+    console.log(\"\u23f3| Creating stock class\");\n \n-    // // create stockClass\n-    // const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerResponse.data.issuer._id));\n+    // create stockClass\n+    const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerResponse.data.issuer._id));\n \n-    // console.log(\"\u2705 | stockClassResponse\", stockClassResponse.data);\n+    console.log(\"\u2705 | stockClassResponse\", stockClassResponse.data);\n };\n \n main()"
            }
        ]
    },
    {
        "sha": "a00fd51a3982f9591bf956cb20ace9965782c3ef",
        "author": "kentkolze",
        "date": "2024-01-11 18:31:33+00:00",
        "message": "get jest unittests setup working",
        "files": [
            {
                "filename": "jest.config.js",
                "additions": 200,
                "deletions": 0,
                "patch": "@@ -0,0 +1,200 @@\n+/**\n+ * For a detailed explanation regarding each configuration property, visit:\n+ * https://jestjs.io/docs/configuration\n+ */\n+\n+/** @type {import('jest').Config} */\n+const config = {\n+  // All imported modules in your tests should be mocked automatically\n+  // automock: false,\n+\n+  // Stop running tests after `n` failures\n+  // bail: 0,\n+\n+  // The directory where Jest should store its cached dependency information\n+  // cacheDirectory: \"/tmp/jest_rs\",\n+\n+  // Automatically clear mock calls, instances, contexts and results before every test\n+  // clearMocks: false,\n+\n+  // Indicates whether the coverage information should be collected while executing the test\n+  // collectCoverage: false,\n+\n+  // An array of glob patterns indicating a set of files for which coverage information should be collected\n+  // collectCoverageFrom: undefined,\n+\n+  // The directory where Jest should output its coverage files\n+  // coverageDirectory: \"coverage\",\n+\n+  // An array of regexp pattern strings used to skip coverage collection\n+  // coveragePathIgnorePatterns: [\n+  //   \"/node_modules/\"\n+  // ],\n+\n+  // Indicates which provider should be used to instrument code for coverage\n+  // coverageProvider: \"babel\",\n+\n+  // A list of reporter names that Jest uses when writing coverage reports\n+  // coverageReporters: [\n+  //   \"json\",\n+  //   \"text\",\n+  //   \"lcov\",\n+  //   \"clover\"\n+  // ],\n+\n+  // An object that configures minimum threshold enforcement for coverage results\n+  // coverageThreshold: undefined,\n+\n+  // A path to a custom dependency extractor\n+  // dependencyExtractor: undefined,\n+\n+  // Make calling deprecated APIs throw helpful error messages\n+  // errorOnDeprecated: false,\n+\n+  // The default configuration for fake timers\n+  // fakeTimers: {\n+  //   \"enableGlobally\": false\n+  // },\n+\n+  // Force coverage collection from ignored files using an array of glob patterns\n+  // forceCoverageMatch: [],\n+\n+  // A path to a module which exports an async function that is triggered once before all test suites\n+  // globalSetup: undefined,\n+\n+  // A path to a module which exports an async function that is triggered once after all test suites\n+  // globalTeardown: undefined,\n+\n+  // A set of global variables that need to be available in all test environments\n+  // globals: {},\n+\n+  // The maximum amount of workers used to run your tests. Can be specified as % or a number. E.g. maxWorkers: 10% will use 10% of your CPU amount + 1 as the maximum worker number. maxWorkers: 2 will use a maximum of 2 workers.\n+  // maxWorkers: \"50%\",\n+\n+  // An array of directory names to be searched recursively up from the requiring module's location\n+  // moduleDirectories: [\n+  //   \"node_modules\"\n+  // ],\n+\n+  // An array of file extensions your modules use\n+  // moduleFileExtensions: [\n+  //   \"js\",\n+  //   \"mjs\",\n+  //   \"cjs\",\n+  //   \"jsx\",\n+  //   \"ts\",\n+  //   \"tsx\",\n+  //   \"json\",\n+  //   \"node\"\n+  // ],\n+\n+  // A map from regular expressions to module names or to arrays of module names that allow to stub out resources with a single module\n+  // moduleNameMapper: {},\n+\n+  // An array of regexp pattern strings, matched against all module paths before considered 'visible' to the module loader\n+  // modulePathIgnorePatterns: [],\n+\n+  // Activates notifications for test results\n+  // notify: false,\n+\n+  // An enum that specifies notification mode. Requires { notify: true }\n+  // notifyMode: \"failure-change\",\n+\n+  // A preset that is used as a base for Jest's configuration\n+  preset: \"ts-jest\",\n+\n+  // Run tests from one or more projects\n+  // projects: undefined,\n+\n+  // Use this configuration option to add custom reporters to Jest\n+  // reporters: undefined,\n+\n+  // Automatically reset mock state before every test\n+  // resetMocks: false,\n+\n+  // Reset the module registry before running each individual test\n+  // resetModules: false,\n+\n+  // A path to a custom resolver\n+  // resolver: undefined,\n+\n+  // Automatically restore mock state and implementation before every test\n+  // restoreMocks: false,\n+\n+  // The root directory that Jest should scan for tests and modules within\n+  // rootDir: undefined,\n+\n+  // A list of paths to directories that Jest should use to search for files in\n+  // roots: [\n+  //   \"<rootDir>\"\n+  // ],\n+\n+  // Allows you to use a custom runner instead of Jest's default test runner\n+  // runner: \"jest-runner\",\n+\n+  // The paths to modules that run some code to configure or set up the testing environment before each test\n+  // setupFiles: [],\n+\n+  // A list of paths to modules that run some code to configure or set up the testing framework before each test\n+  // setupFilesAfterEnv: [],\n+\n+  // The number of seconds after which a test is considered as slow and reported as such in the results.\n+  // slowTestThreshold: 5,\n+\n+  // A list of paths to snapshot serializer modules Jest should use for snapshot testing\n+  // snapshotSerializers: [],\n+\n+  // The test environment that will be used for testing\n+  // testEnvironment: \"jest-environment-node\",\n+\n+  // Options that will be passed to the testEnvironment\n+  // testEnvironmentOptions: {},\n+\n+  // Adds a location field to test results\n+  // testLocationInResults: false,\n+\n+  // The glob patterns Jest uses to detect test files\n+  // testMatch: [\n+  //   \"**/__tests__/**/*.[jt]s?(x)\",\n+  //   \"**/?(*.)+(spec|test).[tj]s?(x)\"\n+  // ],\n+\n+  // An array of regexp pattern strings that are matched against all test paths, matched tests are skipped\n+  // testPathIgnorePatterns: [\n+  //   \"/node_modules/\"\n+  // ],\n+\n+  // The regexp pattern or array of patterns that Jest uses to detect test files\n+  // testRegex: [],\n+\n+  // This option allows the use of a custom results processor\n+  // testResultsProcessor: undefined,\n+\n+  // This option allows use of a custom test runner\n+  // testRunner: \"jest-circus/runner\",\n+\n+  // A map from regular expressions to paths to transformers\n+  transform: {\n+    \".*\\\\.(tsx?|js)$\": \"ts-jest\",\n+  },\n+\n+  // An array of regexp pattern strings that are matched against all source file paths, matched files will skip transformation\n+  // transformIgnorePatterns: [\n+  //   \"/node_modules/\",\n+  //   \"\\\\.pnp\\\\.[^\\\\/]+$\"\n+  // ],\n+\n+  // An array of regexp pattern strings that are matched against all modules before the module loader will automatically return a mock for them\n+  // unmockedModulePathPatterns: undefined,\n+\n+  // Indicates whether each individual test should be reported during the run\n+  verbose: true,\n+\n+  // An array of regexp patterns that are matched against all source file paths before re-running tests in watch mode\n+  // watchPathIgnorePatterns: [],\n+\n+  // Whether to use watchman for file crawling\n+  // watchman: true,\n+};\n+\n+export default config;"
            },
            {
                "filename": "package.json",
                "additions": 7,
                "deletions": 1,
                "patch": "@@ -19,6 +19,8 @@\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n         \"deseed\": \"node src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\",\n+        \"test-js\": \"jest --testPathPattern /src/tests/unit\",\n+        \"test-js-integration\": \"jest --testPathPattern /src/tests/integration\",\n         \"validate-poet-files\": \"cd ocf && yarn validate-poet-files\",\n         \"test-onchain-cap-table-local\": \"node src/chain-operations/capTable.cjs local\",\n         \"test-onchain-cap-table-optimism-goerli\": \"node src/chain-operations/capTable.cjs optimism-goerli\",\n@@ -41,6 +43,7 @@\n         \"express\": \"^4.18.2\",\n         \"mongoose\": \"^7.4.2\",\n         \"solc\": \"^0.8.20\",\n+        \"typescript\": \"^5.3.3\",\n         \"xstate\": \"^4.38.2\",\n         \"yauzl\": \"^2.10.0\"\n     },\n@@ -50,18 +53,21 @@\n         \"maintained node versions\"\n     ],\n     \"devDependencies\": {\n+        \"@types/jest\": \"^29.5.11\",\n         \"@types/node\": \"^20.3.2\",\n         \"@types/uuid\": \"^9.0.2\",\n         \"eslint\": \"^8.21.0\",\n         \"eslint-config-next\": \"^13.1.6\",\n         \"eslint-config-prettier\": \"^8.5.0\",\n         \"eslint-plugin-import\": \"^2.26.0\",\n         \"husky\": \"^8.0.1\",\n+        \"jest\": \"^29.7.0\",\n         \"lint-staged\": \"^13.0.3\",\n         \"nodemon\": \"^3.0.1\",\n         \"prettier\": \"^2.7.1\",\n         \"solhint\": \"^3.4.1\",\n-        \"typescript\": \"^5.1.6\",\n+        \"ts-jest\": \"^29.1.1\",\n+        \"ts-node\": \"^10.9.2\",\n         \"uuid\": \"^9.0.0\"\n     }\n }"
            },
            {
                "filename": "src/chain-operations/seed.js",
                "additions": 2,
                "deletions": 4,
                "patch": "@@ -1,12 +1,10 @@\n import { convertAndReflectStakeholderOnchain } from \"../controllers/stakeholderController.js\";\n import { convertAndReflectStockClassOnchain } from \"../controllers/stockClassController.js\";\n-import { convertAndSeedIssuanceStockOnchain, convertAndSeedTransferStockOnchain } from \"../controllers/transactions/seed.js\";\n-import { getAllIssuerDataById } from \"../db/operations/read.js\";\n+import { getAllIssuerDataById, readIssuerById } from \"../db/operations/read.js\";\n+import { preProcessorCache } from \"../utils/caches.ts\";\n import { convertTimeStampToUint40, toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import { convertBytes16ToUUID, convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import { extractArrays } from \"../utils/flattenPreprocessorCache.js\";\n-import { preProcessorCache } from \"../utils/caches.js\";\n-import { readIssuerById } from \"../db/operations/read.js\";\n import sleep from \"../utils/sleep.js\";\n \n export const verifyIssuerAndSeed = async (contract, id) => {"
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 7,
                "deletions": 4,
                "patch": "@@ -1,9 +1,9 @@\n import { AbiCoder } from \"ethers\";\n-import connectDB from \"../db/config/mongoose.js\";\n-import { withGlobalTransaction } from \"../db/operations/atomic.js\";\n+import { connectDB } from \"../db/config/mongoose.ts\";\n+import { withGlobalTransaction } from \"../db/operations/atomic.ts\";\n import { readAllIssuers } from \"../db/operations/read.js\";\n import { updateIssuerById } from \"../db/operations/update.js\";\n-import { getIssuerContract } from \"../utils/caches.js\";\n+import { getIssuerContract } from \"../utils/caches.ts\";\n import { verifyIssuerAndSeed } from \"./seed.js\";\n import {\n     IssuerAuthorizedSharesAdjustment,\n@@ -56,6 +56,7 @@ const txFuncs = new Map(\n     Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_x, [name, _y, handleFunc]]) => [name, handleFunc])\n );\n \n+\n const sleep = (delay) => new Promise((resolve) => setTimeout(resolve, delay));\n \n export const startSynchronousEventProcessing = async () => {\n@@ -115,6 +116,8 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBl\n             continue;\n         }\n         // TODO: does txTypeIdx even come with the event??? need to test this...\n+        let txTypeIdx;\n+        let txData;\n         const [type, structType] = txMapper[txTypeIdx];\n         const decodedData = abiCoder.decode([structType], txData);\n         const { timestamp } = await provider.getBlock(event.blockNumber);\n@@ -176,7 +179,7 @@ const persistEvents = async (issuerId, events) => {\n     }\n };\n \n-const trimEvents = (events, maxEvents, endBlock) => {\n+export const trimEvents = (events, maxEvents, endBlock) => {\n     let index = 0;    \n     while (index < maxEvents && index < events.length) {\n         // Iterate through the entire next block"
            },
            {
                "filename": "src/db/config/mongoose.ts",
                "additions": 2,
                "deletions": 4,
                "patch": "@@ -1,11 +1,11 @@\n-import mongoose from \"mongoose\";\n import dotenv from \"dotenv\";\n+import mongoose from \"mongoose\";\n \n dotenv.config();\n \n const DATABASE_URL = process.env.DATABASE_URL;\n \n-const connectDB = async () => {\n+export const connectDB = async () => {\n     try {\n         await mongoose.connect(DATABASE_URL);\n         console.log(\"\u2705 | Mongo connected succesfully\");\n@@ -16,5 +16,3 @@ const connectDB = async () => {\n         process.exit(1);\n     }\n };\n-\n-export default connectDB;"
            },
            {
                "filename": "src/db/operations/atomic.ts",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,7 +1,7 @@\n // Store a global mongo session to allows us to bundle CRUD operations into one transaction\n \n import { Connection, QueryOptions } from \"mongoose\";\n-import connectDB from \"../config/mongoose\";\n+import { connectDB } from \"../config/mongoose.ts\";\n type TQueryOptions = QueryOptions | null;\n \n let _globalSession = null;"
            },
            {
                "filename": "src/db/operations/create.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -8,7 +8,7 @@ import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import { save } from \"./atomic.js\";\n+import { save } from \"./atomic.ts\";\n \n export const createIssuer = (issuerData) => {\n     return save(new Issuer(issuerData));"
            },
            {
                "filename": "src/db/operations/read.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -9,7 +9,7 @@ import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import { countDocuments, find, findById } from \"./atomic.js\";\n+import { countDocuments, find, findById } from \"./atomic.ts\";\n \n // READ By ID\n export const readIssuerById = async (id) => {"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -8,8 +8,8 @@ import { countIssuers, readIssuerById } from \"../db/operations/read.js\";\n import { convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import validateInputAgainstOCF from \"../utils/validateInputAgainstSchema.js\";\n \n-import { contractCache } from \"../utils/caches.js\";\n import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n+import { contractCache } from \"../utils/caches.ts\";\n \n const issuer = Router();\n "
            },
            {
                "filename": "src/server.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -19,7 +19,7 @@ import valuationRoutes from \"./routes/valuation.js\";\n import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n \n import { readIssuerById } from \"./db/operations/read.js\";\n-import { getIssuerContract } from \"./utils/caches.js\";\n+import { getIssuerContract } from \"./utils/caches.ts\";\n \n const app = express();\n "
            },
            {
                "filename": "src/state-machines/process.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,6 +1,6 @@\n import { interpret } from \"xstate\";\n+import { preProcessorCache } from \"../utils/caches.ts\";\n import { parentMachine } from \"./parent.js\";\n-import { preProcessorCache } from \"../utils/caches.js\";\n \n /*\n     @dev: Parent-Child machines are created to calculate current context then deleted."
            },
            {
                "filename": "src/tests/integration/transactionPoller.test.ts",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -0,0 +1 @@\n+"
            },
            {
                "filename": "src/tests/unit/transactionPoller.test.ts",
                "additions": 19,
                "deletions": 0,
                "patch": "@@ -0,0 +1,19 @@\n+import { trimEvents } from \"../../chain-operations/transactionPoller\";\n+\n+// TODO: if starts failing again: yarn add --dev jest-esm-transformer\n+\n+const myEvents = [5, 6, 6, 6, 7].map((x, i) => { return {blockNumber: x, i}; });\n+\n+test('trimEvents partial', () => {\n+    const [events, block] = trimEvents(myEvents, 2, 10);\n+    expect(events.length).toBe(4);\n+    expect(events).toBe(myEvents.slice(0, 4));\n+    expect(block).toBe(6);\n+});\n+\n+test('trimEvents full', () => {\n+    const [events, block] = trimEvents(myEvents, 2, 10);\n+    expect(events.length).toBe(myEvents.length);\n+    expect(events).toBe(myEvents);\n+    expect(block).toBe(10);\n+});"
            },
            {
                "filename": "tsconfig.json",
                "additions": 109,
                "deletions": 0,
                "patch": "@@ -0,0 +1,109 @@\n+{\n+  \"compilerOptions\": {\n+    /* Visit https://aka.ms/tsconfig to read more about this file */\n+\n+    /* Projects */\n+    // \"incremental\": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */\n+    // \"composite\": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */\n+    // \"tsBuildInfoFile\": \"./.tsbuildinfo\",              /* Specify the path to .tsbuildinfo incremental compilation file. */\n+    // \"disableSourceOfProjectReferenceRedirect\": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */\n+    // \"disableSolutionSearching\": true,                 /* Opt a project out of multi-project reference checking when editing. */\n+    // \"disableReferencedProjectLoad\": true,             /* Reduce the number of projects loaded automatically by TypeScript. */\n+\n+    /* Language and Environment */\n+    \"target\": \"ESNext\",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */\n+    // \"lib\": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */\n+    // \"jsx\": \"preserve\",                                /* Specify what JSX code is generated. */\n+    // \"experimentalDecorators\": true,                   /* Enable experimental support for legacy experimental decorators. */\n+    // \"emitDecoratorMetadata\": true,                    /* Emit design-type metadata for decorated declarations in source files. */\n+    // \"jsxFactory\": \"\",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */\n+    // \"jsxFragmentFactory\": \"\",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */\n+    // \"jsxImportSource\": \"\",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */\n+    // \"reactNamespace\": \"\",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */\n+    // \"noLib\": true,                                    /* Disable including any library files, including the default lib.d.ts. */\n+    // \"useDefineForClassFields\": true,                  /* Emit ECMAScript-standard-compliant class fields. */\n+    // \"moduleDetection\": \"auto\",                        /* Control what method is used to detect module-format JS files. */\n+\n+    /* Modules */\n+    \"module\": \"ESNext\",                                /* Specify what module code is generated. */\n+    // \"rootDir\": \"./\",                                  /* Specify the root folder within your source files. */\n+    \"moduleResolution\": \"node\",                     /* Specify how TypeScript looks up a file from a given module specifier. */\n+    // \"baseUrl\": \"./\",                                  /* Specify the base directory to resolve non-relative module names. */\n+    // \"paths\": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */\n+    // \"rootDirs\": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */\n+    // \"typeRoots\": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */\n+    // \"types\": [],                                      /* Specify type package names to be included without being referenced in a source file. */\n+    // \"allowUmdGlobalAccess\": true,                     /* Allow accessing UMD globals from modules. */\n+    // \"moduleSuffixes\": [],                             /* List of file name suffixes to search when resolving a module. */\n+    \"allowImportingTsExtensions\": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */\n+    // \"resolvePackageJsonExports\": true,                /* Use the package.json 'exports' field when resolving package imports. */\n+    // \"resolvePackageJsonImports\": true,                /* Use the package.json 'imports' field when resolving imports. */\n+    // \"customConditions\": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */\n+    // \"resolveJsonModule\": true,                        /* Enable importing .json files. */\n+    // \"allowArbitraryExtensions\": true,                 /* Enable importing files with any extension, provided a declaration file is present. */\n+    // \"noResolve\": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */\n+\n+    /* JavaScript Support */\n+    \"allowJs\": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */\n+    // \"checkJs\": true,                                  /* Enable error reporting in type-checked JavaScript files. */\n+    // \"maxNodeModuleJsDepth\": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */\n+\n+    /* Emit */\n+    // \"declaration\": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */\n+    // \"declarationMap\": true,                           /* Create sourcemaps for d.ts files. */\n+    // \"emitDeclarationOnly\": true,                      /* Only output d.ts files and not JavaScript files. */\n+    \"sourceMap\": true,                                /* Create source map files for emitted JavaScript files. */\n+    // \"inlineSourceMap\": true,                          /* Include sourcemap files inside the emitted JavaScript. */\n+    // \"outFile\": \"./\",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */\n+    // \"outDir\": \"./\",                                   /* Specify an output folder for all emitted files. */\n+    // \"removeComments\": true,                           /* Disable emitting comments. */\n+    // \"noEmit\": true,                                   /* Disable emitting files from a compilation. */\n+    // \"importHelpers\": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */\n+    // \"importsNotUsedAsValues\": \"remove\",               /* Specify emit/checking behavior for imports that are only used for types. */\n+    // \"downlevelIteration\": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */\n+    // \"sourceRoot\": \"\",                                 /* Specify the root path for debuggers to find the reference source code. */\n+    // \"mapRoot\": \"\",                                    /* Specify the location where debugger should locate map files instead of generated locations. */\n+    // \"inlineSources\": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */\n+    // \"emitBOM\": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */\n+    // \"newLine\": \"crlf\",                                /* Set the newline character for emitting files. */\n+    // \"stripInternal\": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */\n+    // \"noEmitHelpers\": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */\n+    // \"noEmitOnError\": true,                            /* Disable emitting files if any type checking errors are reported. */\n+    // \"preserveConstEnums\": true,                       /* Disable erasing 'const enum' declarations in generated code. */\n+    // \"declarationDir\": \"./\",                           /* Specify the output directory for generated declaration files. */\n+    // \"preserveValueImports\": true,                     /* Preserve unused imported values in the JavaScript output that would otherwise be removed. */\n+\n+    /* Interop Constraints */\n+    // \"isolatedModules\": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */\n+    // \"verbatimModuleSyntax\": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */\n+    // \"allowSyntheticDefaultImports\": true,             /* Allow 'import x from y' when a module doesn't have a default export. */\n+    \"esModuleInterop\": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */\n+    // \"preserveSymlinks\": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */\n+    \"forceConsistentCasingInFileNames\": true,            /* Ensure that casing is correct in imports. */\n+\n+    /* Type Checking */\n+    \"strict\": false,                                      /* Enable all strict type-checking options. */\n+    // \"noImplicitAny\": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */\n+    // \"strictNullChecks\": true,                         /* When type checking, take into account 'null' and 'undefined'. */\n+    // \"strictFunctionTypes\": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */\n+    // \"strictBindCallApply\": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */\n+    // \"strictPropertyInitialization\": true,             /* Check for class properties that are declared but not set in the constructor. */\n+    // \"noImplicitThis\": true,                           /* Enable error reporting when 'this' is given the type 'any'. */\n+    // \"useUnknownInCatchVariables\": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */\n+    // \"alwaysStrict\": true,                             /* Ensure 'use strict' is always emitted. */\n+    // \"noUnusedLocals\": true,                           /* Enable error reporting when local variables aren't read. */\n+    // \"noUnusedParameters\": true,                       /* Raise an error when a function parameter isn't read. */\n+    // \"exactOptionalPropertyTypes\": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */\n+    // \"noImplicitReturns\": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */\n+    // \"noFallthroughCasesInSwitch\": true,               /* Enable error reporting for fallthrough cases in switch statements. */\n+    // \"noUncheckedIndexedAccess\": true,                 /* Add 'undefined' to a type when accessed using an index. */\n+    // \"noImplicitOverride\": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */\n+    // \"noPropertyAccessFromIndexSignature\": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */\n+    // \"allowUnusedLabels\": true,                        /* Disable error reporting for unused labels. */\n+    // \"allowUnreachableCode\": true,                     /* Disable error reporting for unreachable code. */\n+\n+    /* Completeness */\n+    // \"skipDefaultLibCheck\": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */\n+    \"skipLibCheck\": true                                 /* Skip type checking all .d.ts files. */\n+  }\n+}"
            },
            {
                "filename": "yarn.lock",
                "additions": 2596,
                "deletions": 812,
                "patch": null
            }
        ]
    },
    {
        "sha": "be19cb75b4f1a18ce9ef085a4f77afffcabf8203",
        "author": "kentkolze",
        "date": "2024-01-11 16:57:05+00:00",
        "message": "add the logic for only processing up to maxEvents at a time",
        "files": [
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 36,
                "deletions": 7,
                "patch": "@@ -52,7 +52,8 @@ const txMapper = {\n \n // Map(event.type => handler) derived from the above\n const txFuncs = new Map(\n-    Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_, [name, _1, handleFunc]]) => [name, handleFunc])\n+    // @ts-ignore\n+    Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_x, [name, _y, handleFunc]]) => [name, handleFunc])\n );\n \n const sleep = (delay) => new Promise((resolve) => setTimeout(resolve, delay));\n@@ -71,9 +72,12 @@ export const startSynchronousEventProcessing = async () => {\n             }\n         }\n     }\n-}\n+};\n \n-const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBlocks = 250, maxEvents = 100) => {\n+const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBlocks = 1500, maxEvents = 250) => {\n+    /*\n+    We process up to `maxEvents` across `maxBlocks` to ensure our transaction sizes dont get too big and bog down our db\n+    */\n     console.log(\" processEvents for issuer\", issuer);\n     let {_id: issuerId, last_processed_block: startBlock, tx_hash: deployedTx} = issuer;\n     if (startBlock === null) {\n@@ -85,7 +89,7 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBl\n         startBlock = await bootstrapTable(issuerId, receipt.blockNumber, contract, dbConn);\n     }\n     const {number: latestBlock} = await provider.getBlock('finalized');\n-    const endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n+    let endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n \n     let events: any[] = [];\n \n@@ -126,12 +130,13 @@ const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBl\n \n     // Process in the correct order\n     events.sort((a, b) => a.blockNumber - b.blockNumber || a.transactionIndex - b.transactionIndex);\n+    [events, endBlock] = trimEvents(events, maxEvents, endBlock);\n \n     await withGlobalTransaction(async () => {\n         await persistEvents(issuerId, events);\n         await updateLastProcessed(issuerId, endBlock);\n     }, dbConn);\n-}\n+};\n \n const bootstrapTable = async (issuerId, deployedBlockNumber, contract, dbConn) => {\n     // TODO: fix the copy-pasted query\n@@ -150,14 +155,15 @@ const bootstrapTable = async (issuerId, deployedBlockNumber, contract, dbConn) =\n     }, dbConn);\n \n     return tMinusOne;\n-}\n+};\n \n const persistEvents = async (issuerId, events) => {\n     // Persist all the necessary changes for each event gathered in process events\n     for (const event of events) {\n         const txHandleFunc = txFuncs.get(event.type);\n         console.log(\"persistEvent: \", event);\n         if (txHandleFunc) {\n+            // @ts-ignore\n             await txHandleFunc(event.data, issuerId, event.timestamp);\n             continue;\n         }\n@@ -168,7 +174,30 @@ const persistEvents = async (issuerId, events) => {\n         }\n         throw new Error(`Invalid transaction type: \"${event.type}\" for ${event}`);\n     }\n-}\n+};\n+\n+const trimEvents = (events, maxEvents, endBlock) => {\n+    let index = 0;    \n+    while (index < maxEvents && index < events.length) {\n+        // Iterate through the entire next block\n+        const includeBlock = events[index].blockNumber;\n+        index++;\n+        while (index < events.length && events[index].blockNumber === includeBlock) {\n+            index++;\n+        }\n+    }\n+\n+    // Nothing to trim!\n+    if (index >= (events.length - 1)) {\n+        return [events, endBlock];\n+    }\n+\n+    // Trim up to index (exclusive)\n+    // We processed up to the last events' blockNumber\n+    const useEvents = [...events.slice(0, index)];\n+    return [useEvents, trimEvents[useEvents.length - 1].blockNumber];\n+};\n+\n \n const updateLastProcessed = async (issuerId, lastProcessedBlock) => {\n     return updateIssuerById(issuerId, {lastProcessedBlock});"
            }
        ]
    },
    {
        "sha": "dfca19dead8b63b85c9953f08c31b11cb2e57f1f",
        "author": "kentkolze",
        "date": "2024-01-11 15:56:00+00:00",
        "message": "first checkin of converting event based processing to poll based processing using transactions",
        "files": [
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 2,
                "deletions": 0,
                "patch": "@@ -54,6 +54,7 @@ async function deployCapTableLocal(issuerId, issuerName, initial_shares_authoriz\n         provider,\n         address: latestCapTableProxyContractAddress,\n         libraries,\n+        deployHash: tx.hash,\n     };\n }\n \n@@ -94,6 +95,7 @@ async function deployCapTableOptimismGoerli(issuerId, issuerName, initial_shares\n         provider,\n         address: latestCapTableProxyContractAddress,\n         libraries,\n+        deployHash: tx.hash,\n     };\n }\n "
            },
            {
                "filename": "src/chain-operations/transactionListener.js",
                "additions": 21,
                "deletions": 15,
                "patch": "@@ -1,18 +1,11 @@\n-import { verifyIssuerAndSeed } from \"./seed.js\";\n-import {\n-    handleStockCancellation,\n-    handleIssuerAuthorizedSharesAdjusted,\n-    handleStockAcceptance,\n-    handleStockReissuance,\n-    handleStockRepurchase,\n-    handleStockRetraction,\n-    handleStockClass,\n-    handleStakeholder,\n-    handleStockIssuance,\n-    handleStockTransfer,\n-    handleStockClassAuthorizedSharesAdjusted,\n-} from \"./transactionHandlers.js\";\n+/*\n+DEPRECATED! DO NOT USE\n+TODO: delete \n+*/\n+\n+\n import { AbiCoder } from \"ethers\";\n+import { verifyIssuerAndSeed } from \"./seed.js\";\n import {\n     IssuerAuthorizedSharesAdjustment,\n     StockAcceptance,\n@@ -24,6 +17,19 @@ import {\n     StockRetraction,\n     StockTransfer,\n } from \"./structs.js\";\n+import {\n+    handleIssuerAuthorizedSharesAdjusted,\n+    handleStakeholder,\n+    handleStockAcceptance,\n+    handleStockCancellation,\n+    handleStockClass,\n+    handleStockClassAuthorizedSharesAdjusted,\n+    handleStockIssuance,\n+    handleStockReissuance,\n+    handleStockRepurchase,\n+    handleStockRetraction,\n+    handleStockTransfer,\n+} from \"./transactionHandlers.js\";\n \n const abiCoder = new AbiCoder();\n const eventQueue = [];\n@@ -120,4 +126,4 @@ async function processEventQueue() {\n     }\n }\n \n-export default startOnchainListeners;\n+// export default startOnchainListeners;"
            },
            {
                "filename": "src/chain-operations/transactionPoller.ts",
                "additions": 175,
                "deletions": 0,
                "patch": "@@ -0,0 +1,175 @@\n+import { AbiCoder } from \"ethers\";\n+import connectDB from \"../db/config/mongoose.js\";\n+import { withGlobalTransaction } from \"../db/operations/atomic.js\";\n+import { readAllIssuers } from \"../db/operations/read.js\";\n+import { updateIssuerById } from \"../db/operations/update.js\";\n+import { getIssuerContract } from \"../utils/caches.js\";\n+import { verifyIssuerAndSeed } from \"./seed.js\";\n+import {\n+    IssuerAuthorizedSharesAdjustment,\n+    StockAcceptance,\n+    StockCancellation,\n+    StockClassAuthorizedSharesAdjustment,\n+    StockIssuance,\n+    StockReissuance,\n+    StockRepurchase,\n+    StockRetraction,\n+    StockTransfer,\n+} from \"./structs.js\";\n+import {\n+    handleIssuerAuthorizedSharesAdjusted,\n+    handleStakeholder,\n+    handleStockAcceptance,\n+    handleStockCancellation,\n+    handleStockClass,\n+    handleStockClassAuthorizedSharesAdjusted,\n+    handleStockIssuance,\n+    handleStockReissuance,\n+    handleStockRepurchase,\n+    handleStockRetraction,\n+    handleStockTransfer,\n+} from \"./transactionHandlers.js\";\n+\n+const abiCoder = new AbiCoder();\n+\n+const contractFuncs = new Map([\n+    [\"StakeholderCreated\", handleStakeholder],\n+    [\"StockClassCreated\", handleStockClass],\n+]);\n+\n+const txMapper = {\n+    0: [\"INVALID\"],\n+    1: [\"ISSUER_AUTHORIZED_SHARES_ADJUSTMENT\", IssuerAuthorizedSharesAdjustment, handleIssuerAuthorizedSharesAdjusted],\n+    2: [\"STOCK_CLASS_AUTHORIZED_SHARES_ADJUSTMENT\", StockClassAuthorizedSharesAdjustment, handleStockClassAuthorizedSharesAdjusted],\n+    3: [\"STOCK_ACCEPTANCE\", StockAcceptance, handleStockAcceptance],\n+    4: [\"STOCK_CANCELLATION\", StockCancellation, handleStockCancellation],\n+    5: [\"STOCK_ISSUANCE\", StockIssuance, handleStockIssuance],\n+    6: [\"STOCK_REISSUANCE\", StockReissuance, handleStockReissuance],\n+    7: [\"STOCK_REPURCHASE\", StockRepurchase, handleStockRepurchase],\n+    8: [\"STOCK_RETRACTION\", StockRetraction, handleStockRetraction],\n+    9: [\"STOCK_TRANSFER\", StockTransfer, handleStockTransfer],\n+};\n+\n+// Map(event.type => handler) derived from the above\n+const txFuncs = new Map(\n+    Object.entries(txMapper).filter((arr) => arr.length === 3).forEach(([_, [name, _1, handleFunc]]) => [name, handleFunc])\n+);\n+\n+const sleep = (delay) => new Promise((resolve) => setTimeout(resolve, delay));\n+\n+export const startSynchronousEventProcessing = async () => {\n+    while (true) {\n+        await sleep(1 * 1000);\n+        const issuers = await readAllIssuers();\n+        const dbConn = await connectDB();\n+        // Process events synchronously for each issuer\n+        console.log(`Processing for ${issuers.length} issuers`);\n+        for (const issuer of issuers) {\n+            if (issuer.deployed_to) {\n+                const { contract, provider, libraries } = await getIssuerContract(issuer);\n+                await processEvents(dbConn, contract, provider, issuer, libraries.txHelper);\n+            }\n+        }\n+    }\n+}\n+\n+const processEvents = async (dbConn, contract, provider, issuer, txHelper, maxBlocks = 250, maxEvents = 100) => {\n+    console.log(\" processEvents for issuer\", issuer);\n+    let {_id: issuerId, last_processed_block: startBlock, tx_hash: deployedTx} = issuer;\n+    if (startBlock === null) {\n+        const receipt = await provider.getTransactionReceipt(deployedTx);\n+        if (!receipt) {\n+            console.error(\"Transaction receipt not found\");\n+            return;\n+        }\n+        startBlock = await bootstrapTable(issuerId, receipt.blockNumber, contract, dbConn);\n+    }\n+    const {number: latestBlock} = await provider.getBlock('finalized');\n+    const endBlock = Math.min(startBlock + maxBlocks, latestBlock);\n+\n+    let events: any[] = [];\n+\n+    // TODO: fix filter\n+    const contractEvents = await contract.queryFilter(\"*\", startBlock, endBlock);\n+    for (const event of contractEvents) {\n+        if (contractFuncs.has(event.type)) {\n+            // TODO: how to deserialize event.data?\n+            console.log(\"contract event: \", event);\n+            events.push(event);\n+        }\n+    }\n+\n+    // TODO: fix filter\n+    const txEvents = await txHelper.queryFilter(\"*\", startBlock, endBlock);\n+    for (const event of txEvents) {\n+        // TODO: the same processing as libraries.txHelper.on     \n+        // TODO:  emit TxCreated(transactions.length, txType, txData);\n+        //  how do we parse the event.data string of each event? \n+        //    https://www.npmjs.com/package/@ethersproject/abstract-provider?activeTab=code (line 102: Log.data is string-type)\n+        console.log(\"txHelper event: \", event);\n+        if (event.removed) {\n+            continue;\n+        }\n+        // TODO: does txTypeIdx even come with the event??? need to test this...\n+        const [type, structType] = txMapper[txTypeIdx];\n+        const decodedData = abiCoder.decode([structType], txData);\n+        const { timestamp } = await provider.getBlock(event.blockNumber);\n+        // TODO: I think the below needs a lot of work\n+        events.push({ ...event, type, timestamp, data: decodedData[0] });\n+    }\n+\n+    // Nothing to process\n+    if (events.length === 0) {\n+        await updateLastProcessed(issuerId, endBlock);\n+        return;\n+    }\n+\n+    // Process in the correct order\n+    events.sort((a, b) => a.blockNumber - b.blockNumber || a.transactionIndex - b.transactionIndex);\n+\n+    await withGlobalTransaction(async () => {\n+        await persistEvents(issuerId, events);\n+        await updateLastProcessed(issuerId, endBlock);\n+    }, dbConn);\n+}\n+\n+const bootstrapTable = async (issuerId, deployedBlockNumber, contract, dbConn) => {\n+    // TODO: fix the copy-pasted query\n+    const issuerCreatedFilter = contract.filters.IssuerCreated;\n+    const issuerEvents = await contract.queryFilter(issuerCreatedFilter);\n+    if (issuerEvents.length === 0) {\n+        throw new Error(`No issuer events found!`);\n+    }\n+    const issuerCreatedEventId = issuerEvents[0].args[0];\n+    console.log(\"IssuerCreated Event Emitted!\", issuerCreatedEventId);\n+    const tMinusOne = deployedBlockNumber - 1;\n+    \n+    await withGlobalTransaction(async () => {\n+        await verifyIssuerAndSeed(contract, issuerCreatedEventId);\n+        await updateLastProcessed(issuerId, tMinusOne);\n+    }, dbConn);\n+\n+    return tMinusOne;\n+}\n+\n+const persistEvents = async (issuerId, events) => {\n+    // Persist all the necessary changes for each event gathered in process events\n+    for (const event of events) {\n+        const txHandleFunc = txFuncs.get(event.type);\n+        console.log(\"persistEvent: \", event);\n+        if (txHandleFunc) {\n+            await txHandleFunc(event.data, issuerId, event.timestamp);\n+            continue;\n+        }\n+        const contractHandleFunc = contractFuncs.get(event.type);\n+        if (contractHandleFunc) {\n+            await contractHandleFunc(event.data);\n+            continue;\n+        }\n+        throw new Error(`Invalid transaction type: \"${event.type}\" for ${event}`);\n+    }\n+}\n+\n+const updateLastProcessed = async (issuerId, lastProcessedBlock) => {\n+    return updateIssuerById(issuerId, {lastProcessedBlock});\n+};"
            },
            {
                "filename": "src/db/config/mongoose.js",
                "additions": 1,
                "deletions": 0,
                "patch": "@@ -9,6 +9,7 @@ const connectDB = async () => {\n     try {\n         await mongoose.connect(DATABASE_URL);\n         console.log(\"\u2705 | Mongo connected succesfully\");\n+        return mongoose.connection;\n     } catch (error) {\n         console.error(\"\u274c | Error connecting to Mongo\", error.message);\n         // Exit process with failure"
            },
            {
                "filename": "src/db/objects/Issuer.js",
                "additions": 2,
                "deletions": 0,
                "patch": "@@ -17,6 +17,8 @@ const IssuerSchema = new mongoose.Schema({\n     initial_shares_authorized: String,\n     comments: [String],\n     deployed_to: String,\n+    tx_hash: String,\n+    last_processed_block: { type: Number, default: null },\n     is_manifest_created: { type: Boolean, default: false },\n }, { timestamps: true });\n "
            },
            {
                "filename": "src/db/operations/atomic.ts",
                "additions": 87,
                "deletions": 0,
                "patch": "@@ -0,0 +1,87 @@\n+// Store a global mongo session to allows us to bundle CRUD operations into one transaction\n+\n+import { Connection, QueryOptions } from \"mongoose\";\n+import connectDB from \"../config/mongoose\";\n+type TQueryOptions = QueryOptions | null;\n+\n+let _globalSession = null;\n+\n+\n+export const setGlobalSession = (session) => {\n+    if (_globalSession !== null) {\n+        throw new Error(\n+            `globalSession is already set! ${_globalSession}. \n+            Nested transactions are not supported`\n+        );\n+    }\n+    _globalSession = session;\n+}\n+\n+export const clearGlobalSession = () => {\n+    _globalSession = null;\n+}\n+\n+export const withGlobalTransaction = async (func: () => Promise<void>, useConn?: Connection) => {\n+    // Wrap a user defined `func` in a global transaction\n+    const db = useConn || await connectDB();\n+    await db.transaction(async (session) => {\n+        setGlobalSession(session);\n+        try {\n+            return await func();\n+        } finally {\n+            clearGlobalSession();\n+        }\n+    });\n+}\n+\n+\n+const includeSession = (options?: TQueryOptions) => {\n+    let useOptions = options || {};\n+    if (!_globalSession) {\n+        if (useOptions.session) {\n+            throw new Error(`options.session is already set!: ${useOptions}`);\n+        }\n+        useOptions.session = _globalSession;\n+    }\n+    return useOptions;\n+}\n+\n+/* \n+Wrapped mongoose db calls. All mongo interaction should go through a function below\n+*/\n+\n+// CREATE\n+\n+export const save = (model, options?: TQueryOptions) => {\n+    return model.save(includeSession(options));\n+}\n+\n+// UPDATE\n+\n+export const findByIdAndUpdate = (model, id, updatedData, options?: TQueryOptions) => {\n+    return model.findByIdAndUpdate(id, updatedData, includeSession(options));\n+}\n+\n+// DELETE\n+\n+export const findByIdAndDelete = (model, id, options?: TQueryOptions) => {\n+    return model.findByIdAndDelete(id, includeSession(options));\n+}\n+\n+// QUERY\n+\n+export const findById = (model, id, projection?, options?: TQueryOptions) => {\n+    return model.findById(id, projection, includeSession(options));\n+}\n+\n+export const findOne = (model, filter, projection?, options?: TQueryOptions) => {\n+    return model.findOne(filter, projection, includeSession(options));\n+}\n+\n+export const find = (model, filter, projection?, options?: TQueryOptions) => {\n+    return model.find(filter, projection, includeSession(options));\n+}\n+\n+export const countDocuments = (model, options?: TQueryOptions) => {\n+    return model.countDocuments(includeSession(options));\n+}"
            },
            {
                "filename": "src/db/operations/atomicity.js",
                "additions": 0,
                "deletions": 3,
                "patch": "@@ -1,3 +0,0 @@\n-/*\n-For database transactions to ensure each event is processed exactly once\n-*/\n\\ No newline at end of file"
            },
            {
                "filename": "src/db/operations/create.js",
                "additions": 12,
                "deletions": 21,
                "patch": "@@ -1,3 +1,4 @@\n+import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n@@ -7,54 +8,44 @@ import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n+import { save } from \"./atomic.js\";\n \n export const createIssuer = (issuerData) => {\n-    const issuer = new Issuer(issuerData);\n-    return issuer.save();\n+    return save(new Issuer(issuerData));\n };\n \n export const createStakeholder = (stakeholderData) => {\n-    const stakeholder = new Stakeholder(stakeholderData);\n-    return stakeholder.save();\n+    return save(new Stakeholder(stakeholderData));\n };\n \n export const createStockClass = (stockClassData) => {\n-    const stockClass = new StockClass(stockClassData);\n-    return stockClass.save();\n+    return save(new StockClass(stockClassData));\n };\n \n export const createStockLegendTemplate = (stockLegendTemplateData) => {\n-    const stockLegendTemplate = new StockLegendTemplate(stockLegendTemplateData);\n-    return stockLegendTemplate.save();\n+    return save(new StockLegendTemplate(stockLegendTemplateData));\n };\n \n export const createStockPlan = (stockPlanData) => {\n-    const stockPlan = new StockPlan(stockPlanData);\n-    return stockPlan.save();\n+    return save(new StockPlan(stockPlanData));\n };\n \n export const createValuation = (valuationData) => {\n-    const valuation = new Valuation(valuationData);\n-    return valuation.save();\n+    return save(new Valuation(valuationData));\n };\n \n export const createVestingTerms = (vestingTermsData) => {\n-    const vestingTerms = new VestingTerms(vestingTermsData);\n-    return vestingTerms.save();\n+    return save(new VestingTerms(vestingTermsData));\n };\n \n export const createHistoricalTransaction = (transactionHistoryData) => {\n-    const historicalTransaction = new HistoricalTransaction(transactionHistoryData);\n-    return historicalTransaction.save();\n+    return save(new HistoricalTransaction(transactionHistoryData));\n };\n \n export const createStockIssuance = (stockIssuanceData) => {\n-    const stockIssuance = new StockIssuance(stockIssuanceData);\n-    return stockIssuance.save();\n+    return save(new StockIssuance(stockIssuanceData));\n };\n \n export const createStockTransfer = (stockTransferData) => {\n-    const stockTransfer = new StockTransfer(stockTransferData);\n-    return stockTransfer.save();\n+    return save(new StockTransfer(stockTransferData));\n };"
            },
            {
                "filename": "src/db/operations/delete.js",
                "additions": 8,
                "deletions": 8,
                "patch": "@@ -10,33 +10,33 @@ import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n // TODO: since we're doing a time series db that's meant to be immutable, is this needed?\n \n export const deleteIssuerById = (issuerId) => {\n-    return Issuer.findByIdAndDelete(issuerId);\n+    return findByIdAndDelete(Issuer, issuerId);\n };\n \n export const deleteStakeholderById = (stakeholderId) => {\n-    return Stakeholder.findByIdAndDelete(stakeholderId);\n+    return findByIdAndDelete(Stakeholder, stakeholderId);\n };\n \n export const deleteStockClassById = (stockClassId) => {\n-    return StockClass.findByIdAndDelete(stockClassId);\n+    return findByIdAndDelete(StockClass, stockClassId);\n };\n \n export const deleteStockLegendTemplateById = (stockLegendTemplateId) => {\n-    return StockLegendTemplate.findByIdAndDelete(stockLegendTemplateId);\n+    return findByIdAndDelete(StockLegendTemplate, stockLegendTemplateId);\n };\n \n export const deleteStockPlanById = (stockPlanId) => {\n-    return StockPlan.findByIdAndDelete(stockPlanId);\n+    return findByIdAndDelete(StockPlan, stockPlanId);\n };\n \n export const deleteValuationById = (valuationId) => {\n-    return Valuation.findByIdAndDelete(valuationId);\n+    return findByIdAndDelete(Valuation, valuationId);\n };\n \n export const deleteVestingTermsById = (vestingTermsId) => {\n-    return VestingTerms.findByIdAndDelete(vestingTermsId);\n+    return findByIdAndDelete(VestingTerms, vestingTermsId);\n };\n \n export const deleteTransactionById = (transactionId) => {\n-    return StockIssuance.findByIdAndDelete(transactionId);\n+    return findByIdAndDelete(StockIssuance, transactionId);\n };"
            },
            {
                "filename": "src/db/operations/read.js",
                "additions": 25,
                "deletions": 40,
                "patch": "@@ -1,97 +1,84 @@\n+import Factory from \"../objects/Factory.js\";\n+import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n-import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import Factory from \"../objects/Factory.js\";\n+import { countDocuments, find, findById } from \"./atomic.js\";\n \n // READ By ID\n export const readIssuerById = async (id) => {\n-    const issuer = await Issuer.findById(id);\n-    return issuer;\n+    return await findById(Issuer, id);\n };\n \n export const readStakeholderById = async (id) => {\n-    const stakeholder = await Stakeholder.findById(id);\n-    return stakeholder;\n+    return await findById(Stakeholder, id);\n };\n \n export const readStockClassById = async (id) => {\n-    const stockClass = await StockClass.findById(id);\n-    return stockClass;\n+    return await findById(StockClass, id);\n };\n \n export const readStockLegendTemplateById = async (id) => {\n-    const stockLegendTemplate = await StockLegendTemplate.findById(id);\n-    return stockLegendTemplate;\n+    return await findById(StockLegendTemplate, id);\n };\n \n export const readStockPlanById = async (id) => {\n-    const stockPlan = await StockPlan.findById(id);\n-    return stockPlan;\n+    return await findById(StockPlan, id);\n };\n \n export const readValuationById = async (id) => {\n-    const valuation = await Valuation.findById(id);\n-    return valuation;\n+    return await findById(Valuation, id);\n };\n \n export const readVestingTermsById = async (id) => {\n-    const vestingTerms = await VestingTerms.findById(id);\n-    return vestingTerms;\n+    return await findById(VestingTerms, id);\n };\n \n+// READ Multiple\n export const readHistoricalTransactionByIssuerId = async (issuerId) => {\n-    const historicalTransactions = await HistoricalTransaction.find({ issuer: issuerId }).populate(\"transaction\");\n-    return historicalTransactions;\n+    return await find(HistoricalTransaction, { issuer: issuerId }).populate(\"transaction\");\n };\n \n // COUNT\n export const countIssuers = async () => {\n-    const totalIssuers = await Issuer.countDocuments();\n-    return totalIssuers;\n+    return await countDocuments(Issuer);\n };\n \n export const countStakeholders = async () => {\n-    const totalStakeholders = await Stakeholder.countDocuments();\n-    return totalStakeholders;\n+    return await countDocuments(Stakeholder);\n };\n \n export const countStockClasses = async () => {\n-    const totalStockClasses = await StockClass.countDocuments();\n-    return totalStockClasses;\n+    return await countDocuments(StockClass);\n };\n \n export const countStockLegendTemplates = async () => {\n-    const totalTemplates = await StockLegendTemplate.countDocuments();\n-    return totalTemplates;\n+    return await countDocuments(StockLegendTemplate);\n };\n \n export const countStockPlans = async () => {\n-    const totalStockPlans = await StockPlan.countDocuments();\n-    return totalStockPlans;\n+    return await countDocuments(StockPlan);\n };\n \n export const countValuations = async () => {\n-    const totalValuations = await Valuation.countDocuments();\n-    return totalValuations;\n+    return await countDocuments(Valuation);\n };\n \n export const countVestingTerms = async () => {\n-    const totalVestingTerms = await VestingTerms.countDocuments();\n-    return totalVestingTerms;\n+    return await countDocuments(VestingTerms);\n };\n \n export const getAllIssuerDataById = async (issuerId) => {\n-    const issuerStakeholders = await Stakeholder.find({ issuer: issuerId });\n-    const issuerStockClasses = await StockClass.find({ issuer: issuerId });\n-    const issuerStockIssuances = await StockIssuance.find({ issuer: issuerId });\n-    const issuerStockTransfers = await StockTransfer.find({ issuer: issuerId });\n+    const issuerStakeholders = await find(Stakeholder, { issuer: issuerId });\n+    const issuerStockClasses = await find(StockClass, { issuer: issuerId });\n+    const issuerStockIssuances = await find(StockIssuance, { issuer: issuerId });\n+    const issuerStockTransfers = await find(StockTransfer, { issuer: issuerId });\n \n     return {\n         stakeholders: issuerStakeholders,\n@@ -102,11 +89,9 @@ export const getAllIssuerDataById = async (issuerId) => {\n };\n \n export const readAllIssuers = async () => {\n-    const issuers = await Issuer.find();\n-    return issuers;\n+    return await find(Issuer);\n }\n \n export const readFactory = async () => {\n-    const factory = await Factory.find();\n-    return factory;\n+    return await find(Factory);\n }\n\\ No newline at end of file"
            },
            {
                "filename": "src/db/operations/update.js",
                "additions": 24,
                "deletions": 38,
                "patch": "@@ -1,95 +1,81 @@\n import Issuer from \"../objects/Issuer.js\";\n import Stakeholder from \"../objects/Stakeholder.js\";\n-import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n-import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockClass from \"../objects/StockClass.js\";\n import StockLegendTemplate from \"../objects/StockLegendTemplate.js\";\n import StockPlan from \"../objects/StockPlan.js\";\n import Valuation from \"../objects/Valuation.js\";\n import VestingTerms from \"../objects/VestingTerms.js\";\n+import StockAcceptance from \"../objects/transactions/acceptance/StockAcceptance.js\";\n+import IssuerAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.js\";\n+import StockClassAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.js\";\n import StockCancellation from \"../objects/transactions/cancellation/StockCancellation.js\";\n-import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n+import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockReissuance from \"../objects/transactions/reissuance/StockReissuance.js\";\n import StockRepurchase from \"../objects/transactions/repurchase/StockRepurchase.js\";\n-import StockAcceptance from \"../objects/transactions/acceptance/StockAcceptance.js\";\n-import StockClassAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/StockClassAuthorizedSharesAdjustment.js\";\n-import IssuerAuthorizedSharesAdjustment from \"../objects/transactions/adjustment/IssuerAuthorizedSharesAdjustment.js\";\n+import StockRetraction from \"../objects/transactions/retraction/StockRetraction.js\";\n+import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n \n \n export const updateIssuerById = async (id, updatedData) => {\n-    const issuer = await Issuer.findByIdAndUpdate(id, updatedData, { new: true });\n-    return issuer;\n+    return await findByIdAndUpdate(Issuer, id, updatedData, { new: true });\n };\n \n export const updateStakeholderById = async (id, updatedData) => {\n-    const stakeholder = await Stakeholder.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stakeholder;\n+    return await findByIdAndUpdate(Stakeholder, id, updatedData, { new: true });\n };\n \n export const updateStockClassById = async (id, updatedData) => {\n-    const stockClass = await StockClass.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockClass;\n+    return await findByIdAndUpdate(StockClass, id, updatedData, { new: true });\n };\n \n export const updateStockLegendTemplateById = async (id, updatedData) => {\n-    const stockLegendTemplate = await StockLegendTemplate.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockLegendTemplate;\n+    return await findByIdAndUpdate(StockLegendTemplate, id, updatedData, { new: true });\n };\n \n export const updateStockPlanById = async (id, updatedData) => {\n-    const stockPlan = await StockPlan.findByIdAndUpdate(id, updatedData, { new: true });\n-    return stockPlan;\n+    return await findByIdAndUpdate(StockPlan, id, updatedData, { new: true });\n };\n \n export const updateValuationById = async (id, updatedData) => {\n-    const valuation = await Valuation.findByIdAndUpdate(id, updatedData, { new: true });\n-    return valuation;\n+    return await findByIdAndUpdate(Valuation, id, updatedData, { new: true });\n };\n \n export const updateVestingTermsById = async (id, updatedData) => {\n-    const vestingTerms = await VestingTerms.findByIdAndUpdate(id, updatedData, { new: true });\n-    return vestingTerms;\n+    return await findByIdAndUpdate(VestingTerms, id, updatedData, { new: true });\n };\n \n export const upsertStockIssuanceById = async (id, updatedData) => {\n-    const stockIssuance = await StockIssuance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockIssuance;\n+    return await findByIdAndUpdate(StockIssuance, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockTransferById = async (id, updatedData) => {\n-    const stockTransfer = await StockTransfer.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockTransfer;\n+    return await findByIdAndUpdate(StockTransfer, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockCancellationById = async (id, updatedData) => {\n-    const stockCancellation = await StockCancellation.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockCancellation;\n+    return await findByIdAndUpdate(StockCancellation, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockRetractionById = async (id, updatedData) => {\n-    const stockRetraction = await StockRetraction.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockRetraction;\n+    return await findByIdAndUpdate(StockRetraction, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockReissuanceById = async (id, updatedData) => {\n-    const stockReissuance = await StockReissuance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockReissuance;\n+    return await findByIdAndUpdate(StockReissuance, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockRepurchaseById = async (id, updatedData) => {\n-    const stockRepurchase = await StockRepurchase.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockRepurchase;\n+    return await findByIdAndUpdate(StockRepurchase, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockAcceptanceById = async (id, updatedData) => {\n-    const stockAcceptance = await StockAcceptance.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n-    return stockAcceptance;\n+    return await findByIdAndUpdate(StockAcceptance, id, updatedData, { new: true, upsert: true, returning: true });\n };\n \n export const upsertStockClassAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await StockClassAuthorizedSharesAdjustment.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n+    return await findByIdAndUpdate(StockClassAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true, returning: true });\n };\n-export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n-    return await IssuerAuthorizedSharesAdjustment.findByIdAndUpdate(id, updatedData, { new: true, upsert: true, returning: true });\n \n-}\n+export const upsertIssuerAuthorizedSharesAdjustment = async (id, updatedData) => {\n+    return await findByIdAndUpdate(IssuerAuthorizedSharesAdjustment, id, updatedData, { new: true, upsert: true, returning: true });\n+};"
            },
            {
                "filename": "src/routes/index.js",
                "additions": 2,
                "deletions": 10,
                "patch": "@@ -1,11 +1,9 @@\n import { Router } from \"express\";\n import deployCapTable from \"../chain-operations/deployCapTable.js\";\n+import { updateIssuerById } from \"../db/operations/update.js\";\n import seedDB from \"../db/scripts/seed.js\";\n-import { contractCache } from \"../utils/caches.js\";\n import { convertUUIDToBytes16 } from \"../utils/convertUUID.js\";\n import processManifest from \"../utils/processManifest.js\";\n-import { updateIssuerById } from \"../db/operations/update.js\";\n-import startOnchainListeners from \"../chain-operations/transactionListener.js\";\n \n const router = Router();\n \n@@ -17,14 +15,8 @@ router.post(\"/mint-cap-table\", async (req, res) => {\n         const issuer = await seedDB(manifest);\n \n         const issuerIdBytes16 = convertUUIDToBytes16(issuer._id);\n-        const { contract, address, provider, libraries } = await deployCapTable(req.chain, issuerIdBytes16, issuer.legal_name, issuer.initial_shares_authorized);\n-\n+        const { address } = await deployCapTable(req.chain, issuerIdBytes16, issuer.legal_name, issuer.initial_shares_authorized);\n         const savedIssuerWithDeployedTo = await updateIssuerById(issuer._id, { deployed_to: address });\n-\n-        // add contract to the cache and start listener\n-        contractCache[issuer._id] = { contract, provider, libraries };\n-        await startOnchainListeners(contract, provider, issuer._id, libraries);\n-\n         res.status(200).send({ issuer: savedIssuerWithDeployedTo });\n     } catch (error) {\n         console.error(`error: ${error}`);"
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 2,
                "deletions": 1,
                "patch": "@@ -58,7 +58,7 @@ issuer.post(\"/create\", async (req, res) => {\n \n         const issuerIdBytes16 = convertUUIDToBytes16(incomingIssuerToValidate.id);\n         console.log(\"\ud83d\udcbe | Issuer id in bytes16 \", issuerIdBytes16);\n-        const { contract, provider, address, libraries } = await deployCapTable(\n+        const { contract, provider, address, libraries, deployHash } = await deployCapTable(\n             chain,\n             issuerIdBytes16,\n             incomingIssuerToValidate.legal_name,\n@@ -72,6 +72,7 @@ issuer.post(\"/create\", async (req, res) => {\n         const incomingIssuerForDB = {\n             ...incomingIssuerToValidate,\n             deployed_to: address,\n+            tx_hash: deployHash,\n         };\n \n         const issuer = await createIssuer(incomingIssuerForDB);"
            },
            {
                "filename": "src/server.js",
                "additions": 8,
                "deletions": 28,
                "patch": "@@ -4,8 +4,7 @@ config();\n \n import connectDB from \"./db/config/mongoose.js\";\n \n-import getContractInstance from \"./chain-operations/getContractInstances.js\";\n-import startOnchainListeners from \"./chain-operations/transactionListener.js\";\n+import startSynchronousEventProcessing from \"./chain-operations/transactionPoller.js\";\n \n // Routes\n import historicalTransactions from \"./routes/historicalTransactions.js\";\n@@ -19,8 +18,8 @@ import transactionRoutes from \"./routes/transactions.js\";\n import valuationRoutes from \"./routes/valuation.js\";\n import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n \n-import { readIssuerById, readAllIssuers } from \"./db/operations/read.js\";\n-import { contractCache } from \"./utils/caches.js\";\n+import { readIssuerById } from \"./db/operations/read.js\";\n+import { getIssuerContract } from \"./utils/caches.js\";\n \n const app = express();\n \n@@ -48,17 +47,9 @@ const contractMiddleware = async (req, res, next) => {\n     const issuer = await readIssuerById(req.body.issuerId);\n     if (!issuer) res.status(400).send(\"issuer not found \");\n \n-    // Check if contract instance already exists in cache\n-    if (!contractCache[req.body.issuerId]) {\n-        const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to);\n-        contractCache[req.body.issuerId] = { contract, provider, libraries };\n-\n-        // Initialize listener for this contract\n-        startOnchainListeners(contract, provider, req.body.issuerId, libraries);\n-    }\n-\n-    req.contract = contractCache[req.body.issuerId].contract;\n-    req.provider = contractCache[req.body.issuerId].provider;\n+    const {contract, provider} = await getIssuerContract(issuer);\n+    req.contract = contract;\n+    req.provider = provider;\n     next();\n };\n app.use(urlencoded({ limit: \"50mb\", extended: true }));\n@@ -81,17 +72,6 @@ app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n \n app.listen(PORT, async () => {\n     console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n-     // Fetch all issuers\n-     const issuers = await readAllIssuers();\n-     if (issuers && issuers.length > 0) {\n-         for (const issuer of issuers) {\n-             if (issuer.deployed_to) {\n-                 // Create a new contract instance for each issuer\n-                 const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to);\n- \n-                 // Initialize listener for this contract\n-                 startOnchainListeners(contract, provider, issuer._id, libraries);\n-             }\n-         }\n-     }\n+    // Kick off asynchronous job to process changes\n+    startSynchronousEventProcessing();\n });"
            },
            {
                "filename": "src/utils/caches.js",
                "additions": 0,
                "deletions": 10,
                "patch": "@@ -1,10 +0,0 @@\n-// Centralized contract manager/cache\n-export const contractCache = {};\n-\n-/*\n-issuerId = {\n-        activePositions: {...},\n-        activeSecurityIdsByStockClass: {...},\n-    };\n-*/\n-export const preProcessorCache = {};"
            },
            {
                "filename": "src/utils/caches.ts",
                "additions": 32,
                "deletions": 0,
                "patch": "@@ -0,0 +1,32 @@\n+import getContractInstance from \"../chain-operations/getContractInstances\";\n+\n+const CHAIN = process.env.CHAIN;\n+\n+interface CachePayload {\n+    contract: any;\n+    provider: any;\n+    libraries: any;\n+}\n+\n+// Centralized contract manager/cache\n+const contractCache: {[key: string]: CachePayload} = {};\n+\n+const cacheIssuerContract = async (issuer, payload: CachePayload) => {\n+    contractCache[issuer._id] = payload;\n+}\n+\n+export const getIssuerContract = async (issuer) => {\n+    if (!contractCache[issuer._id]) {\n+        const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to); \n+        await cacheIssuerContract(issuer, { contract, provider, libraries });\n+    }\n+    return contractCache[issuer._id];\n+}\n+\n+/*\n+issuerId = {\n+        activePositions: {...},\n+        activeSecurityIdsByStockClass: {...},\n+    };\n+*/\n+export const preProcessorCache = {};"
            }
        ]
    },
    {
        "sha": "c361a5d4d410b3491e7fec23c2c8565c3d13fc65",
        "author": "victormimo",
        "date": "2024-01-09 22:57:06+00:00",
        "message": "fixing custom network",
        "files": [
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 14,
                "deletions": 4,
                "patch": "@@ -13,10 +13,20 @@ async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     const RPC_URL = process.env.RPC_URL;\n     const CHAIN_ID = process.env.CHAIN_ID;\n \n-    const customNetwork = {\n-        // Change the CHAIN_ID in the .env file to deploy to a different network\n-        chainId: parseInt(CHAIN_ID),\n-    };\n+    let customNetwork;\n+\n+    // Change the CHAIN_ID in the .env file to deploy to a different network\n+    if (RPC_URL === \"http://127.0.0.1:8545\") {\n+        customNetwork =  {\n+            chainId: parseInt(CHAIN_ID),\n+            name: \"local\"\n+        };\n+    } else {\n+        customNetwork =  {\n+            // Change the CHAIN_ID in the .env file to deploy to a different network\n+            chainId: parseInt(CHAIN_ID),\n+        };\n+    }\n \n     const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 14,
                "deletions": 4,
                "patch": "@@ -10,10 +10,20 @@ async function getContractInstance(address) {\n     const RPC_URL = process.env.RPC_URL;\n     const CHAIN_ID = parseInt(process.env.CHAIN_ID); // Convert to integer\n \n-    const customNetwork = {\n-        chainId: CHAIN_ID, // Use the CHAIN_ID from .env\n-        name: \"network\",\n-    };\n+    let customNetwork;\n+\n+    // Change the CHAIN_ID in the .env file to deploy to a different network\n+    if (RPC_URL === \"http://127.0.0.1:8545\") {\n+        customNetwork =  {\n+            chainId: parseInt(CHAIN_ID),\n+            name: \"local\"\n+        };\n+    } else {\n+        customNetwork =  {\n+            // Change the CHAIN_ID in the .env file to deploy to a different network\n+            chainId: parseInt(CHAIN_ID),\n+        };\n+    }\n \n     const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);"
            }
        ]
    },
    {
        "sha": "afdd51972f4c599055a61f4e3e15d71c5ab4e18a",
        "author": "victormimo",
        "date": "2024-01-09 22:22:55+00:00",
        "message": "Merge pull request #115 from transfer-agent-protocol/vmimo/cleanup-jan9\n\nfix (removing CHAIN) removes CHAIN env variable and associated functions",
        "files": [
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -5,14 +5,14 @@ import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n \n config();\n \n-async function getContractInstance(chain, address) {\n+async function getContractInstance(address) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n     const RPC_URL = process.env.RPC_URL;\n     const CHAIN_ID = parseInt(process.env.CHAIN_ID); // Convert to integer\n \n     const customNetwork = {\n         chainId: CHAIN_ID, // Use the CHAIN_ID from .env\n-        name: chain, // Use the chain parameter as the network name\n+        name: \"network\",\n     };\n \n     const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);"
            },
            {
                "filename": "src/routes/index.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -17,7 +17,7 @@ router.post(\"/mint-cap-table\", async (req, res) => {\n         const issuer = await seedDB(manifest);\n \n         const issuerIdBytes16 = convertUUIDToBytes16(issuer._id);\n-        const { contract, address, provider, libraries } = await deployCapTable(req.chain, issuerIdBytes16, issuer.legal_name, issuer.initial_shares_authorized);\n+        const { contract, address, provider, libraries } = await deployCapTable(issuerIdBytes16, issuer.legal_name, issuer.initial_shares_authorized);\n \n         const savedIssuerWithDeployedTo = await updateIssuerById(issuer._id, { deployed_to: address });\n "
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 0,
                "deletions": 3,
                "patch": "@@ -42,8 +42,6 @@ issuer.get(\"/total-number\", async (req, res) => {\n });\n \n issuer.post(\"/create\", async (req, res) => {\n-    const { chain } = req;\n-\n     try {\n         // OCF doesn't allow extra fields in their validation\n         const incomingIssuerToValidate = {\n@@ -59,7 +57,6 @@ issuer.post(\"/create\", async (req, res) => {\n         const issuerIdBytes16 = convertUUIDToBytes16(incomingIssuerToValidate.id);\n         console.log(\"\ud83d\udcbe | Issuer id in bytes16 \", issuerIdBytes16);\n         const { contract, provider, address, libraries } = await deployCapTable(\n-            chain,\n             issuerIdBytes16,\n             incomingIssuerToValidate.legal_name,\n             incomingIssuerToValidate.initial_shares_authorized"
            },
            {
                "filename": "src/server.js",
                "additions": 4,
                "deletions": 11,
                "patch": "@@ -28,13 +28,6 @@ const app = express();\n connectDB();\n \n const PORT = process.env.PORT;\n-const CHAIN = process.env.CHAIN;\n-\n-// Middlewares\n-const chainMiddleware = (req, res, next) => {\n-    req.chain = CHAIN;\n-    next();\n-};\n \n // Middleware to get or create contract instance\n // the listener is first started on deployment, then here as a backup\n@@ -50,7 +43,7 @@ const contractMiddleware = async (req, res, next) => {\n \n     // Check if contract instance already exists in cache\n     if (!contractCache[req.body.issuerId]) {\n-        const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to);\n+        const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to);\n         contractCache[req.body.issuerId] = { contract, provider, libraries };\n \n         // Initialize listener for this contract\n@@ -65,8 +58,8 @@ app.use(urlencoded({ limit: \"50mb\", extended: true }));\n app.use(json({ limit: \"50mb\" }));\n app.enable(\"trust proxy\");\n \n-app.use(\"/\", chainMiddleware, mainRoutes);\n-app.use(\"/issuer\", chainMiddleware, issuerRoutes);\n+app.use(\"/\", mainRoutes);\n+app.use(\"/issuer\", issuerRoutes);\n app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n // No middleware required since these are only created offchain\n@@ -87,7 +80,7 @@ app.listen(PORT, async () => {\n          for (const issuer of issuers) {\n              if (issuer.deployed_to) {\n                  // Create a new contract instance for each issuer\n-                 const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to);\n+                 const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to);\n  \n                  // Initialize listener for this contract\n                  startOnchainListeners(contract, provider, issuer._id, libraries);"
            }
        ]
    },
    {
        "sha": "b94a48965645f35ac6faefe4804cac7684f8f676",
        "author": "victormimo",
        "date": "2024-01-09 22:18:37+00:00",
        "message": "stripping chain parameter",
        "files": [
            {
                "filename": "src/routes/index.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -17,7 +17,7 @@ router.post(\"/mint-cap-table\", async (req, res) => {\n         const issuer = await seedDB(manifest);\n \n         const issuerIdBytes16 = convertUUIDToBytes16(issuer._id);\n-        const { contract, address, provider, libraries } = await deployCapTable(req.chain, issuerIdBytes16, issuer.legal_name, issuer.initial_shares_authorized);\n+        const { contract, address, provider, libraries } = await deployCapTable(issuerIdBytes16, issuer.legal_name, issuer.initial_shares_authorized);\n \n         const savedIssuerWithDeployedTo = await updateIssuerById(issuer._id, { deployed_to: address });\n "
            },
            {
                "filename": "src/routes/issuer.js",
                "additions": 0,
                "deletions": 3,
                "patch": "@@ -42,8 +42,6 @@ issuer.get(\"/total-number\", async (req, res) => {\n });\n \n issuer.post(\"/create\", async (req, res) => {\n-    const { chain } = req;\n-\n     try {\n         // OCF doesn't allow extra fields in their validation\n         const incomingIssuerToValidate = {\n@@ -59,7 +57,6 @@ issuer.post(\"/create\", async (req, res) => {\n         const issuerIdBytes16 = convertUUIDToBytes16(incomingIssuerToValidate.id);\n         console.log(\"\ud83d\udcbe | Issuer id in bytes16 \", issuerIdBytes16);\n         const { contract, provider, address, libraries } = await deployCapTable(\n-            chain,\n             issuerIdBytes16,\n             incomingIssuerToValidate.legal_name,\n             incomingIssuerToValidate.initial_shares_authorized"
            }
        ]
    },
    {
        "sha": "2cf759b3f7920577e04a00eeecf5151ff7c8f6b7",
        "author": "victormimo",
        "date": "2024-01-09 22:18:24+00:00",
        "message": "removing chain middleware (CHAIN .env)",
        "files": [
            {
                "filename": "src/server.js",
                "additions": 4,
                "deletions": 11,
                "patch": "@@ -28,13 +28,6 @@ const app = express();\n connectDB();\n \n const PORT = process.env.PORT;\n-const CHAIN = process.env.CHAIN;\n-\n-// Middlewares\n-const chainMiddleware = (req, res, next) => {\n-    req.chain = CHAIN;\n-    next();\n-};\n \n // Middleware to get or create contract instance\n // the listener is first started on deployment, then here as a backup\n@@ -50,7 +43,7 @@ const contractMiddleware = async (req, res, next) => {\n \n     // Check if contract instance already exists in cache\n     if (!contractCache[req.body.issuerId]) {\n-        const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to);\n+        const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to);\n         contractCache[req.body.issuerId] = { contract, provider, libraries };\n \n         // Initialize listener for this contract\n@@ -65,8 +58,8 @@ app.use(urlencoded({ limit: \"50mb\", extended: true }));\n app.use(json({ limit: \"50mb\" }));\n app.enable(\"trust proxy\");\n \n-app.use(\"/\", chainMiddleware, mainRoutes);\n-app.use(\"/issuer\", chainMiddleware, issuerRoutes);\n+app.use(\"/\", mainRoutes);\n+app.use(\"/issuer\", issuerRoutes);\n app.use(\"/stakeholder\", contractMiddleware, stakeholderRoutes);\n app.use(\"/stock-class\", contractMiddleware, stockClassRoutes);\n // No middleware required since these are only created offchain\n@@ -87,7 +80,7 @@ app.listen(PORT, async () => {\n          for (const issuer of issuers) {\n              if (issuer.deployed_to) {\n                  // Create a new contract instance for each issuer\n-                 const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to);\n+                 const { contract, provider, libraries } = await getContractInstance(issuer.deployed_to);\n  \n                  // Initialize listener for this contract\n                  startOnchainListeners(contract, provider, issuer._id, libraries);"
            }
        ]
    },
    {
        "sha": "d1943f7500d95e190abc43b467def1569f36797a",
        "author": "victormimo",
        "date": "2024-01-09 22:18:08+00:00",
        "message": "trying general custom_network name",
        "files": [
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -5,14 +5,14 @@ import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n \n config();\n \n-async function getContractInstance(chain, address) {\n+async function getContractInstance(address) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n     const RPC_URL = process.env.RPC_URL;\n     const CHAIN_ID = parseInt(process.env.CHAIN_ID); // Convert to integer\n \n     const customNetwork = {\n         chainId: CHAIN_ID, // Use the CHAIN_ID from .env\n-        name: chain, // Use the chain parameter as the network name\n+        name: \"network\",\n     };\n \n     const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);"
            }
        ]
    },
    {
        "sha": "1a34b3fdec1b622d349d1cb8754c25efbad7bbf1",
        "author": "ThatAlexPalmer",
        "date": "2024-01-05 19:48:23+00:00",
        "message": "Correct license convention",
        "files": [
            {
                "filename": "LICENSE",
                "additions": 3,
                "deletions": 1,
                "patch": "@@ -1,4 +1,6 @@\n-Copyright 2023 Alex Palmer, Victor Mimo.\n+MIT License\n+\n+Copyright (c) [2023] [Victor Mimo, Alex Palmer]\n \n Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n "
            }
        ]
    },
    {
        "sha": "d43b21042f9a9662883c072a37049c4b35c685f3",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 18:39:56+00:00",
        "message": "Provide correct server port",
        "files": [
            {
                "filename": ".env.example",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -16,4 +16,4 @@ ETHERSCAN_L2_API_KEY=UPDATE_ME\n ETHERSCAN_L1_API_KEY=UPDATE_ME\n \n # Server port\n-PORT=3000\n\\ No newline at end of file\n+PORT=8080\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "a297775e13221a7e35368c23f76efbcc3393b402",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 17:42:20+00:00",
        "message": "Merge pull request #108 from transfer-agent-protocol/apalmer/deployment-scripts\n\nimprovement(scripts) - improves deployment scripts",
        "files": [
            {
                "filename": ".env.example",
                "additions": 18,
                "deletions": 8,
                "patch": "@@ -1,9 +1,19 @@\n+# Offchain db connection string for mongodb\n DATABASE_URL=\"mongodb://tap:tap@localhost:27017/mongo?authSource=admin&retryWrites=true&w=majority\"\n-OPTIMISM_GOERLI_RPC_URL=UPDATE_ME\n-LOCAL_RPC_URL=http://127.0.0.1:8545\n-PRIVATE_KEY_FAKE_ACCOUNT=UPDATE_ME\n-PRIVATE_KEY_POET_TEST=UPDATE_ME\n-ETHERSCAN_OPTIMISM_API_KEY=UPDATE_ME\n-ETHERSCAN_ETHEREUM_API_KEY=UPDATE_ME\n-CHAIN=local\n-# CHAIN=\"optimism-goerli\"\n+\n+# RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n+RPC_URL=http://127.0.0.1:8545\n+\n+# Change this to the chain id of the network you are deploying to\n+# Use 31337 for Anvil's, 32586980208 for Arbitrum's\n+CHAIN_ID=31337\n+\n+# Update with the private key of the account that will be used to deploy the contracts\n+PRIVATE_KEY=UPDATE_ME\n+\n+# Etherscan API keys\n+ETHERSCAN_L2_API_KEY=UPDATE_ME\n+ETHERSCAN_L1_API_KEY=UPDATE_ME\n+\n+# Server port\n+PORT=3000\n\\ No newline at end of file"
            },
            {
                "filename": "LICENSE",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,4 +1,4 @@\n-Copyright 2023 Poet Network Inc.\n+Copyright 2023 Alex Palmer, Victor Mimo.\n \n Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n "
            },
            {
                "filename": "README.md",
                "additions": 13,
                "deletions": 9,
                "patch": "@@ -2,15 +2,15 @@\n \n Developed by:\n \n-- [Poet](https://poet.network/)\n+- [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n - [Plural Energy](https://www.pluralenergy.co/)\n - [Fairmint](https://www.fairmint.com/)\n \n This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety. In development, it's meant to be run in a Docker container with a local MongoDB instance. While in active development, it's meant to be run with [Anvil](https://book.getfoundry.sh/anvil/) and [Forge](https://book.getfoundry.sh/forge/).\n \n <div align=\"center\">\n-  <a href=\"https://github.com/poet-network/tap-cap-table/blob/main/LICENSE\">\n-    <img alt=\"License\" src=\"https://img.shields.io/github/license/poet-network/tap-cap-table\">\n+  <a href=\"https://github.com/transfer-agent-protocol/tap-cap-table/blob/main/LICENSE\">\n+    <img alt=\"License\" src=\"https://img.shields.io/github/license/transfer-agent-protocol/tap-cap-table\">\n   </a>\n </div>\n \n@@ -36,7 +36,7 @@ We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo)\n \n ## Official links\n \n-- [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n+- [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n - [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n \n ## Getting started\n@@ -87,17 +87,21 @@ Install dependencies and setup [Foundry](https://book.getfoundry.sh/) and `forge\n yarn install && yarn setup\n ```\n \n-## Deploying external libraries\n+## Deploying the cap table smart contracts\n \n-In our architecture, each transaction is mapped to an external library, which ensures bytecode limits are never met.\n-\n-To deploy these libraries:\n+In our architecture, each transaction is mapped to an external library, which ensures bytecode limits are never met. To deploy these libraries:\n \n 1. Ensure you have Anvil running in the `/chain` directory\n-2. Then, inside of the root directory run `yarn build`\n+2. Then, inside of the root directory run\n+\n+```sh\n+yarn build\n+```\n \n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n+To deploy the cap table smart contracts to a testnet, update `RPC_URL` and `CHAIN_ID` in the `.env` file, then run the same `yarn build` command.\n+\n \n ## Running the cap table server\n "
            },
            {
                "filename": "chain/.env.example",
                "additions": 13,
                "deletions": 6,
                "patch": "@@ -1,6 +1,13 @@\n-OPTIMISM_GOERLI_RPC_URL=UPDATE_ME\n-LOCAL_RPC_URL=http://127.0.0.1:8545\n-PRIVATE_KEY_FAKE_ACCOUNT=UPDATE_ME\n-PRIVATE_KEY_POET_TEST=UPDATE_ME\n-ETHERSCAN_OPTIMISM_API_KEY=UPDATE_ME\n-ETHERSCAN_ETHEREUM_API_KEY=UPDATE_ME\n+# RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n+RPC_URL=http://127.0.0.1:8545\n+\n+# Change this to the chain id of the network you are deploying to\n+# Use 31337 for Anvil's, 32586980208 for Arbitrum's\n+CHAIN_ID=31337\n+\n+# Update with the private key of the account that will be used to deploy the contracts\n+PRIVATE_KEY=UPDATE_ME\n+\n+# Etherscan API keys\n+ETHERSCAN_L2_API_KEY=UPDATE_ME\n+ETHERSCAN_L1_API_KEY=UPDATE_ME"
            },
            {
                "filename": "chain/foundry.toml",
                "additions": 2,
                "deletions": 3,
                "patch": "@@ -8,11 +8,10 @@ cbor_metadata = false\n \n \n [rpc_endpoints]\n-optimism_goerli = \"${OPTIMISM_GOERLI_RPC_URL}\"\n-local = \"${LOCAL_RPC_URL}\"\n+rpc_url = \"${RPC_URL}\"\n \n [etherscan]\n-optimism_goerli_etherscan = { key = \"${ETHERSCAN_OPTIMISM_API_KEY}\", chain = \"goerli\" }\n+optimism_goerli_etherscan = { key = \"${ETHERSCAN_L2_API_KEY}\", chain = \"goerli\" }\n \n \n "
            },
            {
                "filename": "chain/script/CapTableFactory.s.sol",
                "additions": 6,
                "deletions": 6,
                "patch": "@@ -9,26 +9,26 @@ import \"../src/CapTableFactory.sol\";\n \n /// @dev Test deployment using `forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast`\n contract DeployCapTableFactoryDeployLocalScript is Script {\n-    uint256 deployerPrivateKeyFakeAccount;\n+    uint256 deployerPrivateKey;\n \n     function setUp() public {\n         console.log(\"Upgrading CapTableFactory with CapTable implementation\");\n \n-        deployerPrivateKeyFakeAccount = vm.envUint(\"PRIVATE_KEY_FAKE_ACCOUNT\");\n+        deployerPrivateKey = vm.envUint(\"PRIVATE_KEY\");\n     }\n \n     function run() external {\n         console.log(\"Deploying CapTableFactory and CapTable implementation\");\n \n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTable capTable = new CapTable();\n         console.log(\"CapTable implementation deployed at:\", address(capTable));\n \n         vm.stopBroadcast(); // End the transaction\n \n         // Deploy CapTableFactory with the address of CapTable implementation\n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTableFactory capTableFactory = new CapTableFactory(address(capTable));\n         console.log(\"CapTableFactory deployed at:\", address(capTableFactory));\n@@ -38,15 +38,15 @@ contract DeployCapTableFactoryDeployLocalScript is Script {\n \n     /// @dev Run using `forge tx script/CapTableFactory.s.sol upgradeCapTable [0x...] --fork-url http://localhost:8545 --broadcast`\n     function upgradeCapTable(address factory) external {\n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTable capTable = new CapTable();\n         console.log(\"CapTable implementation deployed at:\", address(capTable));\n \n         vm.stopBroadcast(); // End the transaction\n \n         // Upgrade CapTableFactory with the address of CapTable implementation\n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTableFactory capTableFactory = CapTableFactory(factory);\n "
            },
            {
                "filename": "package.json",
                "additions": 2,
                "deletions": 12,
                "patch": "@@ -2,9 +2,9 @@\n     \"name\": \"tap-cap-table\",\n     \"version\": \"1.0.0-alpha.0\",\n     \"private\": true,\n-    \"author\": \"Alex Palmer <alex@poet.network>, Victor Mimo <victor@poet.network>\",\n+    \"author\": \"Alex Palmer, Victor Mimo\",\n     \"license\": \"MIT\",\n-    \"description\": \"Transfer Agent Protocol compliant cap table\",\n+    \"description\": \"Transfer Agent Protocol onchain cap table\",\n     \"type\": \"module\",\n     \"scripts\": {\n         \"start\": \"nodemon src/server.js\",\n@@ -13,21 +13,11 @@\n         \"format\": \"prettier '**/*' --ignore-unknown --write\",\n         \"typecheck\": \"concurrently --raw yarn:typecheck:*\",\n         \"typecheck:app\": \"tsc --noEmit\",\n-        \"typecheck:cypress\": \"tsc --noEmit -p cypress/tsconfig.json\",\n         \"prepare\": \"husky install\",\n         \"build\": \"cd chain && node ../src/scripts/deployAndLinkLibs.js\",\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n         \"deseed\": \"node src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\",\n-        \"validate-poet-files\": \"cd ocf && yarn validate-poet-files\",\n-        \"test-onchain-cap-table-local\": \"node src/chain-operations/capTable.cjs local\",\n-        \"test-onchain-cap-table-optimism-goerli\": \"node src/chain-operations/capTable.cjs optimism-goerli\",\n-        \"test-onchain-cap-table-factory-local\": \"node src/chain-operations/capTableFactory.cjs local\",\n-        \"test-onchain-cap-table-factory-optimism-goerli\": \"node src/chain-operations/capTableFactory.cjs optimism-goerli\",\n-        \"forge-deploy-captable-local\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-optimism-goerli\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-factory-local\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-factory-optimism-goerli\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n         \"export-manifest\": \"cd src/db/samples && zip -r notPoet.zip notPoet && mv notPoet.zip $HOME/Downloads\"\n     },\n     \"dependencies\": {"
            },
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 13,
                "deletions": 64,
                "patch": "@@ -8,16 +8,20 @@ import { readFactory } from \"../db/operations/read.js\";\n \n config();\n \n-async function deployCapTableLocal(issuerId, issuerName, initial_shares_authorized) {\n-    // Replace with your private key and provider endpoint\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n+async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n+    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n+    const RPC_URL = process.env.RPC_URL;\n+    const CHAIN_ID = process.env.CHAIN_ID;\n+\n     const customNetwork = {\n-        chainId: 31337,\n-        name: \"local\",\n+        // Change the CHAIN_ID in the .env file to deploy to a different network\n+        chainId: parseInt(CHAIN_ID),\n     };\n-    const provider = new ethers.JsonRpcProvider(\"http://127.0.0.1:8545\", customNetwork);\n+\n+    const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-    console.log(\"\ud83d\uddfd | Wallet address \", wallet.address);\n+\n+    console.log(\"\ud83d\uddfd | Wallet address: \", wallet.address);\n \n     const factory = await readFactory();\n     const factoryAddress = factory[0].factory_address;\n@@ -30,12 +34,6 @@ async function deployCapTableLocal(issuerId, issuerName, initial_shares_authoriz\n    \n     const capTableFactory = new ethers.Contract(factoryAddress, CAP_TABLE_FACTORY.abi, wallet);\n \n-    console.log(\n-        `\u2705 | Issuer id inside of deployment: ${issuerId},\n-\t\t\u2705 | Issuer name inside of deployment: ${issuerName},\n-\t\t\u2705 | With initial shares: ${initial_shares_authorized}`\n-    );\n-\n     const tx = await capTableFactory.createCapTable(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));\n     await tx.wait();\n \n@@ -46,7 +44,7 @@ async function deployCapTableLocal(issuerId, issuerName, initial_shares_authoriz\n     const contract = new ethers.Contract(latestCapTableProxyContractAddress, CAP_TABLE.abi, wallet); \n \n     console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n-    console.log(\"cap table contract address \", latestCapTableProxyContractAddress);\n+    console.log(\"\u2705 | Cap table contract address \", latestCapTableProxyContractAddress);\n     const libraries = getTXLibContracts(latestCapTableProxyContractAddress, wallet);\n \n     return {\n@@ -57,53 +55,4 @@ async function deployCapTableLocal(issuerId, issuerName, initial_shares_authoriz\n     };\n }\n \n-async function deployCapTableOptimismGoerli(issuerId, issuerName, initial_shares_authorized) {\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_POET_TEST;\n-    const provider = new ethers.JsonRpcProvider(process.env.OPTIMISM_GOERLI_RPC_URL);\n-    const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-\n-    const factory = await readFactory(); // Assumes this function fetches the factory address for Goerli\n-    const factoryAddress = factory[0].factory_address;\n-\n-    if(!factoryAddress) {\n-        throw new Error(`\u274c | Factory address not found`);\n-    }\n-\n-    const capTableFactory = new ethers.Contract(factoryAddress, CAP_TABLE_FACTORY.abi, wallet);\n-\n-    console.log(\n-        `\u2705 | Issuer id inside of deployment: ${issuerId},\n-        \u2705 | Issuer name inside of deployment: ${issuerName},\n-        \u2705 | With initial shares: ${initial_shares_authorized}`\n-    );\n-\n-    const tx = await capTableFactory.createCapTable(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));\n-    await tx.wait();\n-\n-    const capTableCount = await capTableFactory.getCapTableCount();\n-    const latestCapTableProxyContractAddress = await capTableFactory.capTableProxies(capTableCount - BigInt(1));\n-\n-    const contract = new ethers.Contract(latestCapTableProxyContractAddress, CAP_TABLE.abi, wallet);\n-\n-    console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n-    console.log(\"cap table contract address \", latestCapTableProxyContractAddress);\n-    const libraries = getTXLibContracts(latestCapTableProxyContractAddress, wallet);\n-\n-    return {\n-        contract,\n-        provider,\n-        address: latestCapTableProxyContractAddress,\n-        libraries,\n-    };\n-}\n-\n-async function deployCapTable(chain, issuerId, issuerName, initial_shares_authorized) {\n-    if (chain === \"local\") {\n-        return deployCapTableLocal(issuerId, issuerName, initial_shares_authorized);\n-    } else if (chain === \"optimism-goerli\") {\n-        return deployCapTableOptimismGoerli(issuerId, issuerName, initial_shares_authorized);\n-    } else {\n-        throw new Error(`\u274c | Unsupported chain: ${chain}`);\n-    }\n-}\n-export default deployCapTable;\n+export default deployCapTable;\n\\ No newline at end of file"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 10,
                "deletions": 34,
                "patch": "@@ -5,46 +5,22 @@ import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n \n config();\n \n-\n-async function getLocalContractInstance(address) {\n-    const CONTRACT_ADDRESS_LOCAL = address;\n-\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n-    const LOCAL_RPC_URL = process.env.LOCAL_RPC_URL;\n+async function getContractInstance(chain, address) {\n+    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n+    const RPC_URL = process.env.RPC_URL;\n+    const CHAIN_ID = parseInt(process.env.CHAIN_ID); // Convert to integer\n \n     const customNetwork = {\n-        chainId: 31337,\n-        name: \"local\",\n+        chainId: CHAIN_ID, // Use the CHAIN_ID from .env\n+        name: chain, // Use the chain parameter as the network name\n     };\n \n-    const provider = new ethers.JsonRpcProvider(LOCAL_RPC_URL, customNetwork);\n+    const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-    const contract = new ethers.Contract(CONTRACT_ADDRESS_LOCAL, CAP_TABLE.abi, wallet);\n-    const libraries = getTXLibContracts(contract.target, wallet)\n-    return { contract, provider, libraries };\n-}\n-\n-async function getOptimismGoerliContractInstance(address) {\n-    const CONTRACT_ADDRESS_OPTIMISM_GOERLI = address;\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_POET_TEST;\n+    const contract = new ethers.Contract(address, CAP_TABLE.abi, wallet);\n+    const libraries = getTXLibContracts(contract.target, wallet);\n \n-    const provider = new ethers.JsonRpcProvider(process.env.OPTIMISM_GOERLI_RPC_URL);\n-    const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-    const contract = new ethers.Contract(CONTRACT_ADDRESS_OPTIMISM_GOERLI, abi, wallet);\n-    const libraries = getTXLibContracts(contract.target, wallet)\n-\n-\n-    return { contract, provider, libraries};\n-}\n-\n-async function getContractInstance(chain, address) {\n-    if (chain === \"local\") {\n-        return getLocalContractInstance(address);\n-    } else if (chain === \"optimism-goerli\") {\n-        return getOptimismGoerliContractInstance(address);\n-    } else {\n-        throw new Error(`Unsupported chain: ${chain}`);\n-    }\n+    return { contract, provider, libraries };\n }\n \n export default getContractInstance;"
            },
            {
                "filename": "src/chain-operations/seed.js",
                "additions": 0,
                "deletions": 1,
                "patch": "@@ -1,6 +1,5 @@\n import { convertAndReflectStakeholderOnchain } from \"../controllers/stakeholderController.js\";\n import { convertAndReflectStockClassOnchain } from \"../controllers/stockClassController.js\";\n-import { convertAndSeedIssuanceStockOnchain, convertAndSeedTransferStockOnchain } from \"../controllers/transactions/seed.js\";\n import { getAllIssuerDataById } from \"../db/operations/read.js\";\n import { convertTimeStampToUint40, toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import { convertBytes16ToUUID, convertUUIDToBytes16 } from \"../utils/convertUUID.js\";"
            },
            {
                "filename": "src/scripts/deployAndLinkLibs.js",
                "additions": 39,
                "deletions": 13,
                "patch": "@@ -1,13 +1,14 @@\n-import { spawn } from \"child_process\";\n import { config } from \"dotenv\";\n+import { spawn } from \"child_process\";\n import fs from \"fs\";\n import path from \"path\";\n import sleep from \"../utils/sleep.js\";\n \n config();\n \n-const privateKey = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n-\n+const PRIVATE_KEY = process.env.PRIVATE_KEY;\n+const RPC_URL = process.env.RPC_URL;\n+const CHAIN_ID = process.env.CHAIN_ID;\n const rootDirectory = \"./src/lib\";\n const excludeDirectory = \"./src/lib/transactions\";\n \n@@ -70,37 +71,62 @@ function getAllLibraries(dirPath) {\n \n async function deployLib(lib, libs) {\n     return new Promise((resolve, reject) => {\n-        console.log(`Deploying ${lib.libraryName || lib.fileName}\\n`);\n-        const librariesDepsArgs = lib.deps.map((idx) => [\"--libraries\", `${libs[idx].path}:${libs[idx].libraryName}:${libs[idx].address}`]).flat();\n+        console.log(`Starting deployment for ${lib.libraryName || lib.fileName}`);\n+\n+        // Logging the dependencies and their addresses\n+        const librariesDepsArgs = lib.deps\n+            .map((idx) => {\n+                console.log(`Dependency: ${libs[idx].libraryName}, Address: ${libs[idx].address}`);\n+                return [\"--libraries\", `${libs[idx].path}:${libs[idx].libraryName}:${libs[idx].address}`];\n+            })\n+            .flat();\n         console.log(\"librariesDepsArgs:\", librariesDepsArgs);\n+\n+        // Forge command arguments\n         const args = [\n             \"c\",\n             \"-r\",\n-            \"http://127.0.0.1:8545\",\n+            // Using the RPC_URL from .env\n+            RPC_URL,\n             \"--chain\",\n-            31337,\n+            CHAIN_ID,\n             \"--private-key\",\n-            privateKey,\n+            PRIVATE_KEY,\n             `${lib.path}:${lib.libraryName || lib.fileName}`,\n-            // \"--via-ir\",\n             \"--json\",\n             ...librariesDepsArgs,\n         ];\n+        // TODO: only log when things break console.log(`Forge command arguments: ${args.join(\" \")}`);\n+\n+        // Executing the forge command\n         const subprocess = spawn(\"forge\", args);\n \n         subprocess.stdout.on(\"data\", (data) => {\n-            lib.address = JSON.parse(data).deployedTo;\n-            console.log(`${lib.fileName} Deployed Successfully - Address: ${lib.address}\\n`);\n+            console.log(`stdout: ${data}`);\n+            try {\n+                lib.address = JSON.parse(data).deployedTo;\n+                console.log(`${lib.fileName} Deployed Successfully - Address: ${lib.address}`);\n+            } catch (err) {\n+                console.error(`Error parsing JSON from stdout: ${err}`);\n+            }\n         });\n \n         subprocess.stderr.on(\"data\", (data) => {\n-            reject(new Error(`${lib.fileName}::stderr: ${data}`));\n+            const errorDetails = data.toString(); // Convert Buffer to string\n+            console.error(`stderr: ${errorDetails}`);\n+        });\n+\n+        subprocess.on(\"error\", (error) => {\n+            console.error(`Spawn error: ${error}`);\n+            reject(new Error(`Error executing forge command for ${lib.fileName}`));\n         });\n \n         subprocess.on(\"close\", (code) => {\n             if (code !== 0) {\n-                reject(new Error(`${lib.fileName}::child process exited with code ${code}`));\n+                console.error(`${lib.fileName} deployment failed with exit code ${code}`);\n+                reject(new Error(`${lib.fileName} deployment failed. Details in the logs above.`));\n             } else {\n+                console.log(`${lib.fileName} deployment process exited successfully`);\n                 resolve();\n             }\n         });"
            }
        ]
    },
    {
        "sha": "59f8045638b2a64b574ad8a880d82230bd96d8f5",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 17:42:06+00:00",
        "message": "Standard logging",
        "files": [
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -44,7 +44,7 @@ async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n     const contract = new ethers.Contract(latestCapTableProxyContractAddress, CAP_TABLE.abi, wallet); \n \n     console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n-    console.log(\"cap table contract address \", latestCapTableProxyContractAddress);\n+    console.log(\"\u2705 | Cap table contract address \", latestCapTableProxyContractAddress);\n     const libraries = getTXLibContracts(latestCapTableProxyContractAddress, wallet);\n \n     return {"
            }
        ]
    },
    {
        "sha": "7b3849ad7e5d9823fbdf16e0f70493776991d35e",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 17:41:46+00:00",
        "message": "Use conventional deployerPrivateKey instead of deployerPrivateFakeKey for consistency",
        "files": [
            {
                "filename": "chain/script/CapTableFactory.s.sol",
                "additions": 6,
                "deletions": 6,
                "patch": "@@ -9,26 +9,26 @@ import \"../src/CapTableFactory.sol\";\n \n /// @dev Test deployment using `forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast`\n contract DeployCapTableFactoryDeployLocalScript is Script {\n-    uint256 deployerPrivateKeyFakeAccount;\n+    uint256 deployerPrivateKey;\n \n     function setUp() public {\n         console.log(\"Upgrading CapTableFactory with CapTable implementation\");\n \n-        deployerPrivateKeyFakeAccount = vm.envUint(\"PRIVATE_KEY_FAKE_ACCOUNT\");\n+        deployerPrivateKey = vm.envUint(\"PRIVATE_KEY\");\n     }\n \n     function run() external {\n         console.log(\"Deploying CapTableFactory and CapTable implementation\");\n \n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTable capTable = new CapTable();\n         console.log(\"CapTable implementation deployed at:\", address(capTable));\n \n         vm.stopBroadcast(); // End the transaction\n \n         // Deploy CapTableFactory with the address of CapTable implementation\n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTableFactory capTableFactory = new CapTableFactory(address(capTable));\n         console.log(\"CapTableFactory deployed at:\", address(capTableFactory));\n@@ -38,15 +38,15 @@ contract DeployCapTableFactoryDeployLocalScript is Script {\n \n     /// @dev Run using `forge tx script/CapTableFactory.s.sol upgradeCapTable [0x...] --fork-url http://localhost:8545 --broadcast`\n     function upgradeCapTable(address factory) external {\n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTable capTable = new CapTable();\n         console.log(\"CapTable implementation deployed at:\", address(capTable));\n \n         vm.stopBroadcast(); // End the transaction\n \n         // Upgrade CapTableFactory with the address of CapTable implementation\n-        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+        vm.startBroadcast(deployerPrivateKey); // Start a new transaction\n \n         CapTableFactory capTableFactory = CapTableFactory(factory);\n "
            }
        ]
    },
    {
        "sha": "1e3977cf9b7ac11c2f503d2ad61cf0fe045782bf",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 17:40:58+00:00",
        "message": "Merge branch 'dev' into apalmer/deployment-scripts",
        "files": [
            {
                "filename": ".github/dependabot.yml",
                "additions": 10,
                "deletions": 0,
                "patch": "@@ -0,0 +1,10 @@\n+# Config for Dependabot to automatically send PRs to keep dependencies up-to-date\n+version: 2\n+updates:\n+  # Configuration for npm\n+  - package-ecosystem: \"npm\"\n+    directory: \"/\"\n+    schedule:\n+      interval: \"daily\"\n+    # Create PRs for version updates on dev branch only\n+    target-branch: \"dev\""
            },
            {
                "filename": ".gitmodules",
                "additions": 4,
                "deletions": 1,
                "patch": "@@ -1,9 +1,12 @@\n [submodule \"ocf\"]\n \tpath = ocf\n-\turl = https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF.git\n+\turl = https://github.com/poet-network/tap-ocf.git\n [submodule \"chain/lib/forge-std\"]\n \tpath = chain/lib/forge-std\n \turl = https://github.com/foundry-rs/forge-std\n [submodule \"chain/lib/openzeppelin-contracts\"]\n \tpath = chain/lib/openzeppelin-contracts\n \turl = https://github.com/OpenZeppelin/openzeppelin-contracts\n+[submodule \"chain/lib/openzeppelin-contracts-upgradeable\"]\n+\tpath = chain/lib/openzeppelin-contracts-upgradeable\n+\turl = https://github.com/openzeppelin/openzeppelin-contracts-upgradeable\n\\ No newline at end of file"
            },
            {
                "filename": "README.md",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,4 +1,4 @@\n-# Transfer Agent Protocol (TAP) Specification for a compliant cap table onchain\n+# Transfer Agent Protocol, Specification for a compliant cap table onchain [TAP](https://transferagentprotocol.xyz)\n \n Developed by:\n "
            },
            {
                "filename": "chain/foundry.toml",
                "additions": 4,
                "deletions": 0,
                "patch": "@@ -2,6 +2,10 @@\n src = \"src\"\n out = \"out\"\n libs = [\"lib\"]\n+solc_version = '0.8.20'\n+bytecode_hash = \"none\"\n+cbor_metadata = false\n+\n \n [rpc_endpoints]\n rpc_url = \"${RPC_URL}\""
            },
            {
                "filename": "chain/remappings.txt",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -1,5 +1,5 @@\n ds-test/=lib/forge-std/lib/ds-test/src/\n forge-std/=lib/forge-std/src/\n-openzeppelin-contracts/=lib/openzeppelin-contracts/\n lib/forge-std:ds-test/=lib/forge-std/lib/ds-test/src/\n-lib/openzeppelin-contracts:openzeppelin/=lib/openzeppelin-contracts/contracts/\n+openzeppelin/=lib/openzeppelin-contracts/\n+openzeppelin-upgradeable/=lib/openzeppelin-contracts-upgradeable/"
            },
            {
                "filename": "chain/script/CapTable.s.sol",
                "additions": 0,
                "deletions": 35,
                "patch": "@@ -1,35 +0,0 @@\n-// SPDX-License-Identifier: MIT\n-pragma solidity ^0.8.20;\n-\n-import \"forge-std/Script.sol\";\n-import \"forge-std/console.sol\";\n-\n-import \"../src/CapTable.sol\";\n-\n-contract CapTableDeployLocalScript is Script {\n-    function setUp() public {}\n-\n-    function run() public {\n-        uint256 deployerPrivateKeyFakeAccount = vm.envUint(\"PRIVATE_KEY\");\n-\n-        vm.startBroadcast(deployerPrivateKeyFakeAccount);\n-\n-        new CapTable(0xd3373e0a4dd9430f8a563281f2800e1e, \"Winston Inc.\", 10000000);\n-\n-        vm.stopBroadcast();\n-    }\n-}\n-\n-contract CapTableDeployOptimismGoerli is Script {\n-    function setUp() public {}\n-\n-    function run() public {\n-        uint256 deployerPrivateKeyPoetTest = vm.envUint(\"PRIVATE_KEY\");\n-\n-        vm.startBroadcast(deployerPrivateKeyPoetTest);\n-\n-        new CapTable(0xd3373e0a4dd9430f8a563281f2800e1e, \"Winston Inc.\", 1000000);\n-\n-        vm.stopBroadcast();\n-    }\n-}"
            },
            {
                "filename": "chain/script/CapTableFactory.s.sol",
                "additions": 58,
                "deletions": 0,
                "patch": "@@ -0,0 +1,58 @@\n+// SPDX-License-Identifier: MIT\n+pragma solidity ^0.8.20;\n+\n+import \"forge-std/Script.sol\";\n+import \"forge-std/console.sol\";\n+\n+import \"../src/CapTable.sol\";\n+import \"../src/CapTableFactory.sol\";\n+\n+/// @dev Test deployment using `forge script script/CapTableFactory.s.sol --fork-url http://localhost:8545 --broadcast`\n+contract DeployCapTableFactoryDeployLocalScript is Script {\n+    uint256 deployerPrivateKeyFakeAccount;\n+\n+    function setUp() public {\n+        console.log(\"Upgrading CapTableFactory with CapTable implementation\");\n+\n+        deployerPrivateKeyFakeAccount = vm.envUint(\"PRIVATE_KEY_FAKE_ACCOUNT\");\n+    }\n+\n+    function run() external {\n+        console.log(\"Deploying CapTableFactory and CapTable implementation\");\n+\n+        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+\n+        CapTable capTable = new CapTable();\n+        console.log(\"CapTable implementation deployed at:\", address(capTable));\n+\n+        vm.stopBroadcast(); // End the transaction\n+\n+        // Deploy CapTableFactory with the address of CapTable implementation\n+        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+\n+        CapTableFactory capTableFactory = new CapTableFactory(address(capTable));\n+        console.log(\"CapTableFactory deployed at:\", address(capTableFactory));\n+\n+        vm.stopBroadcast(); // End the transaction\n+    }\n+\n+    /// @dev Run using `forge tx script/CapTableFactory.s.sol upgradeCapTable [0x...] --fork-url http://localhost:8545 --broadcast`\n+    function upgradeCapTable(address factory) external {\n+        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+\n+        CapTable capTable = new CapTable();\n+        console.log(\"CapTable implementation deployed at:\", address(capTable));\n+\n+        vm.stopBroadcast(); // End the transaction\n+\n+        // Upgrade CapTableFactory with the address of CapTable implementation\n+        vm.startBroadcast(deployerPrivateKeyFakeAccount); // Start a new transaction\n+\n+        CapTableFactory capTableFactory = CapTableFactory(factory);\n+\n+        capTableFactory.updateCapTableImplementation(address(capTable));\n+        console.log(\"CapTableFactory upgraded to:\", address(capTable));\n+\n+        vm.stopBroadcast(); // End the transaction\n+    }\n+}"
            },
            {
                "filename": "chain/src/CapTable.sol",
                "additions": 16,
                "deletions": 8,
                "patch": "@@ -1,13 +1,14 @@\n // SPDX-License-Identifier: MIT\n pragma solidity ^0.8.20;\n \n-import { AccessControlDefaultAdminRules } from \"openzeppelin-contracts/contracts/access/AccessControlDefaultAdminRules.sol\";\n-import { ICapTable } from \"./ICapTable.sol\";\n+import { AccessControlDefaultAdminRulesUpgradeable } from \"openzeppelin-upgradeable/contracts/access/AccessControlDefaultAdminRulesUpgradeable.sol\";\n+\n+import { ICapTable } from \"./interfaces/ICapTable.sol\";\n import { StockTransferParams, Issuer, Stakeholder, StockClass, InitialShares, ActivePositions, SecIdsStockClass, StockLegendTemplate, StockParams, StockParamsQuantity, StockIssuanceParams } from \"./lib/Structs.sol\";\n import \"./lib/transactions/Adjustment.sol\";\n import \"./lib/Stock.sol\";\n \n-contract CapTable is ICapTable, AccessControlDefaultAdminRules {\n+contract CapTable is ICapTable, AccessControlDefaultAdminRulesUpgradeable {\n     Issuer public issuer;\n     Stakeholder[] public stakeholders;\n     StockClass[] public stockClasses;\n@@ -50,14 +51,18 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRules {\n     error WalletAlreadyExists(address wallet);\n     error NoActivePositionFound();\n \n-    constructor(bytes16 _id, string memory _name, uint256 _initial_shares_authorized) AccessControlDefaultAdminRules(0 seconds, _msgSender()) {\n-        _grantRole(ADMIN_ROLE, _msgSender());\n+    constructor() {\n+        _disableInitializers();\n+    }\n+\n+    function initialize(bytes16 id, string memory name, uint256 initial_shares_authorized, address admin) external initializer {\n+        __AccessControlDefaultAdminRules_init(0 seconds, admin);\n+        _grantRole(ADMIN_ROLE, admin);\n         _setRoleAdmin(ADMIN_ROLE, ADMIN_ROLE);\n         _setRoleAdmin(OPERATOR_ROLE, ADMIN_ROLE);\n \n-        nonce = 0;\n-        issuer = Issuer(_id, _name, 0, _initial_shares_authorized);\n-        emit IssuerCreated(_id, _name);\n+        issuer = Issuer(id, name, 0, initial_shares_authorized);\n+        emit IssuerCreated(id, name);\n     }\n \n     /// @inheritdoc ICapTable\n@@ -117,6 +122,9 @@ contract CapTable is ICapTable, AccessControlDefaultAdminRules {\n         );\n \n         for (uint256 i = 0; i < stakeholderIds.length; i++) {\n+            // perform requires to ensure valid stakeholders and stock classes\n+            _checkStakeholderIsStored(stakeholderIds[i]);\n+            _checkInvalidStockClass(stockClassIds[i]);\n             positions.activePositions[stakeholderIds[i]][securityIds[i]] = ActivePosition(\n                 stockClassIds[i],\n                 quantities[i],"
            },
            {
                "filename": "chain/src/CapTableFactory.sol",
                "additions": 40,
                "deletions": 0,
                "patch": "@@ -0,0 +1,40 @@\n+// SPDX-License-Identifier: MIT\n+pragma solidity ^0.8.20;\n+\n+import { UpgradeableBeacon } from \"openzeppelin/contracts/proxy/beacon/UpgradeableBeacon.sol\";\n+import { BeaconProxy } from \"openzeppelin/contracts/proxy/beacon/BeaconProxy.sol\";\n+import { Ownable } from \"openzeppelin/contracts/access/Ownable.sol\";\n+import { ICapTableFactory } from \"./interfaces/ICapTableFactory.sol\";\n+import { ICapTable } from \"./interfaces/ICapTable.sol\";\n+\n+contract CapTableFactory is ICapTableFactory, Ownable {\n+    address public capTableImplementation;\n+    UpgradeableBeacon public capTableBeacon;\n+    address[] public capTableProxies;\n+\n+    constructor(address _capTableImplementation) {\n+        require(_capTableImplementation != address(0), \"Invalid implementation address\");\n+        capTableImplementation = _capTableImplementation;\n+        capTableBeacon = new UpgradeableBeacon(capTableImplementation);\n+    }\n+\n+    function createCapTable(bytes16 id, string memory name, uint256 initial_shares_authorized) external onlyOwner returns (address) {\n+        require(id != bytes16(0) && initial_shares_authorized != 0, \"Invalid issuer params\");\n+\n+        bytes memory initializationData = abi.encodeCall(ICapTable.initialize, (id, name, initial_shares_authorized, msg.sender));\n+        BeaconProxy capTableProxy = new BeaconProxy(address(capTableBeacon), initializationData);\n+        capTableProxies.push(address(capTableProxy));\n+        emit CapTableCreated(address(capTableProxy));\n+        return address(capTableProxy);\n+    }\n+\n+    function updateCapTableImplementation(address newImplementation) external onlyOwner {\n+        require(newImplementation != address(0), \"Invalid implementation address\");\n+        capTableBeacon.upgradeTo(newImplementation);\n+        capTableImplementation = newImplementation;\n+    }\n+\n+    function getCapTableCount() external view returns (uint256) {\n+        return capTableProxies.length;\n+    }\n+}"
            },
            {
                "filename": "chain/src/interfaces/ICapTable.sol",
                "additions": 5,
                "deletions": 3,
                "patch": "@@ -1,9 +1,8 @@\n // SPDX-License-Identifier: MIT\n pragma solidity ^0.8.20;\n \n-import \"openzeppelin-contracts/contracts/utils/math/SafeMath.sol\";\n-import { AccessControlDefaultAdminRules } from \"openzeppelin-contracts/contracts/access/AccessControlDefaultAdminRules.sol\";\n-import { Issuer, Stakeholder, StockClass, ActivePositions, SecIdsStockClass, StockLegendTemplate, InitialShares, ShareNumbersIssued, StockParams, StockParamsQuantity, StockIssuanceParams } from \"./lib/Structs.sol\";\n+import { AccessControlDefaultAdminRules } from \"openzeppelin/contracts/access/AccessControlDefaultAdminRules.sol\";\n+import { Issuer, Stakeholder, StockClass, ActivePositions, SecIdsStockClass, StockLegendTemplate, InitialShares, ShareNumbersIssued, StockParams, StockParamsQuantity, StockIssuanceParams } from \"../lib/Structs.sol\";\n \n interface ICapTable {\n     // @dev Transactions will be created on-chain then reflected off-chain.\n@@ -20,6 +19,9 @@ interface ICapTable {\n \n     function OPERATOR_ROLE() external returns (bytes32);\n \n+    /// @notice Initializer for the CapTable, sets access control and initializes issuer struct.\n+    function initialize(bytes16 id, string memory name, uint256 initial_shares_authorized, address admin) external;\n+\n     function seedMultipleActivePositionsAndSecurityIds(\n         bytes16[] calldata stakeholderIds,\n         bytes16[] calldata securityIds,"
            },
            {
                "filename": "chain/src/interfaces/ICapTableFactory.sol",
                "additions": 12,
                "deletions": 0,
                "patch": "@@ -0,0 +1,12 @@\n+// SPDX-License-Identifier: MIT\n+pragma solidity ^0.8.20;\n+\n+interface ICapTableFactory {\n+    event CapTableCreated(address indexed capTableProxy);\n+\n+    function createCapTable(bytes16 id, string memory name, uint256 initial_shares_authorized) external returns (address);\n+\n+    function updateCapTableImplementation(address newImplementation) external;\n+\n+    function getCapTableCount() external view returns (uint256);\n+}"
            },
            {
                "filename": "chain/src/lib/Stock.sol",
                "additions": 1,
                "deletions": 2,
                "patch": "@@ -327,8 +327,7 @@ library StockLib {\n \n         TxHelper.createTx(TxType.STOCK_TRANSFER, abi.encode(transfer), transactions);\n \n-        issuer.shares_issued = issuer.shares_issued - transferorActivePosition.quantity;\n-        stockClass.shares_issued = stockClass.shares_issued - transferorActivePosition.quantity;\n+        _subtractSharesIssued(issuer, stockClass, transferorActivePosition.quantity);\n \n         DeleteContext.deleteActivePosition(params.transferor_stakeholder_id, transferorSecurityId, positions);\n         DeleteContext.deleteActiveSecurityIdsByStockClass(params.transferor_stakeholder_id, params.stock_class_id, transferorSecurityId, activeSecs);"
            },
            {
                "filename": "chain/test/AccessControl.t.sol",
                "additions": 57,
                "deletions": 65,
                "patch": "@@ -1,65 +1,57 @@\n-// // SPDX-License-Identifier: MIT\n-// pragma solidity ^0.8.20;\n-\n-// import \"forge-std/Test.sol\";\n-// import \"./CapTable.t.sol\";\n-\n-// contract RolesTests is CapTableTest {\n-//     address RANDO_ADDR = address(0xf001);\n-//     address OPERATOR_ADDR = address(0xf002);\n-\n-//     function testOperatorRole() public {\n-//         _attemptOperatorFunc(false, true);\n-\n-//         capTable.addOperator(RANDO_ADDR);\n-//         _attemptOperatorFunc(true, true);\n-\n-//         capTable.removeOperator(RANDO_ADDR);\n-//         _attemptOperatorFunc(false, true);\n-//     }\n-\n-//     function testAdminRole() public {\n-//         _attemptAdminFunc(false);\n-\n-//         capTable.addAdmin(RANDO_ADDR);\n-//         _attemptAdminFunc(true);\n-\n-//         capTable.removeAdmin(RANDO_ADDR);\n-//         _attemptAdminFunc(false);\n-//     }\n-\n-//     function testAdminIsOperatorRole() public {\n-//         _attemptOperatorFunc(true, false);\n-//     }\n-\n-//     function _attemptOperatorFunc(bool shouldBeOperator, bool asRando) internal {\n-//         if (shouldBeOperator) {\n-//             vm.expectRevert(\"No transferor\");\n-//         } else {\n-//             vm.expectRevert(\"Does not have operator role\");\n-//         }\n-//         if (asRando) {\n-//             vm.prank(RANDO_ADDR);\n-//         }\n-//         /// @dev this was chosen arbitrarily bc it is onlyOperator. The arguments are garbage and we expect it to throw\n-//         capTable.transferStock(\n-//             0x0000000000000000000000000000000a,\n-//             0x0000000000000000000000000000000b,\n-//             0x0000000000000000000000000000000c,\n-//             false,\n-//             0,\n-//             0\n-//         );\n-//     }\n-\n-//     function _attemptAdminFunc(bool shouldBeAdmin) internal {\n-//         if (shouldBeAdmin) {\n-//             vm.expectRevert(\"Invalid wallet\");\n-//         } else {\n-//             vm.expectRevert(\"Does not have admin role\");\n-//         }\n-//         vm.prank(RANDO_ADDR);\n-//         /// @dev this was chosen arbitrarily bc it is onlyAdmin. The arguments are garbage and we expect it to throw\n-//         capTable.addWalletToStakeholder(0x0000000000000000000000000000000d, address(0));\n-//     }\n-// }\n+// SPDX-License-Identifier: MIT\n+pragma solidity ^0.8.20;\n+\n+import \"./CapTable.t.sol\";\n+\n+contract RolesTests is CapTableTest {\n+    address RANDO_ADDR = address(0xf001);\n+    address OPERATOR_ADDR = address(0xf002);\n+\n+    function testIssuerInitialization() public {\n+        (bytes16 id, string memory legalName, , ) = capTable.issuer();\n+        assertEq(id, issuerId);\n+        assertEq(legalName, \"Winston, Inc.\");\n+    }\n+\n+    function testOperatorTransfer() public {\n+        bytes16[] memory stakeholderIds = new bytes16[](2);\n+\n+        for (uint256 i = 0; i < 2; i++) {\n+            bytes16 stakeholderId = bytes16(keccak256(abi.encodePacked(\"STAKEHOLDER\", i)));\n+            string memory stakeholderType = \"Individual\";\n+            string memory currentRelationship = \"Investor\";\n+            capTable.createStakeholder(stakeholderId, stakeholderType, currentRelationship);\n+            stakeholderIds[i] = stakeholderId;\n+        }\n+\n+        bytes16 stockClassId = bytes16(keccak256(abi.encodePacked(\"STOCKCLASS\")));\n+        string memory classType = \"Common\";\n+        uint256 pricePerShare = 100; // Example value\n+        uint256 initialSharesAuthorized = 1000; // Example value\n+        capTable.createStockClass(stockClassId, classType, pricePerShare, initialSharesAuthorized);\n+\n+        // add operator and change address\n+        capTable.addOperator(OPERATOR_ADDR);\n+        vm.prank(OPERATOR_ADDR);\n+\n+        vm.expectRevert(\"No active security ids found\");\n+\n+        // will revert because we haven't performed an issuance, but it would have already verified operator\n+        // role working\n+        capTable.transferStock(\n+            stakeholderIds[0], // transferor\n+            stakeholderIds[1], // transferee\n+            stockClassId,\n+            true,\n+            100,\n+            100\n+        );\n+    }\n+\n+    function testNotAdminReverting() public {\n+        vm.prank(RANDO_ADDR);\n+\n+        vm.expectRevert(\"Does not have admin role\");\n+        capTable.createStakeholder(bytes16(\"0101\"), \"Individual\", \"Investor\");\n+    }\n+}"
            },
            {
                "filename": "chain/test/Adjustment.t.sol",
                "additions": 0,
                "deletions": 9,
                "patch": "@@ -3,15 +3,6 @@ pragma solidity ^0.8.20;\n \n import \"./CapTable.t.sol\";\n \n-/*\n-\n-    to test\n-    - Adjusting Issuer Authorized Shares\n-        - Fails\n-        - \n-\n-*/\n-\n contract AdjustmentTest is CapTableTest {\n     function testAdjustIssuerAuthorizedSharesBelowIssuedFails() public {\n         // Create stock class and stakeholder"
            },
            {
                "filename": "chain/test/CapTable.t.sol",
                "additions": 9,
                "deletions": 2,
                "patch": "@@ -3,15 +3,22 @@ pragma solidity ^0.8.20;\n \n import \"forge-std/Test.sol\";\n import \"../src/CapTable.sol\";\n+import \"../src/CapTableFactory.sol\";\n import { StockIssuanceParams } from \"../src/lib/Structs.sol\";\n \n contract CapTableTest is Test {\n+    CapTableFactory public factory;\n     CapTable public capTable;\n     uint256 public issuerInitialSharesAuthorized = 1000000;\n+    string public issuerName = \"Winston, Inc.\";\n+    bytes16 issuerId = 0xd3373e0a4dd9430f8a563281f2800e1e;\n \n     function setUp() public {\n-        bytes16 issuerId = 0xd3373e0a4dd9430f8a563281f2800e1e;\n-        capTable = new CapTable(issuerId, \"Winston, Inc.\", issuerInitialSharesAuthorized);\n+        CapTable capTableImplementation = new CapTable();\n+\n+        factory = new CapTableFactory(address(capTableImplementation));\n+\n+        capTable = CapTable(factory.createCapTable(issuerId, issuerName, issuerInitialSharesAuthorized));\n     }\n \n     // HELPERS //"
            },
            {
                "filename": "chain/test/CapTableFactory.sol",
                "additions": 114,
                "deletions": 0,
                "patch": "@@ -0,0 +1,114 @@\n+// SPDX-License-Identifier: MIT\n+pragma solidity ^0.8.20;\n+\n+import \"forge-std/Test.sol\";\n+import \"../src/CapTableFactory.sol\";\n+import \"../src/CapTable.sol\";\n+import \"../src/interfaces/ICapTable.sol\";\n+\n+contract CapTableFactoryTest is Test {\n+    CapTableFactory private _capTableFactory;\n+    CapTable private _capTableImplementation;\n+\n+    function setUp() public {\n+        // Deploy empty implementation\n+        _capTableImplementation = new CapTable();\n+\n+        _capTableFactory = new CapTableFactory(address(_capTableImplementation));\n+    }\n+\n+    function testRevertsInvalidImplementationAddress() public {\n+        vm.expectRevert(\"Invalid implementation address\");\n+        _capTableFactory = new CapTableFactory(address(0));\n+    }\n+\n+    function testCreateCapTable() public {\n+        bytes16 issuerId = 0xd3373e0a4dd9430f8a563281f2800e1e;\n+        string memory issuerName = \"Winston, Inc.\";\n+        uint256 issuerInitialSharesAuthorized = 1000000;\n+\n+        address capTableProxy = _capTableFactory.createCapTable(issuerId, issuerName, issuerInitialSharesAuthorized);\n+\n+        // Assert the cap table was created\n+        CapTable capTable = CapTable(capTableProxy);\n+\n+        (bytes16 id, string memory name, uint256 shares_issued, uint256 initial_shares_authorized) = capTable.issuer();\n+\n+        assertEq(id, issuerId);\n+        assertEq(name, issuerName);\n+        assertEq(shares_issued, 0);\n+        assertEq(initial_shares_authorized, issuerInitialSharesAuthorized);\n+\n+        // Verify by creating a stakeholder\n+        bytes16 stakeholderId = 0xd3373e0a4dd940000000000000000010;\n+        string memory stakeholderType = \"INDIVIDUAL\";\n+        string memory stakeholderRelationship = \"INVESTOR\";\n+\n+        capTable.createStakeholder(stakeholderId, stakeholderType, stakeholderRelationship);\n+\n+        (bytes16 actualStakeolderId, string memory actualStakeholderType, string memory actualStakeholderRelationship) = capTable.getStakeholderById(\n+            stakeholderId\n+        );\n+\n+        assertEq(actualStakeolderId, stakeholderId);\n+        assertEq(actualStakeholderType, stakeholderType);\n+        assertEq(actualStakeholderRelationship, stakeholderRelationship);\n+\n+        // Testing that only owner can create stakeholder\n+        vm.prank(address(1));\n+        vm.expectRevert(\"Does not have admin role\");\n+        capTable.createStakeholder(0xd3373e0a4dd940000000000000000005, \"INDIVIDUAL\", \"EMPLOYEE\");\n+    }\n+\n+    function testUpdateCapTableImplementation() public {\n+        // Create cap table prior to upgrade\n+        bytes16 issuerId0 = 0xd3373e0a4dd9430f8a563281f2800333;\n+        string memory issuerName0 = \"Alpha, Inc.\";\n+        uint256 issuerInitialSharesAuthorized0 = 1000000;\n+\n+        address capTableProxy0 = _capTableFactory.createCapTable(issuerId0, issuerName0, issuerInitialSharesAuthorized0);\n+\n+        // Assert the cap table was created\n+        CapTable capTable0 = CapTable(capTableProxy0);\n+\n+        // Deploy new implementation\n+        CapTable newCapTableImplementation = new CapTable();\n+\n+        // Update implementation\n+        _capTableFactory.updateCapTableImplementation(address(newCapTableImplementation));\n+\n+        // Assert the implementation was updated\n+        assertEq(address(_capTableFactory.capTableImplementation()), address(newCapTableImplementation));\n+        assertEq(address(_capTableFactory.capTableBeacon().implementation()), address(newCapTableImplementation));\n+\n+        // Create a cap table\n+        bytes16 issuerId = 0xd3373e0a4dd9430f8a563281f2800e1e;\n+        string memory issuerName = \"Winston, Inc.\";\n+        uint256 issuerInitialSharesAuthorized = 1000000;\n+\n+        address capTableProxy = _capTableFactory.createCapTable(issuerId, issuerName, issuerInitialSharesAuthorized);\n+\n+        // Assert the cap table was created\n+        CapTable capTable = CapTable(capTableProxy);\n+\n+        (bytes16 id, string memory name, uint256 shares_issued, uint256 initial_shares_authorized) = capTable.issuer();\n+\n+        assertEq(id, issuerId);\n+        assertEq(name, issuerName);\n+        assertEq(shares_issued, 0);\n+        assertEq(initial_shares_authorized, issuerInitialSharesAuthorized);\n+\n+        // make sure previous cap table still works\n+        bytes16 stakeholderId = 0xd3373e0a4dd940000000000000000010;\n+\n+        capTable0.createStakeholder(stakeholderId, \"INDIVIDUAL\", \"INVESTOR\");\n+\n+        (bytes16 actualStakeolderId, string memory actualStakeholderType, string memory actualStakeholderRelationship) = capTable0.getStakeholderById(\n+            stakeholderId\n+        );\n+\n+        assertEq(actualStakeolderId, stakeholderId);\n+        assertEq(actualStakeholderType, \"INDIVIDUAL\");\n+        assertEq(actualStakeholderRelationship, \"INVESTOR\");\n+    }\n+}"
            },
            {
                "filename": "chain/test/Seeding.t.sol",
                "additions": 64,
                "deletions": 49,
                "patch": "@@ -2,68 +2,53 @@\n pragma solidity ^0.8.20;\n \n import \"./CapTable.t.sol\";\n-import { InitialShares, IssuerInitialShares, StockClassInitialShares, Issuer, StockClass } from \"../src/lib/Structs.sol\";\n+import { InitialShares, IssuerInitialShares, StockClassInitialShares } from \"../src/lib/Structs.sol\";\n \n contract SeedingTest is CapTableTest {\n-    // TODO: should this be a global test helper?\n-    function _createStockClassAndStakeholder(uint256 stockClassInitialSharesAuthorized) private returns (bytes16, bytes16) {\n-        bytes16 stakeholderId = 0xd3373e0a4dd940000000000000000005;\n-        capTable.createStakeholder(stakeholderId, \"INDIVIDUAL\", \"EMOLOYEE\");\n-\n-        bytes16 stockClassId = 0xd3373e0a4dd940000000000000000000;\n-        capTable.createStockClass(stockClassId, \"Common\", 100, stockClassInitialSharesAuthorized);\n-\n-        return (stockClassId, stakeholderId);\n-    }\n-\n-    function createInitialDummyStockClassData() public pure returns (bytes16, string memory, uint256, uint256) {\n+    function createInitialDummyStockClassData() private pure returns (bytes16, string memory, uint256, uint256) {\n         bytes16 id = 0xd3373e0a4dd9430f8a563281f2454545;\n         string memory classType = \"Common\";\n         uint256 pricePerShare = 10000000000; // $1.00 with 10 decimals\n         uint256 initialSharesAuthorized = 100000000000000000; // 10,000,000\n         return (id, classType, pricePerShare, initialSharesAuthorized);\n     }\n \n-    function testSeedingAuthorizedAndIssuedShares() public {\n-        (\n-            bytes16 stockClassId,\n-            string memory classType,\n-            uint256 initialPricePerShare,\n-            uint256 initialInitialSharesAuthorized\n-        ) = createInitialDummyStockClassData();\n-        capTable.createStockClass(stockClassId, classType, initialPricePerShare, initialInitialSharesAuthorized);\n-        // stock class\n-        uint256 expectedStockClassSharesAuthorized = 1000000000000000000; // 100M\n-        uint256 expectedStockClassSharesIssued = 350000000000000000; // 35M\n+    function testValidSeedingOfShares() public {\n+        (bytes16 stockClassId, string memory classType, uint256 pricePerShare, uint256 initialSharesAuthorized) = createInitialDummyStockClassData();\n+        capTable.createStockClass(stockClassId, classType, pricePerShare, initialSharesAuthorized);\n \n-        // issuer\n         uint256 expectedIssuerSharesAuthorized = 1000000000000000000; // 100M\n         uint256 expectedIssuerSharesIssued = 350000000000000000; // 35M\n+        uint256 expectedStockClassSharesAuthorized = 1000000000000000000; // 100M\n+        uint256 expectedStockClassSharesIssued = 350000000000000000; // 35M\n \n-        StockClassInitialShares[] memory expectedStockClassInitialShares = new StockClassInitialShares[](1);\n-        expectedStockClassInitialShares[0] = StockClassInitialShares(\n-            stockClassId,\n-            expectedStockClassSharesAuthorized,\n-            expectedStockClassSharesIssued\n-        );\n+        StockClassInitialShares[] memory stockClassInitialShares = new StockClassInitialShares[](1);\n+        stockClassInitialShares[0] = StockClassInitialShares(stockClassId, expectedStockClassSharesAuthorized, expectedStockClassSharesIssued);\n \n         InitialShares memory params = InitialShares(\n             IssuerInitialShares(expectedIssuerSharesAuthorized, expectedIssuerSharesIssued),\n-            expectedStockClassInitialShares\n+            stockClassInitialShares\n         );\n \n         capTable.seedSharesAuthorizedAndIssued(params);\n \n         (, , uint256 actualIssuerSharesIssued, uint256 actualIssuerSharesAuthorized) = capTable.issuer();\n-\n-        (, , , uint256 scSharesAuthorized, uint256 scSharesIssued) = capTable.getStockClassById(stockClassId);\n+        (, , , uint256 scSharesIssued, uint256 scSharesAuthorized) = capTable.getStockClassById(stockClassId);\n \n         assertEq(actualIssuerSharesAuthorized, expectedIssuerSharesAuthorized);\n         assertEq(actualIssuerSharesIssued, expectedIssuerSharesIssued);\n         assertEq(scSharesAuthorized, expectedStockClassSharesAuthorized);\n         assertEq(scSharesIssued, expectedStockClassSharesIssued);\n     }\n \n+    function testSeedingWithInvalidParameters() public {\n+        // Attempt to seed with zero shares authorized and issued\n+        InitialShares memory params = InitialShares(IssuerInitialShares(0, 0), new StockClassInitialShares[](0));\n+\n+        vm.expectRevert(\"Invalid Seeding Shares Params\");\n+        capTable.seedSharesAuthorizedAndIssued(params);\n+    }\n+\n     function testSeedMultipleActivePositionsAndSecurityIds() public {\n         bytes16[] memory stakeholderIds = new bytes16[](5);\n         bytes16[] memory securityIds = new bytes16[](5);\n@@ -72,30 +57,60 @@ contract SeedingTest is CapTableTest {\n         uint256[] memory sharePrices = new uint256[](5);\n         uint40[] memory timestamps = new uint40[](5);\n \n-        (\n-            bytes16 stockClassId,\n-            string memory classType,\n-            uint256 initialPricePerShare,\n-            uint256 initialInitialSharesAuthorized\n-        ) = createInitialDummyStockClassData();\n-        capTable.createStockClass(stockClassId, classType, initialPricePerShare, initialInitialSharesAuthorized);\n         for (uint256 i = 0; i < 5; i++) {\n-            // (bytes16 stockClassId, bytes16 stakeholderId) = _createStockClassAndStakeholder(100000);\n-            stakeholderIds[i] = bytes16(uint128(i + 1));\n-            securityIds[i] = bytes16(uint128(i + 1)); // Dummy security IDs\n+            // Generate unique identifiers for stock classes and stakeholders\n+            bytes16 stockClassId = bytes16(keccak256(abi.encodePacked(\"STOCKCLASS\", i)));\n+            bytes16 stakeholderId = bytes16(keccak256(abi.encodePacked(\"STAKEHOLDER\", i)));\n+            bytes16 securityId = bytes16(keccak256(abi.encodePacked(\"SECURITY\", i)));\n+\n+            // Create stock classes and stakeholders\n+            capTable.createStockClass(stockClassId, \"Common\", 10000000000, 100000000000000000);\n+            capTable.createStakeholder(stakeholderId, \"INDIVIDUAL\", \"INVESTOR\");\n+\n+            stakeholderIds[i] = stakeholderId;\n+            securityIds[i] = securityId; // Dummy security IDs\n             stockClassIds[i] = stockClassId;\n             quantities[i] = 1000; // Dummy quantities\n             sharePrices[i] = 10000000000; // Dummy share prices\n             timestamps[i] = uint40(block.timestamp + i); // Dummy timestamps\n         }\n-        for (uint256 i = 0; i < stakeholderIds.length; i++) {\n-            capTable.createStakeholder(stakeholderIds[i], \"INDIVIDUAL\", \"INVESTOR\");\n-        }\n \n         capTable.seedMultipleActivePositionsAndSecurityIds(stakeholderIds, securityIds, stockClassIds, quantities, sharePrices, timestamps);\n \n         uint256 transactionCount = capTable.getTotalActiveSecuritiesCount();\n-        // get active position count\n-        assertEq(transactionCount, 5, \"Transaction count should be 5 after seeding multiple active positions and security IDs\");\n+        assertEq(transactionCount, 5);\n+    }\n+\n+    function testSeedingWithMismatchedArrayLengths() public {\n+        bytes16[] memory stakeholderIds = new bytes16[](1);\n+        bytes16[] memory securityIds = new bytes16[](2); // Mismatched length\n+        bytes16[] memory stockClassIds = new bytes16[](1);\n+        uint256[] memory quantities = new uint256[](1);\n+        uint256[] memory sharePrices = new uint256[](1);\n+        uint40[] memory timestamps = new uint40[](1);\n+\n+        vm.expectRevert(\"Input arrays must have the same length\");\n+        capTable.seedMultipleActivePositionsAndSecurityIds(stakeholderIds, securityIds, stockClassIds, quantities, sharePrices, timestamps);\n+    }\n+\n+    function testSeedingWithNonExistentStakeholdersOrStockClasses() public {\n+        bytes16[] memory stakeholderIds = new bytes16[](1);\n+        bytes16[] memory securityIds = new bytes16[](1);\n+        bytes16[] memory stockClassIds = new bytes16[](1);\n+        uint256[] memory quantities = new uint256[](1);\n+        uint256[] memory sharePrices = new uint256[](1);\n+        uint40[] memory timestamps = new uint40[](1);\n+\n+        stakeholderIds[0] = 0x12345678901234567890123456789012; // Non-existent stakeholder\n+        securityIds[0] = 0x12345678901234567890123456789012;\n+        stockClassIds[0] = 0x12345678901234567890123456789012; // Non-existent stock class\n+        quantities[0] = 1000;\n+        sharePrices[0] = 10000000000;\n+        timestamps[0] = uint40(block.timestamp);\n+\n+        bytes memory expectedError = abi.encodeWithSignature(\"NoStakeholder(bytes16)\", stakeholderIds[0]);\n+        vm.expectRevert(expectedError);\n+\n+        capTable.seedMultipleActivePositionsAndSecurityIds(stakeholderIds, securityIds, stockClassIds, quantities, sharePrices, timestamps);\n     }\n }"
            },
            {
                "filename": "chain/test/StockTransfer.t.sol",
                "additions": 39,
                "deletions": 75,
                "patch": "@@ -4,21 +4,10 @@ pragma solidity ^0.8.20;\n import \"forge-std/console.sol\";\n \n import \"./CapTable.t.sol\";\n-import {\n-    InitialShares,\n-    IssuerInitialShares,\n-    StockClassInitialShares,\n-    Issuer,\n-    StockClass,\n-    StockIssuanceParams,\n-    ShareNumbersIssued,\n-    StockIssuance,\n-    StockTransfer,\n-    StockParams\n-} from \"../src/lib/Structs.sol\";\n+import { InitialShares, IssuerInitialShares, StockClassInitialShares, Issuer, StockClass, StockIssuanceParams, ShareNumbersIssued, StockIssuance, StockTransfer, StockParams } from \"../src/lib/Structs.sol\";\n \n contract StockTransferTest is CapTableTest {\n-    function testTransferStock() public {\n+    function createTransferSetup() private returns (bytes16, bytes16, bytes16, uint256) {\n         // Create stakeholders\n         bytes16 transferorStakeholderId = 0xd3373e0a4dd940000000000000000006;\n         bytes16 transfereeStakeholderId = 0xd3373e0a4dd940000000000000000007;\n@@ -29,68 +18,26 @@ contract StockTransferTest is CapTableTest {\n         bytes16 stockClassId = 0xd3373e0a4dd940000000000000000008;\n         capTable.createStockClass(stockClassId, \"Common\", 100, 1000000);\n \n-        // Create stock issuance\n-        StockIssuanceParams memory issuanceParams1 = StockIssuanceParams({\n-            stock_class_id: stockClassId,\n-            stock_plan_id: 0x00000000000000000000000000000000,\n-            share_numbers_issued: ShareNumbersIssued(0, 0),\n-            share_price: 100,\n-            quantity: 1000,\n-            vesting_terms_id: 0x00000000000000000000000000000000,\n-            cost_basis: 50,\n-            stock_legend_ids: new bytes16[](0),\n-            issuance_type: \"RSA\",\n-            comments: new string[](0),\n-            custom_id: \"R2-D2\",\n-            stakeholder_id: transferorStakeholderId,\n-            board_approval_date: \"2023-01-01\",\n-            stockholder_approval_date: \"2023-01-02\",\n-            consideration_text: \"For services rendered\",\n-            security_law_exemptions: new string[](0)\n-        });\n-        capTable.issueStock(issuanceParams1);\n-\n-        StockIssuanceParams memory issuanceParams2 = StockIssuanceParams({\n-            stock_class_id: stockClassId,\n-            stock_plan_id: 0x00000000000000000000000000000000,\n-            share_numbers_issued: ShareNumbersIssued(0, 0),\n-            share_price: 100,\n-            quantity: 2000,\n-            vesting_terms_id: 0x00000000000000000000000000000000,\n-            cost_basis: 50,\n-            stock_legend_ids: new bytes16[](0),\n-            issuance_type: \"RSA\",\n-            comments: new string[](0),\n-            custom_id: \"R2-D2\",\n-            stakeholder_id: transferorStakeholderId,\n-            board_approval_date: \"2023-01-01\",\n-            stockholder_approval_date: \"2023-01-02\",\n-            consideration_text: \"For services rendered\",\n-            security_law_exemptions: new string[](0)\n-        });\n-        capTable.issueStock(issuanceParams2);\n+        uint256 firstIssuanceQty = 3000;\n+        uint256 secondIssuanceQty = 2000;\n+\n+        // Issue twice to the same stakeholder\n+        issueStock(stockClassId, transferorStakeholderId, firstIssuanceQty);\n+        issueStock(stockClassId, transferorStakeholderId, secondIssuanceQty);\n+\n+        uint256 totalIssued = firstIssuanceQty + secondIssuanceQty;\n+\n+        return (transferorStakeholderId, transfereeStakeholderId, stockClassId, totalIssued);\n+    }\n+\n+    function testTransferStockAcrossMultiplePositions() public {\n+        (bytes16 transferorStakeholderId, bytes16 transfereeStakeholderId, bytes16 stockClassId, uint256 totalIssued) = createTransferSetup();\n \n         // Transfer stock\n-        uint256 quantityToTransfer = 2500;\n-        capTable.transferStock(\n-            transferorStakeholderId,\n-            transfereeStakeholderId,\n-            stockClassId,\n-            true,\n-            quantityToTransfer,\n-            issuanceParams1.share_price\n-        );\n-\n-        /*\n-            Transactions Array:\n-                 1. Delete 1000 position from sh1\n-                 2. issue 1000 to sh2\n-                 3. create tx transfer (quantity = 1000)\n-                 4. delete 2000 position from sh 1\n-                 5. issue 500 position to sh1\n-                 6. issue 1500 to position sh1\n-                 7. create tx transfer (quantity = 1500)\n-        */\n+        uint256 quantityToTransfer = 3500;\n+        uint256 price = 25;\n+        capTable.transferStock(transferorStakeholderId, transfereeStakeholderId, stockClassId, true, quantityToTransfer, price);\n+\n         uint256 transactionsCount = capTable.getTransactionsCount();\n         bytes memory lastIssuanceTx = capTable.transactions(transactionsCount - 2);\n         bytes memory firstTransferTx = capTable.transactions(transactionsCount - 4);\n@@ -100,8 +47,25 @@ contract StockTransferTest is CapTableTest {\n         bytes16 remainingIssuanceSecurityId = abi.decode(lastIssuanceTx, (StockIssuance)).security_id;\n         StockTransfer memory secondTransfer = abi.decode(secondTransferTx, (StockTransfer));\n \n-        assertEq(firstTransfer.quantity, 1000);\n-        assertEq(secondTransfer.quantity, 1500);\n+        assertEq(firstTransfer.quantity, 3000);\n+        assertEq(secondTransfer.quantity, 500);\n         assertEq(secondTransfer.balance_security_id, remainingIssuanceSecurityId);\n+\n+        (, , uint256 shares_issued, ) = capTable.issuer();\n+\n+        // shares issued should not have changed.\n+        assertEq(shares_issued, totalIssued);\n+    }\n+\n+    function testTransferMoreThanAvailable() public {\n+        (bytes16 transferorStakeholderId, bytes16 transfereeStakeholderId, bytes16 stockClassId, uint256 totalIssued) = createTransferSetup();\n+\n+        // Transfer stock\n+        uint256 quantityToTransfer = 5500;\n+        uint256 price = 25;\n+\n+        bytes memory expectedError = abi.encodeWithSignature(\"InsufficientShares(uint256,uint256)\", totalIssued, quantityToTransfer);\n+        vm.expectRevert(expectedError);\n+        capTable.transferStock(transferorStakeholderId, transfereeStakeholderId, stockClassId, true, quantityToTransfer, price);\n     }\n }"
            },
            {
                "filename": "ocf",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1 +1 @@\n-Subproject commit 91ad013d346430f136cd87893dabcade463a9927\n+Subproject commit 7ef3887404655d1f17e8de8d6697d8ae0c468dab"
            },
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 24,
                "deletions": 7,
                "patch": "@@ -1,8 +1,10 @@\n import { config } from \"dotenv\";\n import { ethers } from \"ethers\";\n-import { toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import CAP_TABLE from \"../../chain/out/CapTable.sol/CapTable.json\" assert { type: \"json\" };\n+import CAP_TABLE_FACTORY from \"../../chain/out/CapTableFactory.sol/CapTableFactory.json\" assert { type: \"json\" };\n+import { toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n+import { readFactory } from \"../db/operations/read.js\";\n \n config();\n \n@@ -21,19 +23,34 @@ async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n \n     console.log(\"\ud83d\uddfd | Wallet address: \", wallet.address);\n \n-    const factory = new ethers.ContractFactory(CAP_TABLE.abi, CAP_TABLE.bytecode, wallet);\n-    const contract = await factory.deploy(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));\n+    const factory = await readFactory();\n+    const factoryAddress = factory[0].factory_address;\n \n-    console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n+    console.log('factory ', factory)\n+    \n+    if(!factoryAddress) {\n+        throw new Error(`\u274c | Factory address not found`);\n+    }\n+   \n+    const capTableFactory = new ethers.Contract(factoryAddress, CAP_TABLE_FACTORY.abi, wallet);\n+\n+    const tx = await capTableFactory.createCapTable(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));\n+    await tx.wait();\n \n-    const libraries = getTXLibContracts(contract.target, wallet);\n+    const capTableCount = await capTableFactory.getCapTableCount();\n \n-    console.log(\"\u2705 | Contract deployed!\");\n+    const latestCapTableProxyContractAddress = await capTableFactory.capTableProxies(capTableCount - BigInt(1));\n+\n+    const contract = new ethers.Contract(latestCapTableProxyContractAddress, CAP_TABLE.abi, wallet); \n+\n+    console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n+    console.log(\"cap table contract address \", latestCapTableProxyContractAddress);\n+    const libraries = getTXLibContracts(latestCapTableProxyContractAddress, wallet);\n \n     return {\n         contract,\n         provider,\n-        address: contract.target,\n+        address: latestCapTableProxyContractAddress,\n         libraries,\n     };\n }"
            },
            {
                "filename": "src/chain-operations/transactionListener.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -43,7 +43,7 @@ const txMapper = {\n };\n \n async function startOnchainListeners(contract, provider, issuerId, libraries) {\n-    console.log(\"\ud83c\udf10 | Initiating on-chain event listeners for \", contract.target);\n+    console.log(\"\ud83c\udf10 | Initiating on-chain event listeners for issuer\", issuerId, \"at address\", contract.target);\n \n     libraries.txHelper.on(\"TxCreated\", async (_, txTypeIdx, txData, event) => {\n         const [type, structType] = txMapper[txTypeIdx];"
            },
            {
                "filename": "src/db/objects/Factory.js",
                "additions": 13,
                "deletions": 0,
                "patch": "@@ -0,0 +1,13 @@\n+import mongoose from \"mongoose\";\n+import { v4 as uuid } from \"uuid\";\n+\n+const FactorySchema = new mongoose.Schema({\n+    _id: { type: String, default: () => uuid() },\n+    object_type: { type: String, default: \"FACTORY\" },\n+    implementation_address: String,\n+    factory_address: String,\n+}, { timestamps: true });\n+\n+const Factory = mongoose.model(\"Factory\", FactorySchema);\n+\n+export default Factory;"
            },
            {
                "filename": "src/db/operations/read.js",
                "additions": 11,
                "deletions": 0,
                "patch": "@@ -8,6 +8,7 @@ import VestingTerms from \"../objects/VestingTerms.js\";\n import HistoricalTransaction from \"../objects/HistoricalTransaction.js\";\n import StockIssuance from \"../objects/transactions/issuance/StockIssuance.js\";\n import StockTransfer from \"../objects/transactions/transfer/StockTransfer.js\";\n+import Factory from \"../objects/Factory.js\";\n \n // READ By ID\n export const readIssuerById = async (id) => {\n@@ -99,3 +100,13 @@ export const getAllIssuerDataById = async (issuerId) => {\n         stockTransfers: issuerStockTransfers,\n     };\n };\n+\n+export const readAllIssuers = async () => {\n+    const issuers = await Issuer.find();\n+    return issuers;\n+}\n+\n+export const readFactory = async () => {\n+    const factory = await Factory.find();\n+    return factory;\n+}\n\\ No newline at end of file"
            },
            {
                "filename": "src/scripts/testMintingCapTable.js",
                "additions": 15,
                "deletions": 15,
                "patch": "@@ -10,32 +10,32 @@ const main = async () => {\n \n     console.log(\"\u2705 | Issuer response \", issuerResponse.data);\n \n-    await sleep(3000);\n+    // await sleep(3000);\n \n-    console.log(\"\u23f3 | Creating first stakeholder\");\n+    // console.log(\"\u23f3 | Creating first stakeholder\");\n \n-    // create two stakeholders\n-    const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerResponse.data.issuer._id));\n+    // // create two stakeholders\n+    // const stakeholder1Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder1(issuerResponse.data.issuer._id));\n \n-    console.log(\"\u2705 | stakeholder1Response\", stakeholder1Response.data);\n-    console.log(\"\u2705 | finished\");\n+    // console.log(\"\u2705 | stakeholder1Response\", stakeholder1Response.data);\n+    // console.log(\"\u2705 | finished\");\n \n-    await sleep(3000);\n+    // await sleep(3000);\n \n-    console.log(\"\u23f3 | Creating second stakeholder\u2026\");\n+    // console.log(\"\u23f3 | Creating second stakeholder\u2026\");\n \n-    const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerResponse.data.issuer._id));\n+    // const stakeholder2Response = await axios.post(\"http://localhost:8080/stakeholder/create\", stakeholder2(issuerResponse.data.issuer._id));\n \n-    console.log(\"\u2705 | stakeholder2Response\", stakeholder2Response.data);\n+    // console.log(\"\u2705 | stakeholder2Response\", stakeholder2Response.data);\n \n-    await sleep(3000);\n+    // await sleep(3000);\n \n-    console.log(\"\u23f3| Creating stock class\");\n+    // console.log(\"\u23f3| Creating stock class\");\n \n-    // create stockClass\n-    const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerResponse.data.issuer._id));\n+    // // create stockClass\n+    // const stockClassResponse = await axios.post(\"http://localhost:8080/stock-class/create\", stockClass(issuerResponse.data.issuer._id));\n \n-    console.log(\"\u2705 | stockClassResponse\", stockClassResponse.data);\n+    // console.log(\"\u2705 | stockClassResponse\", stockClassResponse.data);\n };\n \n main()"
            },
            {
                "filename": "src/server.js",
                "additions": 14,
                "deletions": 1,
                "patch": "@@ -19,7 +19,7 @@ import transactionRoutes from \"./routes/transactions.js\";\n import valuationRoutes from \"./routes/valuation.js\";\n import vestingTermsRoutes from \"./routes/vestingTerms.js\";\n \n-import { readIssuerById } from \"./db/operations/read.js\";\n+import { readIssuerById, readAllIssuers } from \"./db/operations/read.js\";\n import { contractCache } from \"./utils/caches.js\";\n \n const app = express();\n@@ -81,4 +81,17 @@ app.use(\"/transactions/\", contractMiddleware, transactionRoutes);\n \n app.listen(PORT, async () => {\n     console.log(`\ud83d\ude80  Server successfully launched. Access at: http://localhost:${PORT}`);\n+     // Fetch all issuers\n+     const issuers = await readAllIssuers();\n+     if (issuers && issuers.length > 0) {\n+         for (const issuer of issuers) {\n+             if (issuer.deployed_to) {\n+                 // Create a new contract instance for each issuer\n+                 const { contract, provider, libraries } = await getContractInstance(CHAIN, issuer.deployed_to);\n+ \n+                 // Initialize listener for this contract\n+                 startOnchainListeners(contract, provider, issuer._id, libraries);\n+             }\n+         }\n+     }\n });"
            }
        ]
    },
    {
        "sha": "ee6a158e79004b29d630d2ad23c7e886b7593da8",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 17:11:24+00:00",
        "message": "Update links and deployment info",
        "files": [
            {
                "filename": "README.md",
                "additions": 13,
                "deletions": 9,
                "patch": "@@ -2,15 +2,15 @@\n \n Developed by:\n \n-- [Poet](https://poet.network/)\n+- [Transfer Agent Protocol](https://transferagentprotocol.xyz/)\n - [Plural Energy](https://www.pluralenergy.co/)\n - [Fairmint](https://www.fairmint.com/)\n \n This repo is based on the [Open Cap Table Coalition](https://github.com/Open-Cap-Table-Coalition/Open-Cap-Format-OCF) standard, with the license included in its entirety. In development, it's meant to be run in a Docker container with a local MongoDB instance. While in active development, it's meant to be run with [Anvil](https://book.getfoundry.sh/anvil/) and [Forge](https://book.getfoundry.sh/forge/).\n \n <div align=\"center\">\n-  <a href=\"https://github.com/poet-network/tap-cap-table/blob/main/LICENSE\">\n-    <img alt=\"License\" src=\"https://img.shields.io/github/license/poet-network/tap-cap-table\">\n+  <a href=\"https://github.com/transfer-agent-protocol/tap-cap-table/blob/main/LICENSE\">\n+    <img alt=\"License\" src=\"https://img.shields.io/github/license/transfer-agent-protocol/tap-cap-table\">\n   </a>\n </div>\n \n@@ -36,7 +36,7 @@ We're using the official [MongoDB Docker image](https://hub.docker.com/_/mongo)\n \n ## Official links\n \n-- [Contributor doc](https://coda.io/d/_drhpwRhDok-/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n+- [Contributor doc](https://coda.io/d/_dFoHg0h07Et/Transfer-Agent-Protocol_sua17) - to read about the project and how to contribute.\n - [Slack](https://transferagentprotocol.slack.com/) - invite only for now.\n \n ## Getting started\n@@ -87,17 +87,21 @@ Install dependencies and setup [Foundry](https://book.getfoundry.sh/) and `forge\n yarn install && yarn setup\n ```\n \n-## Deploying external libraries\n+## Deploying the cap table smart contracts\n \n-In our architecture, each transaction is mapped to an external library, which ensures bytecode limits are never met.\n-\n-To deploy these libraries:\n+In our architecture, each transaction is mapped to an external library, which ensures bytecode limits are never met. To deploy these libraries:\n \n 1. Ensure you have Anvil running in the `/chain` directory\n-2. Then, inside of the root directory run `yarn build`\n+2. Then, inside of the root directory run\n+\n+```sh\n+yarn build\n+```\n \n This will build all libraries and will take at least 5 minutes to complete. Each library is being deployed one at a time using a dependency graph that's generated with the command.\n \n+To deploy the cap table smart contracts to a testnet, update `RPC_URL` and `CHAIN_ID` in the `.env` file, then run the same `yarn build` command.\n+\n \n ## Running the cap table server\n "
            }
        ]
    },
    {
        "sha": "a1b5abf0d04dcb36bf1ffea528fca5d06b6f2c2c",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 16:43:52+00:00",
        "message": "Update license",
        "files": [
            {
                "filename": "LICENSE",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,4 +1,4 @@\n-Copyright 2023 Poet Network Inc.\n+Copyright 2023 Alex Palmer, Victor Mimo.\n \n Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u201cSoftware\u201d), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n "
            }
        ]
    },
    {
        "sha": "5a3822812158ac33bde803feacf84ccf7c907c39",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 16:43:42+00:00",
        "message": "Remove unused import",
        "files": [
            {
                "filename": "src/chain-operations/seed.js",
                "additions": 0,
                "deletions": 1,
                "patch": "@@ -1,6 +1,5 @@\n import { convertAndReflectStakeholderOnchain } from \"../controllers/stakeholderController.js\";\n import { convertAndReflectStockClassOnchain } from \"../controllers/stockClassController.js\";\n-import { convertAndSeedIssuanceStockOnchain, convertAndSeedTransferStockOnchain } from \"../controllers/transactions/seed.js\";\n import { getAllIssuerDataById } from \"../db/operations/read.js\";\n import { convertTimeStampToUint40, toScaledBigNumber } from \"../utils/convertToFixedPointDecimals.js\";\n import { convertBytes16ToUUID, convertUUIDToBytes16 } from \"../utils/convertUUID.js\";"
            }
        ]
    },
    {
        "sha": "a62ef0a176ac443537f1c588426858c240276e6d",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 16:43:29+00:00",
        "message": "Remove unused scripts",
        "files": [
            {
                "filename": "package.json",
                "additions": 2,
                "deletions": 12,
                "patch": "@@ -2,9 +2,9 @@\n     \"name\": \"tap-cap-table\",\n     \"version\": \"1.0.0-alpha.0\",\n     \"private\": true,\n-    \"author\": \"Alex Palmer <alex@poet.network>, Victor Mimo <victor@poet.network>\",\n+    \"author\": \"Alex Palmer, Victor Mimo\",\n     \"license\": \"MIT\",\n-    \"description\": \"Transfer Agent Protocol compliant cap table\",\n+    \"description\": \"Transfer Agent Protocol onchain cap table\",\n     \"type\": \"module\",\n     \"scripts\": {\n         \"start\": \"nodemon src/server.js\",\n@@ -13,21 +13,11 @@\n         \"format\": \"prettier '**/*' --ignore-unknown --write\",\n         \"typecheck\": \"concurrently --raw yarn:typecheck:*\",\n         \"typecheck:app\": \"tsc --noEmit\",\n-        \"typecheck:cypress\": \"tsc --noEmit -p cypress/tsconfig.json\",\n         \"prepare\": \"husky install\",\n         \"build\": \"cd chain && node ../src/scripts/deployAndLinkLibs.js\",\n         \"setup\": \"cd chain && foundryup && forge build --via-ir\",\n         \"deseed\": \"node src/db/scripts/deseed.js\",\n         \"test\": \"cd chain && forge test\",\n-        \"validate-poet-files\": \"cd ocf && yarn validate-poet-files\",\n-        \"test-onchain-cap-table-local\": \"node src/chain-operations/capTable.cjs local\",\n-        \"test-onchain-cap-table-optimism-goerli\": \"node src/chain-operations/capTable.cjs optimism-goerli\",\n-        \"test-onchain-cap-table-factory-local\": \"node src/chain-operations/capTableFactory.cjs local\",\n-        \"test-onchain-cap-table-factory-optimism-goerli\": \"node src/chain-operations/capTableFactory.cjs optimism-goerli\",\n-        \"forge-deploy-captable-local\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-optimism-goerli\": \"cd chain && forge script script/CapTable.s.sol:CapTableDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-factory-local\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployLocalScript --fork-url local --broadcast --verify -vvvv\",\n-        \"forge-deploy-captable-factory-optimism-goerli\": \"cd chain && forge script script/CapTableFactory.s.sol:CapTableFactoryDeployOptimismGoerli --rpc-url optimism_goerli --etherscan-api-key optimism_goerli_etherscan --broadcast --verify -vvvv\",\n         \"export-manifest\": \"cd src/db/samples && zip -r notPoet.zip notPoet && mv notPoet.zip $HOME/Downloads\"\n     },\n     \"dependencies\": {"
            }
        ]
    },
    {
        "sha": "281a4a23ba556102d8cdd603d388462276f6d2e0",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 16:43:17+00:00",
        "message": "Simplify .env variables",
        "files": [
            {
                "filename": "src/scripts/deployAndLinkLibs.js",
                "additions": 8,
                "deletions": 12,
                "patch": "@@ -1,15 +1,14 @@\n-import { spawn } from \"child_process\";\n import { config } from \"dotenv\";\n+import { spawn } from \"child_process\";\n import fs from \"fs\";\n import path from \"path\";\n import sleep from \"../utils/sleep.js\";\n \n config();\n \n-const privateKey = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n-// Use environment variable for the RPC URL\n-const LOCAL_RPC_URL = process.env.LOCAL_RPC_URL;\n-\n+const PRIVATE_KEY = process.env.PRIVATE_KEY;\n+const RPC_URL = process.env.RPC_URL;\n+const CHAIN_ID = process.env.CHAIN_ID;\n const rootDirectory = \"./src/lib\";\n const excludeDirectory = \"./src/lib/transactions\";\n \n@@ -87,15 +86,12 @@ async function deployLib(lib, libs) {\n         const args = [\n             \"c\",\n             \"-r\",\n-            // Using the LOCAL_RPC_URL from .env\n-            LOCAL_RPC_URL,\n+            // Using the RPC_URL from .env\n+            RPC_URL,\n             \"--chain\",\n-            // TODO: Handle changing Anvil's chain id better\n-            // 31337,\n-            // This is Arbitrum Orbit's chain id\n-            32586980208,\n+            CHAIN_ID,\n             \"--private-key\",\n-            privateKey,\n+            PRIVATE_KEY,\n             `${lib.path}:${lib.libraryName || lib.fileName}`,\n             \"--json\",\n             ...librariesDepsArgs,"
            }
        ]
    },
    {
        "sha": "29b8ac9b0c9b3947110c80b58efd43bdf96adcd9",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 16:42:33+00:00",
        "message": "Simplify RPC_URL",
        "files": [
            {
                "filename": "chain/foundry.toml",
                "additions": 2,
                "deletions": 3,
                "patch": "@@ -4,11 +4,10 @@ out = \"out\"\n libs = [\"lib\"]\n \n [rpc_endpoints]\n-rpc_url = \"${L2_TESTNET_RPC_URL}\"\n-local = \"${LOCAL_RPC_URL}\"\n+rpc_url = \"${RPC_URL}\"\n \n [etherscan]\n-optimism_goerli_etherscan = { key = \"${ETHERSCAN_OPTIMISM_API_KEY}\", chain = \"goerli\" }\n+optimism_goerli_etherscan = { key = \"${ETHERSCAN_L2_API_KEY}\", chain = \"goerli\" }\n \n \n "
            },
            {
                "filename": "chain/script/CapTable.s.sol",
                "additions": 2,
                "deletions": 2,
                "patch": "@@ -10,7 +10,7 @@ contract CapTableDeployLocalScript is Script {\n     function setUp() public {}\n \n     function run() public {\n-        uint256 deployerPrivateKeyFakeAccount = vm.envUint(\"PRIVATE_KEY_FAKE_ACCOUNT\");\n+        uint256 deployerPrivateKeyFakeAccount = vm.envUint(\"PRIVATE_KEY\");\n \n         vm.startBroadcast(deployerPrivateKeyFakeAccount);\n \n@@ -24,7 +24,7 @@ contract CapTableDeployOptimismGoerli is Script {\n     function setUp() public {}\n \n     function run() public {\n-        uint256 deployerPrivateKeyPoetTest = vm.envUint(\"PRIVATE_KEY_POET_TEST\");\n+        uint256 deployerPrivateKeyPoetTest = vm.envUint(\"PRIVATE_KEY\");\n \n         vm.startBroadcast(deployerPrivateKeyPoetTest);\n "
            }
        ]
    },
    {
        "sha": "6b45c065609175292e1a83fb274538dd535217f8",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 16:41:56+00:00",
        "message": "Simplify getting contract instances and remove unused OP Goerli function",
        "files": [
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 10,
                "deletions": 34,
                "patch": "@@ -5,46 +5,22 @@ import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n \n config();\n \n-\n-async function getLocalContractInstance(address) {\n-    const CONTRACT_ADDRESS_LOCAL = address;\n-\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n-    const LOCAL_RPC_URL = process.env.LOCAL_RPC_URL;\n+async function getContractInstance(chain, address) {\n+    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n+    const RPC_URL = process.env.RPC_URL;\n+    const CHAIN_ID = parseInt(process.env.CHAIN_ID); // Convert to integer\n \n     const customNetwork = {\n-        chainId: 31337,\n-        name: \"local\",\n+        chainId: CHAIN_ID, // Use the CHAIN_ID from .env\n+        name: chain, // Use the chain parameter as the network name\n     };\n \n-    const provider = new ethers.JsonRpcProvider(LOCAL_RPC_URL, customNetwork);\n+    const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-    const contract = new ethers.Contract(CONTRACT_ADDRESS_LOCAL, CAP_TABLE.abi, wallet);\n-    const libraries = getTXLibContracts(contract.target, wallet)\n-    return { contract, provider, libraries };\n-}\n-\n-async function getOptimismGoerliContractInstance(address) {\n-    const CONTRACT_ADDRESS_OPTIMISM_GOERLI = address;\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_POET_TEST;\n+    const contract = new ethers.Contract(address, CAP_TABLE.abi, wallet);\n+    const libraries = getTXLibContracts(contract.target, wallet);\n \n-    const provider = new ethers.JsonRpcProvider(process.env.L2_TESTNET_RPC_URL);\n-    const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-    const contract = new ethers.Contract(CONTRACT_ADDRESS_OPTIMISM_GOERLI, abi, wallet);\n-    const libraries = getTXLibContracts(contract.target, wallet)\n-\n-\n-    return { contract, provider, libraries};\n-}\n-\n-async function getContractInstance(chain, address) {\n-    if (chain === \"local\") {\n-        return getLocalContractInstance(address);\n-    } else if (chain === \"optimism-goerli\") {\n-        return getOptimismGoerliContractInstance(address);\n-    } else {\n-        throw new Error(`Unsupported chain: ${chain}`);\n-    }\n+    return { contract, provider, libraries };\n }\n \n export default getContractInstance;"
            }
        ]
    },
    {
        "sha": "02bd7ea77583638483adfd12595e90ad6352950e",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 16:41:12+00:00",
        "message": "Simplify and cleanup deployment",
        "files": [
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 13,
                "deletions": 46,
                "patch": "@@ -6,37 +6,30 @@ import getTXLibContracts from \"../utils/getLibrariesContracts.js\";\n \n config();\n \n-async function deployCapTableLocal(issuerId, issuerName, initial_shares_authorized) {\n-    // Use environment variables for private key and provider endpoint\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n-    // Use the LOCAL_RPC_URL from .env\n-    const LOCAL_RPC_URL = process.env.LOCAL_RPC_URL;\n+async function deployCapTable(issuerId, issuerName, initial_shares_authorized) {\n+    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY;\n+    const RPC_URL = process.env.RPC_URL;\n+    const CHAIN_ID = process.env.CHAIN_ID;\n \n     const customNetwork = {\n-        // TODO: handle changing Anvil's chain id better\n-        // chainId: 31337,\n-        // This one is Arbitrum Orbit's chain id\n-        chainId: 32586980208,\n-        name: \"local\",\n+        // Change the CHAIN_ID in the .env file to deploy to a different network\n+        chainId: parseInt(CHAIN_ID),\n     };\n \n-    // Use the LOCAL_RPC_URL for the provider\n-    const provider = new ethers.JsonRpcProvider(LOCAL_RPC_URL, customNetwork);\n+    const provider = new ethers.JsonRpcProvider(RPC_URL, customNetwork);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-    console.log(\"\ud83d\uddfd | Wallet address \", wallet.address);\n \n-    const factory = new ethers.ContractFactory(CAP_TABLE.abi, CAP_TABLE.bytecode, wallet);\n-    console.log(\n-        `\u2705 | Issuer id inside of deployment: ${issuerId},\n-        \u2705 | Issuer name inside of deployment: ${issuerName},\n-        \u2705 | With initial shares: ${initial_shares_authorized}`\n-    );\n+    console.log(\"\ud83d\uddfd | Wallet address: \", wallet.address);\n \n+    const factory = new ethers.ContractFactory(CAP_TABLE.abi, CAP_TABLE.bytecode, wallet);\n     const contract = await factory.deploy(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));\n \n     console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n+\n     const libraries = getTXLibContracts(contract.target, wallet);\n \n+    console.log(\"\u2705 | Contract deployed!\");\n+\n     return {\n         contract,\n         provider,\n@@ -45,30 +38,4 @@ async function deployCapTableLocal(issuerId, issuerName, initial_shares_authoriz\n     };\n }\n \n-async function deployCapTableOptimismGoerli(issuerId, issuerName, initial_shares_authorized) {\n-    const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_POET_TEST;\n-\n-    const provider = new ethers.JsonRpcProvider(process.env.L2_TESTNET_RPC_URL);\n-    const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n-    const factory = new ethers.ContractFactory(CAP_TABLE.abi, CAP_TABLE.bytecode, wallet);\n-    const contract = await factory.deploy(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));\n-\n-    console.log(\"\u23f3 | Waiting for contract to be deployed...\");\n-\n-    console.log(\"\u2705 | Contract deployed!\");\n-\n-    const libraries = getTXLibContracts(contract.target, wallet);\n-\n-    return { contract, provider, address: contract.target, libraries };\n-}\n-\n-async function deployCapTable(chain, issuerId, issuerName, initial_shares_authorized) {\n-    if (chain === \"local\") {\n-        return deployCapTableLocal(issuerId, issuerName, initial_shares_authorized);\n-    } else if (chain === \"optimism-goerli\") {\n-        return deployCapTableOptimismGoerli(issuerId, issuerName, initial_shares_authorized);\n-    } else {\n-        throw new Error(`\u274c | Unsupported chain: ${chain}`);\n-    }\n-}\n-export default deployCapTable;\n+export default deployCapTable;\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "78f122738ee44c10ad2d0c08259f347d9012f315",
        "author": "ThatAlexPalmer",
        "date": "2024-01-04 16:40:45+00:00",
        "message": "Simplify .env files (used in initial setup and deployments to different networks)",
        "files": [
            {
                "filename": ".env.example",
                "additions": 18,
                "deletions": 8,
                "patch": "@@ -1,9 +1,19 @@\n+# Offchain db connection string for mongodb\n DATABASE_URL=\"mongodb://tap:tap@localhost:27017/mongo?authSource=admin&retryWrites=true&w=majority\"\n-L2_TESTNET_RPC_UR=UPDATE_ME\n-LOCAL_RPC_URL=http://127.0.0.1:8545\n-PRIVATE_KEY_FAKE_ACCOUNT=UPDATE_ME\n-PRIVATE_KEY_POET_TEST=UPDATE_ME\n-ETHERSCAN_OPTIMISM_API_KEY=UPDATE_ME\n-ETHERSCAN_ETHEREUM_API_KEY=UPDATE_ME\n-CHAIN=local\n-# CHAIN=\"optimism-goerli\"\n+\n+# RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n+RPC_URL=http://127.0.0.1:8545\n+\n+# Change this to the chain id of the network you are deploying to\n+# Use 31337 for Anvil's, 32586980208 for Arbitrum's\n+CHAIN_ID=31337\n+\n+# Update with the private key of the account that will be used to deploy the contracts\n+PRIVATE_KEY=UPDATE_ME\n+\n+# Etherscan API keys\n+ETHERSCAN_L2_API_KEY=UPDATE_ME\n+ETHERSCAN_L1_API_KEY=UPDATE_ME\n+\n+# Server port\n+PORT=3000\n\\ No newline at end of file"
            },
            {
                "filename": "chain/.env.example",
                "additions": 13,
                "deletions": 6,
                "patch": "@@ -1,6 +1,13 @@\n-L2_TESTNET_RPC_URL=UPDATE_ME\n-LOCAL_RPC_URL=http://127.0.0.1:8545\n-PRIVATE_KEY_FAKE_ACCOUNT=UPDATE_ME\n-PRIVATE_KEY_POET_TEST=UPDATE_ME\n-ETHERSCAN_OPTIMISM_API_KEY=UPDATE_ME\n-ETHERSCAN_ETHEREUM_API_KEY=UPDATE_ME\n+# RPC url for testnet (defaults to Anvil's http://127.0.0.1:8545)\n+RPC_URL=http://127.0.0.1:8545\n+\n+# Change this to the chain id of the network you are deploying to\n+# Use 31337 for Anvil's, 32586980208 for Arbitrum's\n+CHAIN_ID=31337\n+\n+# Update with the private key of the account that will be used to deploy the contracts\n+PRIVATE_KEY=UPDATE_ME\n+\n+# Etherscan API keys\n+ETHERSCAN_L2_API_KEY=UPDATE_ME\n+ETHERSCAN_L1_API_KEY=UPDATE_ME"
            }
        ]
    },
    {
        "sha": "1350be9221d0795cc3645fc2ede46f2c80200fb0",
        "author": "kentkolze",
        "date": "2024-01-04 15:51:52+00:00",
        "message": "first push attempt",
        "files": [
            {
                "filename": "src/db/operations/atomicity.js",
                "additions": 3,
                "deletions": 0,
                "patch": "@@ -0,0 +1,3 @@\n+/*\n+For database transactions to ensure each event is processed exactly once\n+*/\n\\ No newline at end of file"
            }
        ]
    },
    {
        "sha": "d969eb4fe4458351b8c7b42468322612ddf6d8bf",
        "author": "ThatAlexPalmer",
        "date": "2024-01-03 21:08:59+00:00",
        "message": "Update L2_TESTNET_RPC_URL",
        "files": [
            {
                "filename": ".env.example",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,5 +1,5 @@\n DATABASE_URL=\"mongodb://tap:tap@localhost:27017/mongo?authSource=admin&retryWrites=true&w=majority\"\n-OPTIMISM_GOERLI_RPC_URL=UPDATE_ME\n+L2_TESTNET_RPC_UR=UPDATE_ME\n LOCAL_RPC_URL=http://127.0.0.1:8545\n PRIVATE_KEY_FAKE_ACCOUNT=UPDATE_ME\n PRIVATE_KEY_POET_TEST=UPDATE_ME"
            },
            {
                "filename": "chain/.env.example",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -1,4 +1,4 @@\n-OPTIMISM_GOERLI_RPC_URL=UPDATE_ME\n+L2_TESTNET_RPC_URL=UPDATE_ME\n LOCAL_RPC_URL=http://127.0.0.1:8545\n PRIVATE_KEY_FAKE_ACCOUNT=UPDATE_ME\n PRIVATE_KEY_POET_TEST=UPDATE_ME"
            },
            {
                "filename": "chain/foundry.toml",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -4,7 +4,7 @@ out = \"out\"\n libs = [\"lib\"]\n \n [rpc_endpoints]\n-optimism_goerli = \"${OPTIMISM_GOERLI_RPC_URL}\"\n+rpc_url = \"${L2_TESTNET_RPC_URL}\"\n local = \"${LOCAL_RPC_URL}\"\n \n [etherscan]"
            },
            {
                "filename": "src/chain-operations/deployCapTable.js",
                "additions": 3,
                "deletions": 3,
                "patch": "@@ -9,7 +9,8 @@ config();\n async function deployCapTableLocal(issuerId, issuerName, initial_shares_authorized) {\n     // Use environment variables for private key and provider endpoint\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_FAKE_ACCOUNT;\n-    const LOCAL_RPC_URL = process.env.LOCAL_RPC_URL; // Use the LOCAL_RPC_URL from .env\n+    // Use the LOCAL_RPC_URL from .env\n+    const LOCAL_RPC_URL = process.env.LOCAL_RPC_URL;\n \n     const customNetwork = {\n         // TODO: handle changing Anvil's chain id better\n@@ -44,11 +45,10 @@ async function deployCapTableLocal(issuerId, issuerName, initial_shares_authoriz\n     };\n }\n \n-\n async function deployCapTableOptimismGoerli(issuerId, issuerName, initial_shares_authorized) {\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_POET_TEST;\n \n-    const provider = new ethers.JsonRpcProvider(process.env.OPTIMISM_GOERLI_RPC_URL);\n+    const provider = new ethers.JsonRpcProvider(process.env.L2_TESTNET_RPC_URL);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n     const factory = new ethers.ContractFactory(CAP_TABLE.abi, CAP_TABLE.bytecode, wallet);\n     const contract = await factory.deploy(issuerId, issuerName, toScaledBigNumber(initial_shares_authorized));"
            },
            {
                "filename": "src/chain-operations/getContractInstances.js",
                "additions": 1,
                "deletions": 1,
                "patch": "@@ -28,7 +28,7 @@ async function getOptimismGoerliContractInstance(address) {\n     const CONTRACT_ADDRESS_OPTIMISM_GOERLI = address;\n     const WALLET_PRIVATE_KEY = process.env.PRIVATE_KEY_POET_TEST;\n \n-    const provider = new ethers.JsonRpcProvider(process.env.OPTIMISM_GOERLI_RPC_URL);\n+    const provider = new ethers.JsonRpcProvider(process.env.L2_TESTNET_RPC_URL);\n     const wallet = new ethers.Wallet(WALLET_PRIVATE_KEY, provider);\n     const contract = new ethers.Contract(CONTRACT_ADDRESS_OPTIMISM_GOERLI, abi, wallet);\n     const libraries = getTXLibContracts(contract.target, wallet)"
            }
        ]
    }
]